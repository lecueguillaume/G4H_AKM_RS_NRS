{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Ignore warnings below\n",
    "# Optimisation via tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Dot, Embedding, Add, Flatten, Activation, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "pd.options.mode.chained_assignment = None  # default='warn' # Remove copy on slice warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph formation without HNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_formation(n_patients,\n",
    "                    n_doctors,\n",
    "                    max_number_connections,\n",
    "                    z=1.4,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.01,\n",
    "                    beta_age_d_graph=0.01,\n",
    "                    beta_sex_p_graph=0.5,\n",
    "                    beta_sex_d_graph=0.5,\n",
    "                    beta_distance_graph=-0.5,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = None\n",
    "                   ):\n",
    "    \"\"\"Génère un dataframe contenant l'ensemble des connexions/inexistantes entre les patients/docteurs selon un modèle logistique.\n",
    "\n",
    "    Args:\n",
    "        n_patients (int): Le nombre de patients\n",
    "        n_doctors (int): Le nombre de docteurs\n",
    "        max_number_connections (int): Le nombre max de connexions par patient que l'on autorise\n",
    "        z (float, optional): Le seuil de distance maximale pour lequel on autorise connexion. Defaults to 1.4.\n",
    "        type_distance (str, optional): Permet de définir la manière dont on génère la distance (par défaut, on génère des points dans [0, 1] x [0, 1]),\n",
    "        sinon on utilise les CODGEO de l'INSEE. Defaults to \"default\".\n",
    "        beta_age_p_graph (float, optional): le paramètre du modèle associé à l'âge des patients. Defaults to 0.01.\n",
    "        beta_age_d_graph (float, optional): le paramètre du modèle associé à l'âge des docteurs. Defaults to 0.01.\n",
    "        beta_sex_p_graph (float, optional): le paramètre du modèle associé au sexe des patients. Defaults to 0.5.\n",
    "        beta_sex_d_graph (float, optional): le paramètre du modèle associé au sexe des docteurs. Defaults to 0.5.\n",
    "        beta_distance_graph (float, optional): le paramètre du modèle associé à la distance. Defaults to -0.5.\n",
    "        alpha_law_graph (tuple, optional): les bornes de la loi uniforme pour la génération de l'effet fixe des patients. Defaults to (-1, 1).\n",
    "        psi_law_graph (tuple, optional): les bornes de la loi uniforme pour la génération de l'effet fixe des docteurs. Defaults to (-1, 1).\n",
    "        nb_latent_factors (_type_, optional): Le nombre de vecteurs latents, par défaut, on les génère aléatoirement. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Le dataframe voulu.\n",
    "    \"\"\"\n",
    "    coor_patients = []\n",
    "    coor_doctors = []\n",
    "    alpha_graph = []\n",
    "    psi_graph = []\n",
    "    rng = np.random.default_rng(None)\n",
    "    D = np.zeros([n_patients, n_doctors], dtype = np.ndarray)\n",
    "\n",
    "    if nb_latent_factors == None: # We pick a random number of latent factors\n",
    "        random_number = random.randint(1, 10)\n",
    "        for i in range(n_patients):\n",
    "            \n",
    "            # We generate the FE for the graph formation model\n",
    "            alpha_graph.append( np.random.uniform(alpha_law_graph[0], alpha_law_graph[1], size = random_number) )\n",
    "\n",
    "        for j in range(n_doctors):\n",
    "\n",
    "            # We generate the FE for the graph formation model\n",
    "            psi_graph.append( np.random.uniform(psi_law_graph[0], psi_law_graph[1], size = random_number ) )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        for i in range(n_patients):\n",
    "            \n",
    "            # We generate the FE for the graph formation model\n",
    "            alpha_graph.append( np.random.uniform(alpha_law_graph[0], alpha_law_graph[1], size = nb_latent_factors) )\n",
    "\n",
    "        for j in range(n_doctors):\n",
    "\n",
    "            # We generate the FE for the graph formation model\n",
    "            psi_graph.append( np.random.uniform(psi_law_graph[0], psi_law_graph[1], size = nb_latent_factors ) )\n",
    "\n",
    "    # Generate distance matrix\n",
    "    if type_distance == 'default':\n",
    "        for i in range(n_patients):\n",
    "            # Generate the coordinates of the patients\n",
    "            coor_patients.append( np.random.uniform(0, 1, 2) )\n",
    "            for j in range(n_doctors):\n",
    "                if i == 0: # We ensure each coordinate is generated once for each doctor\n",
    "                    # Generate the coordinates of the doctors\n",
    "                    coor_doctors.append( np.random.uniform(0, 1, 2) )\n",
    "                d = np.sqrt(np.power((coor_patients[i][0] - coor_doctors[j][0]), 2) + np.power((coor_patients[i][1] - coor_doctors[j][1]), 2))\n",
    "                D[i][j] = d\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Assign randomly a CODGEO, DEP, or REG to patients and doctors\n",
    "        dist_matrix = pd.read_csv('../Data/' + type_distance + '.csv')\n",
    "        \n",
    "        del dist_matrix[dist_matrix.columns[0]]\n",
    "        dist_matrix.index = dist_matrix.columns\n",
    "        for i in range(len(dist_matrix)):\n",
    "            dist_matrix.iloc[i, i] = 0\n",
    "        arr = dist_matrix.columns.values\n",
    "        for i, col in enumerate(arr):\n",
    "            arr[i] = int(float(arr[i]))\n",
    "        dist_matrix.columns = arr\n",
    "        dist_matrix.index = arr\n",
    "\n",
    "        # Generate code for patient and doctor\n",
    "        code_patient = []\n",
    "        code_doctor = []\n",
    "        for i in range(n_patients):\n",
    "            random_code = np.random.choice(dist_matrix.columns.values)\n",
    "            code_patient.append( random_code )\n",
    "        for j in range(n_doctors):\n",
    "            random_code = np.random.choice(dist_matrix.columns.values)\n",
    "            code_doctor.append( random_code )\n",
    "        for i in range(n_patients):\n",
    "            for j in range(n_doctors):\n",
    "                D[i, j] = dist_matrix.loc[code_patient[i], code_doctor[j]]\n",
    "\n",
    "    # D_normed = ( D - D.mean() ) / D.std()\n",
    "    # Random draws of ages for patients and doctors\n",
    "    sim_patient_age = rng.integers(low = 1, high = 99, size = n_patients)\n",
    "    sim_patient_age_normed = ( sim_patient_age - sim_patient_age.mean() ) / sim_patient_age.std()\n",
    "    sim_doctor_age = rng.integers(low = 26, high = 99, size = n_doctors)\n",
    "    sim_doctor_age_normed = ( sim_doctor_age - sim_doctor_age.mean() ) / sim_doctor_age.std()\n",
    "\n",
    "    # Random draws of genders of patients and doctors\n",
    "    sim_patient_gender = np.random.choice(np.array([0, 1]), n_patients)\n",
    "    sim_doctor_gender = np.random.choice(np.array([0, 1]), n_doctors)\n",
    "    sim_patient_gender_normed = ( sim_patient_gender - sim_patient_gender.mean() ) / sim_patient_gender.std()\n",
    "    sim_doctor_gender_normed = ( sim_doctor_gender - sim_doctor_gender.mean() ) / sim_doctor_gender.std()\n",
    "\n",
    "    # Compile ids\n",
    "    id_p = np.repeat(range(n_patients), n_doctors)\n",
    "    id_d = np.tile(range(n_doctors), n_patients)\n",
    "\n",
    "    # Compile observed features\n",
    "    age_p_data = np.repeat(sim_patient_age, n_doctors)\n",
    "    age_d_data = np.tile(sim_doctor_age, n_patients)\n",
    "    sex_p_data = np.repeat(sim_patient_gender, n_doctors)\n",
    "    sex_d_data = np.tile(sim_doctor_gender, n_patients)\n",
    "    if type_distance != 'default':\n",
    "        code_patient_data = np.repeat(code_patient, n_doctors)\n",
    "        code_doctor_data = np.tile(code_doctor, n_patients)\n",
    "    # # P is the matrix with all the connection probabilities\n",
    "    # P = np.zeros((n_patients, n_doctors))\n",
    "    # Generate the identifier matrix A based on the distance\n",
    "    A = np.zeros([n_patients, n_doctors], dtype = np.ndarray)\n",
    "    for i in range(n_patients):\n",
    "        for j in range(n_doctors):\n",
    "            if D[i][j] <= z: # if patient i and doctor j are too far away, there is no relation\n",
    "                T = np.dot(alpha_graph[i], psi_graph[j]) + beta_age_p_graph * sim_patient_age_normed[i] + beta_age_d_graph * sim_doctor_age_normed[j] \\\n",
    "                + beta_sex_p_graph * sim_patient_gender_normed[i] + beta_sex_d_graph * sim_doctor_gender_normed[j] + beta_distance_graph * D[i][j]\n",
    "                p = 1 / (1 + np.exp(-T))\n",
    "                # P[i][j] = p\n",
    "                A[i][j] = np.random.binomial(1, p)\n",
    "    \n",
    "    # Compile relations between doctors and patients\n",
    "    relation = A.flatten()\n",
    "\n",
    "    # Merge all columns into a dataframe\n",
    "    dataframe = pd.DataFrame(data={'i': id_p, 'j': id_d, 'y' : relation, 'age_p': age_p_data, 'age_d': age_d_data, \n",
    "                           'sex_p': sex_p_data, 'sex_d': sex_d_data\n",
    "                            })\n",
    "    dataframe['distance'] = D[dataframe['i'], dataframe['j']].astype(float)\n",
    "\n",
    "    # Now, we bound the number of connections (1 <= connections <= max_number_connections)\n",
    "    # First, we detect the patients who have 0 connection.\n",
    "    number_of_connections = dataframe.groupby('i').agg({'y': 'sum'})\n",
    "    zero_connection = number_of_connections[number_of_connections['y'] == 0].index\n",
    "    for patient in zero_connection:\n",
    "        # If patient has zero connection, we connect him with the nearest doctor (even if the threshold z isn't respected)\n",
    "        min_index = dataframe[dataframe['i'] == patient]['distance'].idxmin()\n",
    "        doctor_to_connect = dataframe.loc[min_index, 'j']\n",
    "        dataframe.loc[(dataframe['i'] == patient) & (dataframe['j'] == doctor_to_connect), 'y'] = 1\n",
    "        \n",
    "    # We also detect the doctors who have 0 connection.\n",
    "    number_of_connections = dataframe.groupby('j').agg({'y': 'sum'})\n",
    "    zero_connection = number_of_connections[number_of_connections['y'] == 0].index\n",
    "    for doctor in zero_connection:\n",
    "        # If doctor has zero connection, we connect him with the nearest patient (even if the threshold z isn't respected)\n",
    "        min_index = dataframe[dataframe['j'] == doctor]['distance'].idxmin()\n",
    "        patient_to_connect = dataframe.loc[min_index, 'i']\n",
    "        dataframe.loc[(dataframe['j'] == doctor) & (dataframe['j'] == patient_to_connect), 'y'] = 1\n",
    "    \n",
    "    # Then, we detect the patients who have more than max_number_connections. We choose the remaining connections by doctor's popularities (possible to choose randomly).\n",
    "    # Note : on ne s'assure pas ici qu'un docteur pourra finir par avoir 0 connexion (cas très peu probable).\n",
    "    number_of_connections = dataframe.groupby('i').agg({'y': 'sum'})\n",
    "    too_much_connection = number_of_connections[number_of_connections['y'] > max_number_connections].index\n",
    "    for patient in too_much_connection:\n",
    "        # We keep the connections with the most popular doctors\n",
    "        patient_df = dataframe[dataframe['i'] == patient]\n",
    "        connected_doctors = patient_df[patient_df['y'] == 1]['j'].values\n",
    "        most_popular_doctors = dataframe[dataframe['j'].isin(connected_doctors)].groupby('j').agg({'y': 'sum'}).sort_values('y', ascending=False)\n",
    "        not_kept_doctors = most_popular_doctors.index[max_number_connections:].values\n",
    "        for doctor in not_kept_doctors:\n",
    "            dataframe.loc[(dataframe['i'] == patient) & (dataframe['j'] == doctor), 'y'] = 0\n",
    "\n",
    "    # Create a dataframe of fixed effects for patients and doctors, then concatenate it with all the data\n",
    "    k = len(alpha_graph[0]) # Number of latent factors\n",
    "    ef_patient = pd.DataFrame(np.zeros((n_patients*n_doctors, k)))\n",
    "    ef_doctor = pd.DataFrame(np.zeros((n_patients*n_doctors, k)))\n",
    "    for i in range(k):\n",
    "        ef_patient_element = []\n",
    "        ef_doctor_element = []\n",
    "        ef_patient.rename(columns = {i :f'ef_p_{i}'}, inplace = True)\n",
    "        ef_doctor.rename(columns = {i :f'ef_d_{i}'}, inplace = True)\n",
    "        for j in range(n_patients):\n",
    "            \n",
    "            ef_patient_element += list(np.repeat(alpha_graph[j][i], n_doctors))\n",
    "\n",
    "        for j in range(n_doctors):\n",
    "            ef_doctor_element.append(psi_graph[j][i])\n",
    "        \n",
    "        ef_patient[f'ef_p_{i}'] = ef_patient_element\n",
    "        ef_doctor[f'ef_d_{i}'] = np.tile(ef_doctor_element, n_patients)\n",
    "    dataframe = pd.concat([dataframe, ef_patient, ef_doctor], axis = 1)\n",
    "    dataframe = dataframe.reset_index().drop(['index'], axis = 1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = graph_formation(n_patients=500,\n",
    "                    n_doctors=10,\n",
    "                    max_number_connections=2,\n",
    "                    z=0.5,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.1,\n",
    "                    beta_age_d_graph=0.1,\n",
    "                    beta_sex_p_graph=0.1,\n",
    "                    beta_sex_d_graph=0.1,\n",
    "                    beta_distance_graph=-1,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = 5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    4230\n",
       "1     770\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>y</th>\n",
       "      <th>age_p</th>\n",
       "      <th>age_d</th>\n",
       "      <th>sex_p</th>\n",
       "      <th>sex_d</th>\n",
       "      <th>distance</th>\n",
       "      <th>ef_p_0</th>\n",
       "      <th>ef_p_1</th>\n",
       "      <th>ef_p_2</th>\n",
       "      <th>ef_p_3</th>\n",
       "      <th>ef_p_4</th>\n",
       "      <th>ef_d_0</th>\n",
       "      <th>ef_d_1</th>\n",
       "      <th>ef_d_2</th>\n",
       "      <th>ef_d_3</th>\n",
       "      <th>ef_d_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.586811</td>\n",
       "      <td>-0.87751</td>\n",
       "      <td>0.190578</td>\n",
       "      <td>0.353325</td>\n",
       "      <td>0.115111</td>\n",
       "      <td>0.123291</td>\n",
       "      <td>-0.735304</td>\n",
       "      <td>-0.326102</td>\n",
       "      <td>0.127537</td>\n",
       "      <td>0.437504</td>\n",
       "      <td>0.901824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.254590</td>\n",
       "      <td>-0.87751</td>\n",
       "      <td>0.190578</td>\n",
       "      <td>0.353325</td>\n",
       "      <td>0.115111</td>\n",
       "      <td>0.123291</td>\n",
       "      <td>-0.268968</td>\n",
       "      <td>-0.456837</td>\n",
       "      <td>-0.099447</td>\n",
       "      <td>-0.719060</td>\n",
       "      <td>-0.831909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.641551</td>\n",
       "      <td>-0.87751</td>\n",
       "      <td>0.190578</td>\n",
       "      <td>0.353325</td>\n",
       "      <td>0.115111</td>\n",
       "      <td>0.123291</td>\n",
       "      <td>0.787078</td>\n",
       "      <td>0.440976</td>\n",
       "      <td>0.711115</td>\n",
       "      <td>-0.606658</td>\n",
       "      <td>0.069243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.977818</td>\n",
       "      <td>-0.87751</td>\n",
       "      <td>0.190578</td>\n",
       "      <td>0.353325</td>\n",
       "      <td>0.115111</td>\n",
       "      <td>0.123291</td>\n",
       "      <td>-0.260726</td>\n",
       "      <td>0.309213</td>\n",
       "      <td>0.185001</td>\n",
       "      <td>0.163602</td>\n",
       "      <td>0.725620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.255637</td>\n",
       "      <td>-0.87751</td>\n",
       "      <td>0.190578</td>\n",
       "      <td>0.353325</td>\n",
       "      <td>0.115111</td>\n",
       "      <td>0.123291</td>\n",
       "      <td>0.661475</td>\n",
       "      <td>0.009490</td>\n",
       "      <td>-0.594460</td>\n",
       "      <td>0.681606</td>\n",
       "      <td>-0.901251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i  j  y  age_p  age_d  sex_p  sex_d  distance   ef_p_0    ef_p_1    ef_p_2  \\\n",
       "0  0  0  0     93     67      0      0  0.586811 -0.87751  0.190578  0.353325   \n",
       "1  0  1  0     93     61      0      1  0.254590 -0.87751  0.190578  0.353325   \n",
       "2  0  2  0     93     98      0      0  0.641551 -0.87751  0.190578  0.353325   \n",
       "3  0  3  0     93     74      0      1  0.977818 -0.87751  0.190578  0.353325   \n",
       "4  0  4  0     93     61      0      0  0.255637 -0.87751  0.190578  0.353325   \n",
       "\n",
       "     ef_p_3    ef_p_4    ef_d_0    ef_d_1    ef_d_2    ef_d_3    ef_d_4  \n",
       "0  0.115111  0.123291 -0.735304 -0.326102  0.127537  0.437504  0.901824  \n",
       "1  0.115111  0.123291 -0.268968 -0.456837 -0.099447 -0.719060 -0.831909  \n",
       "2  0.115111  0.123291  0.787078  0.440976  0.711115 -0.606658  0.069243  \n",
       "3  0.115111  0.123291 -0.260726  0.309213  0.185001  0.163602  0.725620  \n",
       "4  0.115111  0.123291  0.661475  0.009490 -0.594460  0.681606 -0.901251  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF + Hard Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negative_sampling_first_step(dataframe,\n",
    "                                      nb_Y = 1,\n",
    "                                     ):\n",
    "    \"\"\"à partir de notre dataframe, on doit pouvoir générer des Y = 0 pertinents. Pour cela, il faut effectuer un entrainement initial sur notre dataframe pour \n",
    "    avoir une première estimation des paramètres de notre modèle (EF, Betas) et ensuite effectuer ce HNS.\n",
    "\n",
    "    Args:\n",
    "        dataframe (_type_): Le dataframe initial sur lequel on applique la première étape de notre Hard Negative Sampling.\n",
    "        nb_Y (int, optional): Le nombre de Y=0 que l'on souhaite échantillonner. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Renvoie le dataframe échantillonné.\n",
    "    \"\"\"\n",
    "    df = dataframe.copy()\n",
    "    # Hard Negative Sampling : \n",
    "\n",
    "    # On enregistre l'ensemble des Y = 0 avant de les supprimer de notre dataframe\n",
    "    negative_connections = df[df['y'] == 0]\n",
    "    negative_patients = negative_connections['i'].unique()\n",
    "    # On supprime les Y = 0 car on va les générer avec le HNS\n",
    "    df.drop(df[df['y'] == 0].index, inplace = True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    # D'abord, on effectue l'étape 1 du HNS. Il nous faut une première estimation des EFs/Betas. On garde un Y=0 en se basant sur une distribution dépendant de la\n",
    "    # popularité/distance à chaque docteur. Pour chaque patient, on garde en mémoire le(s) Y=0 le plus intéressant.\n",
    "    negative_connections_first_step = negative_connections.copy()\n",
    "    for i in negative_patients:\n",
    "        patient_df = negative_connections_first_step[negative_connections_first_step['i'] == i]\n",
    "        distance = np.zeros(len(patient_df)) # Array pour stocker les distances. Il faut faire attention, indice de l'array =/= indice \"réel\" du docteur\n",
    "        popularity = np.zeros(len(patient_df)) # Array pour stocker les popularités\n",
    "        for j, doctor in enumerate(patient_df['j']):\n",
    "            distance[j] = patient_df[patient_df['j'] == doctor]['distance'].iloc[0]\n",
    "            popularity[j] = df[df['j'] == doctor]['y'].sum()\n",
    "        score = popularity / distance\n",
    "        score = score/score.sum()\n",
    "        \n",
    "        # On conserve les échantillons négatifs les plus significatifs (ceux pour lesquels la probabilité de connexion est la plus grande mais n'a pas eu lieu)\n",
    "        best_score_array = score.argsort()[-nb_Y:]\n",
    "        doctors_chosen = patient_df['j'].iloc[best_score_array].to_list()\n",
    "        doctors_to_eliminate = patient_df[patient_df['j'].isin([j for j in patient_df['j'] if j not in doctors_chosen])]\n",
    "        negative_connections_first_step.drop(doctors_to_eliminate.index, inplace = True)\n",
    "    # On a maintenant le Y = 0 désiré pour chaque patient, il faut fusionner les Y = 0 et Y = 1\n",
    "    return pd.concat([df, negative_connections_first_step]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_10048\\3575637160.py:2: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf # Pour les Warnings de dépréciation de méthodes\n",
    "tf.disable_eager_execution()\n",
    "# Pour arrêter la descente lorsque la loss est suffisamment faible\n",
    "class StopTrainingBelowLoss(Callback):\n",
    "    def __init__(self, target_loss):\n",
    "        super(StopTrainingBelowLoss, self).__init__()\n",
    "        self.target_loss = target_loss\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('loss') is not None and logs.get('loss') < self.target_loss:\n",
    "            print(f\"\\nTraining stopped as the loss reached below {self.target_loss}\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Récupérer la valeur de la loss (utile si on ne fixe pas de seuil à atteindre)\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super(LossHistory, self).__init__()\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "def get_estimations(df, nb_epochs=50, initial_weights=None, target_loss=None, show_print=1):\n",
    "    \"\"\"Etant donné un dataframe, get_estimations renvoie l'estimation des effets fixes et des Bêtas en utilisant TensorFlow.keras\n",
    "\n",
    "    Args:\n",
    "        df (_type_): le dataframe pour lequel on souhaite obtenir l'estimation\n",
    "        nb_epochs (int, optional): Le nombre d'itérations pour notre descente de gradient. Defaults to 50.\n",
    "        initial_weights (_type_, optional): Si on a déjà lancé un entraînement et qu'on souhaite le poursuivre, on peut rentrer l'estimation obtenue auparavant\n",
    "        (voir plus bas pour un exemple). Defaults to None.\n",
    "        target_loss (_type_, optional): Si une valeur est indiquée, la descente de gradient ne s'arrête pas tant que la loss n'est pas inférieure\n",
    "        à cette valeur. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: l'ensemble des estimations (voir plus bas pour un exemple)\n",
    "    \"\"\"\n",
    "    # Définition des dimensions\n",
    "    num_patients = df['i'].nunique()\n",
    "    num_doctors = df['j'].nunique()\n",
    "    embedding_dim = int(len(df.columns[8:])/2) # Dimension des vecteurs d'effets fixes\n",
    "\n",
    "    # Créer un LabelEncoder pour les ID des patients et des docteurs, utile si les id ne sont pas nécessairement une suite consécutive d'entiers (1, 2, ..., n)\n",
    "    patient_encoder = LabelEncoder()\n",
    "    i_encoded = patient_encoder.fit_transform(df['i'])\n",
    "\n",
    "    doctor_encoder = LabelEncoder()\n",
    "    j_encoded = doctor_encoder.fit_transform(df['j'])\n",
    "\n",
    "    # Enregistrer les mappings ID -> entier pour retrouver les bons indices associés à chaque patient/docteur si besoin\n",
    "    patient_id_mapping = dict(zip(patient_encoder.classes_, range(num_patients)))\n",
    "    doctor_id_mapping = dict(zip(doctor_encoder.classes_, range(num_doctors)))\n",
    "\n",
    "    y = df['y'].values.astype(np.float32)\n",
    "    df = df.astype(np.float32)\n",
    "    X = [i_encoded, j_encoded, df['age_p'].values, df['age_d'].values, df['sex_p'].values,\\\n",
    "            df['sex_d'].values, df['distance'].values]\n",
    "    \n",
    "    # Entrées du modèle\n",
    "    user_input = Input(shape=(1,))\n",
    "    doctor_input = Input(shape=(1,))\n",
    "    age_patient_input = Input(shape=(1,), name='age_p')\n",
    "    age_doctor_input = Input(shape=(1,), name='age_d')\n",
    "    sex_patient_input = Input(shape=(1,), name='sex_p')\n",
    "    sex_doctor_input = Input(shape=(1,), name='sex_d')\n",
    "    distance_input = Input(shape=(1,))\n",
    "\n",
    "    # Incorporation des utilisateurs et des docteurs dans des espaces latents\n",
    "    user_embedding = Embedding(input_dim=num_patients, output_dim=embedding_dim, input_length=1)(user_input)\n",
    "    doctor_embedding = Embedding(input_dim=num_doctors, output_dim=embedding_dim, input_length=1)(doctor_input)\n",
    "\n",
    "    # Obtention des vecteurs latents des utilisateurs et des docteurs\n",
    "    user_latent = Flatten()(user_embedding)\n",
    "    doctor_latent = Flatten()(doctor_embedding)\n",
    "\n",
    "    # Produit scalaire entre les vecteurs latents des utilisateurs et des docteurs\n",
    "    dot_product = Dot(axes=1)([user_latent, doctor_latent])\n",
    "\n",
    "    # Création d'une couche pour paramétriser les Beta\n",
    "    class CustomLayer(Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(CustomLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.beta_age_patient = self.add_weight(shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.beta_age_doctor = self.add_weight(shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.beta_sexe_patient = self.add_weight(shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.beta_sexe_doctor = self.add_weight(shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.beta_distance = self.add_weight(shape=(1,), initializer='random_normal', trainable=True)\n",
    "            super(CustomLayer, self).build(input_shape)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            age_patient_input, age_doctor_input, sex_patient_input, sex_doctor_input, distance_input = inputs\n",
    "            linear_term = self.beta_age_patient * age_patient_input + \\\n",
    "                        self.beta_age_doctor * age_doctor_input + \\\n",
    "                        self.beta_sexe_patient * sex_patient_input + \\\n",
    "                        self.beta_sexe_doctor * sex_doctor_input + \\\n",
    "                        self.beta_distance * distance_input\n",
    "            return linear_term\n",
    "\n",
    "    # Ajout de la couche personnalisée dans le modèle\n",
    "    linear_term = CustomLayer()([age_patient_input, age_doctor_input, sex_patient_input, sex_doctor_input, distance_input])\n",
    "    output = Add()([dot_product, linear_term])\n",
    "    output = Activation('sigmoid')(output)\n",
    "\n",
    "    # Création du modèle\n",
    "    model = Model(inputs=[user_input, doctor_input, age_patient_input, age_doctor_input, sex_patient_input, sex_doctor_input, distance_input], outputs=output)\n",
    "\n",
    "    # Compilation du modèle\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Chargement des poids initiaux si disponibles (utile pour reprendre l'entraînement)\n",
    "    if initial_weights != None:\n",
    "        model.set_weights(initial_weights)\n",
    "\n",
    "    # Ajout du rappel pour arrêter l'entraînement seulement si la perte atteint un seuil\n",
    "    if target_loss != None:\n",
    "        callbacks = []\n",
    "        callbacks.append(StopTrainingBelowLoss(target_loss))\n",
    "        # Initialisation du nombre d'époques\n",
    "        epoch = 0\n",
    "    # Boucle d'entraînement jusqu'à ce que la perte atteigne le seuil\n",
    "        while True:\n",
    "            # Entraînement d'une époque\n",
    "            model.fit(X, y, epochs=1, batch_size=64, validation_split=0.2, verbose=show_print, callbacks=callbacks)\n",
    "\n",
    "            # Incrémentation du nombre d'époques\n",
    "            epoch += 1\n",
    "\n",
    "            # Vérification si l'entraînement doit être arrêté\n",
    "            if model.stop_training:\n",
    "                break\n",
    "        return epoch, model.get_weights(), patient_id_mapping, doctor_id_mapping\n",
    "    else:\n",
    "        # Ajout du rappel pour enregistrer la perte\n",
    "        loss_history = LossHistory()\n",
    "        callbacks = [loss_history]\n",
    "        # Entraînement du modèle\n",
    "        model.fit(X, y, epochs=nb_epochs, batch_size=64, validation_split=0.2, verbose=show_print, callbacks=callbacks)\n",
    "        # model.fit(X, y, epochs=nb_epochs, batch_size=64, validation_split=0.2, verbose=0, callbacks=callbacks) # Pour ne pas afficher le print des epochs\n",
    "        loss_values = loss_history.losses\n",
    "        return loss_values, model.get_weights(), patient_id_mapping, doctor_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrouver l'ensemble des estimations\n",
    "\n",
    "# ef_patient = model.get_weights()[0]\n",
    "# ef_doctor = model.get_weights()[1]\n",
    "# beta_age_p = model.get_weights()[2]\n",
    "# beta_age_d = model.get_weights()[3]\n",
    "# beta_sex_p = model.get_weights()[4]\n",
    "# beta_sex_d = model.get_weights()[5]\n",
    "# beta_distance = model.get_weights()[6]\n",
    "# Beta = [beta_age_p, beta_age_d, beta_sex_p, beta_sex_d, beta_distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\version_utils.py:76: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:635: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_utils_v1.py:50: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 69us/sample - loss: 0.4420 - accuracy: 0.8480 - val_loss: 0.4667 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.4361 - accuracy: 0.8480 - val_loss: 0.4538 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.3975 - accuracy: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.4317 - accuracy: 0.8480 - val_loss: 0.4512 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.4277 - accuracy: 0.8480 - val_loss: 0.4479 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.4246 - accuracy: 0.8480 - val_loss: 0.4455 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.4183 - accuracy: 0.8480 - val_loss: 0.4433 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.4113 - accuracy: 0.8480 - val_loss: 0.4401 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.4038 - accuracy: 0.8480 - val_loss: 0.4371 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.3970 - accuracy: 0.8480 - val_loss: 0.4369 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 18us/sample - loss: 0.3864 - accuracy: 0.8480 - val_loss: 0.4320 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.3768 - accuracy: 0.8480 - val_loss: 0.4300 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.3667 - accuracy: 0.8480 - val_loss: 0.4284 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.3552 - accuracy: 0.8487 - val_loss: 0.4255 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.3454 - accuracy: 0.8487 - val_loss: 0.4257 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.3336 - accuracy: 0.8503 - val_loss: 0.4239 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.3234 - accuracy: 0.8550 - val_loss: 0.4227 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.3128 - accuracy: 0.8587 - val_loss: 0.4179 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.3021 - accuracy: 0.8655 - val_loss: 0.4195 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2923 - accuracy: 0.8668 - val_loss: 0.4136 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2821 - accuracy: 0.8742 - val_loss: 0.4171 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2725 - accuracy: 0.8767 - val_loss: 0.4165 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2631 - accuracy: 0.8820 - val_loss: 0.4107 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2543 - accuracy: 0.8895 - val_loss: 0.4129 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2455 - accuracy: 0.8953 - val_loss: 0.4126 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2378 - accuracy: 0.9032 - val_loss: 0.4085 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2296 - accuracy: 0.9107 - val_loss: 0.4026 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2238 - accuracy: 0.9150 - val_loss: 0.4135 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2160 - accuracy: 0.9210 - val_loss: 0.4114 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2080 - accuracy: 0.9265 - val_loss: 0.4057 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.2016 - accuracy: 0.9295 - val_loss: 0.4066 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.1957 - accuracy: 0.9380 - val_loss: 0.4111 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.1893 - accuracy: 0.9365 - val_loss: 0.4101 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.1835 - accuracy: 0.9398 - val_loss: 0.4066 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.1782 - accuracy: 0.9445 - val_loss: 0.4271 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.1733 - accuracy: 0.9417 - val_loss: 0.4024 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.1680 - accuracy: 0.9490 - val_loss: 0.4153 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.1913 - accuracy: 0.9375WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0006s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.1632 - accuracy: 0.9500 - val_loss: 0.4109 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.1581 - accuracy: 0.9530 - val_loss: 0.4180 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.1535 - accuracy: 0.9525 - val_loss: 0.4174 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.1490 - accuracy: 0.9557 - val_loss: 0.4304 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.1457 - accuracy: 0.9555 - val_loss: 0.4102 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.1408 - accuracy: 0.9588 - val_loss: 0.4302 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.1370 - accuracy: 0.9588 - val_loss: 0.4171 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.1336 - accuracy: 0.9640 - val_loss: 0.4339 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.1307 - accuracy: 0.9617 - val_loss: 0.4192 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.1708 - accuracy: 0.9375WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.1264 - accuracy: 0.9647 - val_loss: 0.4098 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.1229 - accuracy: 0.9657 - val_loss: 0.4181 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.1196 - accuracy: 0.9693 - val_loss: 0.4180 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.1165 - accuracy: 0.9710 - val_loss: 0.4311 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.1136 - accuracy: 0.9722 - val_loss: 0.4498 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.1110 - accuracy: 0.9718 - val_loss: 0.4309 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.1076 - accuracy: 0.9747 - val_loss: 0.4359 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.1053 - accuracy: 0.9740 - val_loss: 0.4131 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.1026 - accuracy: 0.9770 - val_loss: 0.4448 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0995 - accuracy: 0.9775 - val_loss: 0.4561 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0972 - accuracy: 0.9790 - val_loss: 0.4431 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0949 - accuracy: 0.9808 - val_loss: 0.4524 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0925 - accuracy: 0.9793 - val_loss: 0.4424 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0901 - accuracy: 0.9815 - val_loss: 0.4541 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0879 - accuracy: 0.9818 - val_loss: 0.4607 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0858 - accuracy: 0.9827 - val_loss: 0.4635 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0835 - accuracy: 0.9835 - val_loss: 0.4677 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0817 - accuracy: 0.9833 - val_loss: 0.4431 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0555 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_begin` time: 0.0007s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0799 - accuracy: 0.9843 - val_loss: 0.4757 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0645 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0775 - accuracy: 0.9855 - val_loss: 0.4564 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0761 - accuracy: 0.9860 - val_loss: 0.4794 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0742 - accuracy: 0.9862 - val_loss: 0.4594 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0728 - accuracy: 0.9870 - val_loss: 0.4790 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0707 - accuracy: 0.9875 - val_loss: 0.4518 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0690 - accuracy: 0.9877 - val_loss: 0.4737 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0675 - accuracy: 0.9872 - val_loss: 0.4765 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0659 - accuracy: 0.9883 - val_loss: 0.4674 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0643 - accuracy: 0.9887 - val_loss: 0.4904 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0629 - accuracy: 0.9890 - val_loss: 0.4871 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0614 - accuracy: 0.9890 - val_loss: 0.5135 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0604 - accuracy: 0.9893 - val_loss: 0.4861 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0591 - accuracy: 0.9890 - val_loss: 0.4836 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0577 - accuracy: 0.9895 - val_loss: 0.5371 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0566 - accuracy: 0.9890 - val_loss: 0.4978 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0552 - accuracy: 0.9893 - val_loss: 0.5188 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0538 - accuracy: 0.9895 - val_loss: 0.4942 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0526 - accuracy: 0.9895 - val_loss: 0.5314 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0517 - accuracy: 0.9895 - val_loss: 0.5153 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0508 - accuracy: 0.9898 - val_loss: 0.5025 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0493 - accuracy: 0.9895 - val_loss: 0.5122 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0483 - accuracy: 0.9895 - val_loss: 0.5116 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0474 - accuracy: 0.9898 - val_loss: 0.5368 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 28us/sample - loss: 0.0466 - accuracy: 0.9895 - val_loss: 0.5429 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0454 - accuracy: 0.9898 - val_loss: 0.5442 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0468 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0445 - accuracy: 0.9900 - val_loss: 0.5499 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0440 - accuracy: 0.9900 - val_loss: 0.5388 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0428 - accuracy: 0.9898 - val_loss: 0.5148 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0422 - accuracy: 0.9900 - val_loss: 0.5681 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0409 - accuracy: 0.9902 - val_loss: 0.5608 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0404 - accuracy: 0.9902 - val_loss: 0.5358 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0394 - accuracy: 0.9908 - val_loss: 0.5946 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0388 - accuracy: 0.9905 - val_loss: 0.5677 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0378 - accuracy: 0.9908 - val_loss: 0.5417 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0372 - accuracy: 0.9910 - val_loss: 0.5723 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0366 - accuracy: 0.9912 - val_loss: 0.5691 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0356 - accuracy: 0.9920 - val_loss: 0.5681 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0351 - accuracy: 0.9920 - val_loss: 0.5949 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0341 - accuracy: 0.9927 - val_loss: 0.6175 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0337 - accuracy: 0.9918 - val_loss: 0.5806 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0413 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0006s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0330 - accuracy: 0.9933 - val_loss: 0.5744 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0324 - accuracy: 0.9933 - val_loss: 0.5692 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0320 - accuracy: 0.9933 - val_loss: 0.5826 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0254 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0311 - accuracy: 0.9935 - val_loss: 0.5833 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0306 - accuracy: 0.9940 - val_loss: 0.5875 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0299 - accuracy: 0.9942 - val_loss: 0.6039 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.5986 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 38us/sample - loss: 0.0290 - accuracy: 0.9948 - val_loss: 0.5971 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0284 - accuracy: 0.9945 - val_loss: 0.5867 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0276 - accuracy: 0.9958 - val_loss: 0.6473 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0273 - accuracy: 0.9955 - val_loss: 0.6041 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0240 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0267 - accuracy: 0.9950 - val_loss: 0.6473 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0263 - accuracy: 0.9960 - val_loss: 0.6248 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0259 - accuracy: 0.9955 - val_loss: 0.6487 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0256 - accuracy: 0.9960 - val_loss: 0.5990 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0247 - accuracy: 0.9958 - val_loss: 0.6270 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0244 - accuracy: 0.9960 - val_loss: 0.6100 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0240 - accuracy: 0.9965 - val_loss: 0.6436 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0235 - accuracy: 0.9965 - val_loss: 0.6811 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0232 - accuracy: 0.9967 - val_loss: 0.6291 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0227 - accuracy: 0.9965 - val_loss: 0.6964 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0225 - accuracy: 0.9962 - val_loss: 0.6413 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0219 - accuracy: 0.9973 - val_loss: 0.6586 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0302 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0215 - accuracy: 0.9970 - val_loss: 0.6943 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0146 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0212 - accuracy: 0.9970 - val_loss: 0.6628 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0207 - accuracy: 0.9967 - val_loss: 0.6710 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0203 - accuracy: 0.9975 - val_loss: 0.6696 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.0202 - accuracy: 0.9975 - val_loss: 0.6845 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0197 - accuracy: 0.9973 - val_loss: 0.6810 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0214 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0194 - accuracy: 0.9973 - val_loss: 0.6693 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0209 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0008s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0190 - accuracy: 0.9975 - val_loss: 0.7081 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0187 - accuracy: 0.9975 - val_loss: 0.7097 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0185 - accuracy: 0.9980 - val_loss: 0.6806 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0179 - accuracy: 0.9977 - val_loss: 0.7091 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0177 - accuracy: 0.9977 - val_loss: 0.7046 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0174 - accuracy: 0.9985 - val_loss: 0.7033 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0232 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0171 - accuracy: 0.9975 - val_loss: 0.6914 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0170 - accuracy: 0.9985 - val_loss: 0.6618 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0166 - accuracy: 0.9987 - val_loss: 0.7097 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0163 - accuracy: 0.9987 - val_loss: 0.7060 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0159 - accuracy: 0.9987 - val_loss: 0.7234 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0156 - accuracy: 0.9985 - val_loss: 0.6847 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0154 - accuracy: 0.9990 - val_loss: 0.7448 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0151 - accuracy: 0.9992 - val_loss: 0.7280 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0149 - accuracy: 0.9990 - val_loss: 0.7458 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0146 - accuracy: 0.9992 - val_loss: 0.7245 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0144 - accuracy: 0.9990 - val_loss: 0.7469 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0141 - accuracy: 0.9990 - val_loss: 0.7462 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0138 - accuracy: 0.9992 - val_loss: 0.7432 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0253 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0008s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0136 - accuracy: 0.9992 - val_loss: 0.7506 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.0133 - accuracy: 0.9992 - val_loss: 0.7715 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0132 - accuracy: 0.9992 - val_loss: 0.7388 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0129 - accuracy: 0.9992 - val_loss: 0.7413 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0127 - accuracy: 0.9992 - val_loss: 0.7500 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0123 - accuracy: 0.9992 - val_loss: 0.8165 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.0122 - accuracy: 0.9992 - val_loss: 0.7214 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0171 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0121 - accuracy: 0.9992 - val_loss: 0.7435 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0118 - accuracy: 0.9992 - val_loss: 0.7854 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0116 - accuracy: 0.9995 - val_loss: 0.7546 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0114 - accuracy: 0.9992 - val_loss: 0.7489 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0171 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.0113 - accuracy: 0.9992 - val_loss: 0.7988 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0110 - accuracy: 0.9995 - val_loss: 0.7620 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0109 - accuracy: 0.9995 - val_loss: 0.8067 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0107 - accuracy: 0.9995 - val_loss: 0.8042 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0104 - accuracy: 0.9995 - val_loss: 0.8530 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0133 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0002s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0103 - accuracy: 0.9995 - val_loss: 0.8264 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 19us/sample - loss: 0.0101 - accuracy: 0.9995 - val_loss: 0.8350 - val_accuracy: 0.8380\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "3328/4000 [=======================>......] - ETA: 0s - loss: 0.0102 - accuracy: 0.9994\n",
      "Training stopped as the loss reached below 0.01\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.0099 - accuracy: 0.9995 - val_loss: 0.8147 - val_accuracy: 0.8380\n"
     ]
    }
   ],
   "source": [
    "results = get_estimations(df, initial_weights=None, target_loss=0.01)\n",
    "# results = get_estimations(df, nb_epochs=50, initial_weights=None, target_loss=None) target_loss l'emporte sur nb_epochs, càd que si on fixe une loss à atteindre, le nombre d'itérations fixé\n",
    "# n'aura plus d'importance car on va faire autant d'itérations que nécessaire pour atteindre une loss faible.\n",
    "# results = get_estimations(df, nb_epochs=50, initial_weights=None, target_loss=0.01) -> on ne fait pas 50 epochs mais autant d'epochs que nécessaire pour avoir une loss <= 0.01\n",
    "\n",
    "ef_patient = results[1][0]\n",
    "ef_doctor = results[1][1]\n",
    "beta_age_p = results[1][2]\n",
    "beta_age_d = results[1][3]\n",
    "beta_sex_p = results[1][4]\n",
    "beta_sex_d = results[1][5]\n",
    "beta_distance = results[1][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = graph_formation(n_patients=1000,\n",
    "                    n_doctors=15,\n",
    "                    max_number_connections=2,\n",
    "                    z=0.5,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.1,\n",
    "                    beta_age_d_graph=0.1,\n",
    "                    beta_sex_p_graph=0.1,\n",
    "                    beta_sex_d_graph=0.1,\n",
    "                    beta_distance_graph=-1,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = 5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HNS_and_estimates(dataframe,\n",
    "                      M,\n",
    "                      nb_Y = 1,\n",
    "                      epochs=50,\n",
    "                      weights=None,\n",
    "                      threshold_loss=0.01,\n",
    "                      show_print=1,\n",
    "                      ):\n",
    "    \"\"\"Cette fonction effectue le Hard Negative Sampling (HNS) dans sa globalité et renvoie l'estimation finale des effets fixes, betas.\n",
    "\n",
    "    Args:\n",
    "        dataframe (_type_): Le dataframe initial\n",
    "        M (_type_): Le nombre associé au Top-M docteurs (le paramètre de notre HNS permettant de tirer aléatoirement parmi\n",
    "        les M docteurs les plus relevants, cf. papier de recherche associé)\n",
    "        nb_Y (int, optional): Le paramètre Y associé à la première étape du HNS. Defaults to 1.\n",
    "        epochs (int, optional): paramètre de get_estimations. Defaults to 50.\n",
    "        weights (_type_, optional): paramètre de get_estimations. Defaults to None.\n",
    "        threshold_loss (float, optional): paramètre de get_estimations. Defaults to 0.01.\n",
    "\n",
    "    Returns:\n",
    "        _type_: l'ensemble des estimations.\n",
    "    \"\"\"\n",
    "    df = dataframe.copy()\n",
    "    n_patients = dataframe['i'].nunique()\n",
    "    n_doctors = dataframe['j'].nunique()\n",
    "    # On récupère le dataframe ayant subi la première étape du HNS\n",
    "    df_first_hns = hard_negative_sampling_first_step(df, nb_Y)\n",
    "    \n",
    "    # On utilise get_first_estimations qui s'occupe d'effectuer la première estimation des EF/Bêtas.\n",
    "    parameters = get_estimations(df_first_hns, nb_epochs=epochs, initial_weights=weights, target_loss=threshold_loss, show_print=show_print)\n",
    "\n",
    "    # parameters[2] contient l'ensemble des premières estimations\n",
    "\n",
    "    # Maintenant, on estime les scores en se servant de la première estimation ci-dessus (https://arxiv.org/abs/2302.03472)\n",
    "    alpha_graph_training = parameters[1][0]\n",
    "    psi_graph_training = parameters[1][1]\n",
    "    beta_age_p_graph_training = parameters[1][2]\n",
    "    beta_age_d_graph_training = parameters[1][3]\n",
    "    beta_sex_p_graph_training = parameters[1][4]\n",
    "    beta_sex_d_graph_training = parameters[1][5]\n",
    "    beta_distance_graph_training =  parameters[1][6]\n",
    "    prediction_scores = np.zeros((n_patients, n_doctors))\n",
    "    negative_drawing_list = []\n",
    "    # Les scores estimés se basent sur notre modèle de LMF (https://arxiv.org/abs/2302.03472)\n",
    "    negative_connections = df[df['y'] == 0]\n",
    "    negative_patients = negative_connections['i'].unique()\n",
    "    highest_prediction_scores_indexes = np.zeros((len(negative_patients), M))\n",
    "    # Pour calculer les scores, il faut au préalable récupérer les âges et sexes normalisés.\n",
    "\n",
    "    df['age_p_normed'] = ( df['age_p'] - df['age_p'].mean() ) / df['age_p'].std()\n",
    "    df['age_d_normed'] = ( df['age_d'] - df['age_d'].mean() ) / df['age_d'].std()\n",
    "    df['sex_p_normed'] = ( df['sex_p'] - df['sex_p'].mean() ) / df['sex_p'].std()\n",
    "    df['sex_d_normed'] = ( df['sex_d'] - df['sex_d'].mean() ) / df['sex_d'].std()\n",
    "\n",
    "    for i in negative_patients:\n",
    "        patient_df = negative_connections[negative_connections['i'] == i]\n",
    "        patient_age_normed = df[df['i'] == i]['age_p_normed'].iloc[0]\n",
    "        patient_sex_normed = df[df['i'] == i]['sex_p_normed'].iloc[0]\n",
    "        # On considère les docteurs pour lesquels le patient i n'a pas de connexions avec\n",
    "        for j in patient_df['j']:\n",
    "            # on récupère la distance entre le patient i et le docteur j\n",
    "            distance = patient_df[patient_df['j'] == j]['distance'].iloc[0]\n",
    "            doctor_age_normed = df[df['j'] == j]['age_d_normed'].iloc[0]\n",
    "            doctor_sex_normed =df[df['j'] == j]['sex_d_normed'].iloc[0]\n",
    "            T = np.dot(alpha_graph_training[i], psi_graph_training[j]) + beta_age_p_graph_training * patient_age_normed + beta_age_d_graph_training * doctor_age_normed \\\n",
    "            + beta_sex_p_graph_training * patient_sex_normed + beta_sex_d_graph_training * doctor_sex_normed + beta_distance_graph_training * distance\n",
    "            prediction_scores[i][j] = 1 / (1 + np.exp(-T))\n",
    "        # on garde les M indices des docteurs avec la plus grande probabilité de connexion pour le patient i dans highest_prediction_scores_indexes\n",
    "        highest_prediction_scores_indexes[i] = np.argsort(prediction_scores[i])[-M:]\n",
    "        # on effectue le negative sampling\n",
    "        negative_drawing_patient = []\n",
    "        # on détermine les y = 0 à rajouter à notre dataframe\n",
    "        negative_drawing = np.random.binomial(1, p=1/M, size=M)\n",
    "        for l in range(M):\n",
    "            if negative_drawing[l] == 1:\n",
    "                doc = highest_prediction_scores_indexes[i][l]\n",
    "                negative_drawing_patient.append(patient_df[patient_df['j'] == doc].iloc[0].to_list())\n",
    "        negative_drawing_list += negative_drawing_patient\n",
    "    # Une fois les connexions négatives récupérées pour chaque patient, on concatène nos Y = 1 et Y = 0\n",
    "    positive_connections = dataframe.drop(dataframe[dataframe['y'] == 0].index)\n",
    "    positive_connections = positive_connections.reset_index(drop=True)\n",
    "    df_second_hns = pd.DataFrame(negative_drawing_list)\n",
    "    df_second_hns.columns = dataframe.columns\n",
    "    df_second_hns = pd.concat([positive_connections, df_second_hns], axis = 0)\n",
    "    df_second_hns = df_second_hns.reset_index(drop=True)\n",
    "    \n",
    "    # On effectue maintenant l'estimation finale des EFs/Betas encore une fois à l'aide de Keras mais cette fois-ci sur notre second dataframe\n",
    "    final_parameters = get_estimations(df_second_hns, nb_epochs=epochs, initial_weights=weights, target_loss=threshold_loss, show_print=show_print)\n",
    "\n",
    "    return final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 96us/sample - loss: 0.5942 - accuracy: 0.6998 - val_loss: 1.9315 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.5188 - accuracy: 0.8067 - val_loss: 1.5497 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.5140 - accuracy: 0.8067 - val_loss: 1.5328 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.5304 - accuracy: 0.7969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2255/2255 [==============================] - 0s 21us/sample - loss: 0.5141 - accuracy: 0.8067 - val_loss: 1.5116 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 34us/sample - loss: 0.5110 - accuracy: 0.8067 - val_loss: 1.5462 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 22us/sample - loss: 0.5084 - accuracy: 0.8067 - val_loss: 1.5259 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 24us/sample - loss: 0.5035 - accuracy: 0.8067 - val_loss: 1.6712 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 25us/sample - loss: 0.5003 - accuracy: 0.8067 - val_loss: 1.5468 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 25us/sample - loss: 0.4933 - accuracy: 0.8067 - val_loss: 1.4885 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.4858 - accuracy: 0.8067 - val_loss: 1.5345 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.3607 - accuracy: 0.8906WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.4764 - accuracy: 0.8067 - val_loss: 1.4432 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.4674 - accuracy: 0.8067 - val_loss: 1.4271 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.4569 - accuracy: 0.8067 - val_loss: 1.5819 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.4454 - accuracy: 0.8067 - val_loss: 1.4452 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.4332 - accuracy: 0.8067 - val_loss: 1.4843 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.4221 - accuracy: 0.8067 - val_loss: 1.5448 - val_accuracy: 0.0000e+00\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.4079 - accuracy: 0.8071 - val_loss: 1.4337 - val_accuracy: 0.0018\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.3941 - accuracy: 0.8084 - val_loss: 1.4250 - val_accuracy: 0.0035\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.3809 - accuracy: 0.8115 - val_loss: 1.4071 - val_accuracy: 0.0035\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.4048 - accuracy: 0.7812WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.3675 - accuracy: 0.8155 - val_loss: 1.3400 - val_accuracy: 0.0071\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.3543 - accuracy: 0.8200 - val_loss: 1.3484 - val_accuracy: 0.0106\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.3395 - accuracy: 0.8253 - val_loss: 1.3166 - val_accuracy: 0.0177\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.3251 - accuracy: 0.8377 - val_loss: 1.4245 - val_accuracy: 0.0160\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.3132 - accuracy: 0.8510 - val_loss: 1.4284 - val_accuracy: 0.0195\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.2517 - accuracy: 0.9219WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.2981 - accuracy: 0.8581 - val_loss: 1.2854 - val_accuracy: 0.0408\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.2858 - accuracy: 0.8772 - val_loss: 1.3221 - val_accuracy: 0.0426\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.2733 - accuracy: 0.8891 - val_loss: 1.2992 - val_accuracy: 0.0745\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.2613 - accuracy: 0.9082 - val_loss: 1.4544 - val_accuracy: 0.0443\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.2888 - accuracy: 0.8438WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.2504 - accuracy: 0.9149 - val_loss: 1.3615 - val_accuracy: 0.0833\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.2375 - accuracy: 0.9370 - val_loss: 1.4162 - val_accuracy: 0.0798\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.2275 - accuracy: 0.9432 - val_loss: 1.4076 - val_accuracy: 0.0957\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.2161 - accuracy: 0.9499 - val_loss: 1.2615 - val_accuracy: 0.1489\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.2066 - accuracy: 0.9676 - val_loss: 1.2637 - val_accuracy: 0.1596\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.1964 - accuracy: 0.9690 - val_loss: 1.3568 - val_accuracy: 0.1294\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.1879 - accuracy: 0.9743 - val_loss: 1.2326 - val_accuracy: 0.1862\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.1792 - accuracy: 0.9814 - val_loss: 1.3026 - val_accuracy: 0.1720\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.1704 - accuracy: 0.9792 - val_loss: 1.2107 - val_accuracy: 0.2145\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.1632 - accuracy: 0.9885 - val_loss: 1.3970 - val_accuracy: 0.1631\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.1553 - accuracy: 0.9863 - val_loss: 1.3940 - val_accuracy: 0.1667\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.1478 - accuracy: 0.9907 - val_loss: 1.2808 - val_accuracy: 0.2128\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.1332 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.1414 - accuracy: 0.9929 - val_loss: 1.3314 - val_accuracy: 0.1986\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.1352 - accuracy: 0.9925 - val_loss: 1.3888 - val_accuracy: 0.1862\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.1291 - accuracy: 0.9916 - val_loss: 1.2835 - val_accuracy: 0.2287\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.1236 - accuracy: 0.9938 - val_loss: 1.2814 - val_accuracy: 0.2305\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 31us/sample - loss: 0.1180 - accuracy: 0.9942 - val_loss: 1.4171 - val_accuracy: 0.1986\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.1130 - accuracy: 0.9947 - val_loss: 1.2820 - val_accuracy: 0.2429\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.1082 - accuracy: 0.9947 - val_loss: 1.4106 - val_accuracy: 0.2145\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.1037 - accuracy: 0.9947 - val_loss: 1.3824 - val_accuracy: 0.2252\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0993 - accuracy: 0.9951 - val_loss: 1.4282 - val_accuracy: 0.2199\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0953 - accuracy: 0.9951 - val_loss: 1.4030 - val_accuracy: 0.2287\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0914 - accuracy: 0.9951 - val_loss: 1.4182 - val_accuracy: 0.2287\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 21us/sample - loss: 0.0878 - accuracy: 0.9947 - val_loss: 1.3344 - val_accuracy: 0.2518\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 21us/sample - loss: 0.0847 - accuracy: 0.9956 - val_loss: 1.4317 - val_accuracy: 0.2358\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0812 - accuracy: 0.9951 - val_loss: 1.3923 - val_accuracy: 0.2482\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.0927 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0008s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 31us/sample - loss: 0.0780 - accuracy: 0.9951 - val_loss: 1.4910 - val_accuracy: 0.2270\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.0715 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0749 - accuracy: 0.9956 - val_loss: 1.4173 - val_accuracy: 0.2500\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0719 - accuracy: 0.9960 - val_loss: 1.5038 - val_accuracy: 0.2323\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0695 - accuracy: 0.9951 - val_loss: 1.3773 - val_accuracy: 0.2553\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0670 - accuracy: 0.9960 - val_loss: 1.3954 - val_accuracy: 0.2553\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0645 - accuracy: 0.9965 - val_loss: 1.4789 - val_accuracy: 0.2465\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0621 - accuracy: 0.9973 - val_loss: 1.4929 - val_accuracy: 0.2465\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0600 - accuracy: 0.9969 - val_loss: 1.4250 - val_accuracy: 0.2571\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0578 - accuracy: 0.9969 - val_loss: 1.4794 - val_accuracy: 0.2518\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0558 - accuracy: 0.9969 - val_loss: 1.4644 - val_accuracy: 0.2535\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.0465 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 21us/sample - loss: 0.0539 - accuracy: 0.9973 - val_loss: 1.4606 - val_accuracy: 0.2553\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 23us/sample - loss: 0.0521 - accuracy: 0.9973 - val_loss: 1.4619 - val_accuracy: 0.2589\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 21us/sample - loss: 0.0503 - accuracy: 0.9973 - val_loss: 1.4376 - val_accuracy: 0.2695\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0488 - accuracy: 0.9987 - val_loss: 1.5518 - val_accuracy: 0.2500\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0471 - accuracy: 0.9973 - val_loss: 1.5203 - val_accuracy: 0.2535\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0457 - accuracy: 0.9973 - val_loss: 1.5506 - val_accuracy: 0.2518\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 34us/sample - loss: 0.0442 - accuracy: 0.9982 - val_loss: 1.5707 - val_accuracy: 0.2518\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0429 - accuracy: 0.9978 - val_loss: 1.4596 - val_accuracy: 0.2748\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0414 - accuracy: 0.9987 - val_loss: 1.5513 - val_accuracy: 0.2606\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0402 - accuracy: 0.9982 - val_loss: 1.5637 - val_accuracy: 0.2624\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0391 - accuracy: 0.9982 - val_loss: 1.5931 - val_accuracy: 0.2606\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0379 - accuracy: 0.9987 - val_loss: 1.5154 - val_accuracy: 0.2695\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0366 - accuracy: 0.9978 - val_loss: 1.5273 - val_accuracy: 0.2695\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0356 - accuracy: 0.9987 - val_loss: 1.6092 - val_accuracy: 0.2642\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0346 - accuracy: 0.9987 - val_loss: 1.4991 - val_accuracy: 0.2748\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 21us/sample - loss: 0.0336 - accuracy: 0.9991 - val_loss: 1.5704 - val_accuracy: 0.2660\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 26us/sample - loss: 0.0326 - accuracy: 0.9991 - val_loss: 1.6429 - val_accuracy: 0.2642\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0318 - accuracy: 0.9982 - val_loss: 1.5857 - val_accuracy: 0.2660\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0308 - accuracy: 0.9991 - val_loss: 1.5555 - val_accuracy: 0.2730\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0299 - accuracy: 0.9991 - val_loss: 1.5804 - val_accuracy: 0.2695\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0291 - accuracy: 0.9991 - val_loss: 1.6326 - val_accuracy: 0.2660\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0282 - accuracy: 0.9991 - val_loss: 1.5781 - val_accuracy: 0.2748\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 21us/sample - loss: 0.0276 - accuracy: 0.9991 - val_loss: 1.6138 - val_accuracy: 0.2677\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0268 - accuracy: 0.9991 - val_loss: 1.6201 - val_accuracy: 0.2677\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0260 - accuracy: 0.9991 - val_loss: 1.6290 - val_accuracy: 0.2695\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0253 - accuracy: 0.9987 - val_loss: 1.5905 - val_accuracy: 0.2819\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0247 - accuracy: 0.9991 - val_loss: 1.7364 - val_accuracy: 0.2660\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.0189 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0241 - accuracy: 0.9991 - val_loss: 1.5635 - val_accuracy: 0.2890\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0236 - accuracy: 0.9991 - val_loss: 1.6670 - val_accuracy: 0.2695\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0228 - accuracy: 0.9991 - val_loss: 1.6774 - val_accuracy: 0.2677\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0223 - accuracy: 0.9991 - val_loss: 1.7089 - val_accuracy: 0.2660\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0217 - accuracy: 0.9991 - val_loss: 1.6310 - val_accuracy: 0.2837\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0211 - accuracy: 0.9991 - val_loss: 1.7341 - val_accuracy: 0.2677\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0206 - accuracy: 0.9991 - val_loss: 1.6744 - val_accuracy: 0.2801\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0200 - accuracy: 0.9991 - val_loss: 1.6922 - val_accuracy: 0.2748\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 35us/sample - loss: 0.0196 - accuracy: 0.9991 - val_loss: 1.6687 - val_accuracy: 0.2837\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 21us/sample - loss: 0.0191 - accuracy: 0.9991 - val_loss: 1.6788 - val_accuracy: 0.2819\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0187 - accuracy: 0.9996 - val_loss: 1.7241 - val_accuracy: 0.2748\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0182 - accuracy: 0.9991 - val_loss: 1.6552 - val_accuracy: 0.2890\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.0195 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0177 - accuracy: 0.9991 - val_loss: 1.7061 - val_accuracy: 0.2819\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0173 - accuracy: 0.9996 - val_loss: 1.7670 - val_accuracy: 0.2730\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0169 - accuracy: 0.9991 - val_loss: 1.6739 - val_accuracy: 0.2890\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0165 - accuracy: 0.9996 - val_loss: 1.7138 - val_accuracy: 0.2837\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0161 - accuracy: 0.9996 - val_loss: 1.7322 - val_accuracy: 0.2801\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0157 - accuracy: 0.9996 - val_loss: 1.7345 - val_accuracy: 0.2837\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 22us/sample - loss: 0.0153 - accuracy: 0.9996 - val_loss: 1.7459 - val_accuracy: 0.2801\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0150 - accuracy: 0.9996 - val_loss: 1.8126 - val_accuracy: 0.2730\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0146 - accuracy: 0.9996 - val_loss: 1.7511 - val_accuracy: 0.2837\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0143 - accuracy: 0.9996 - val_loss: 1.7346 - val_accuracy: 0.2872\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0139 - accuracy: 0.9996 - val_loss: 1.7728 - val_accuracy: 0.2837\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0137 - accuracy: 0.9996 - val_loss: 1.8183 - val_accuracy: 0.2766\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0133 - accuracy: 0.9996 - val_loss: 1.7958 - val_accuracy: 0.2819\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0130 - accuracy: 0.9996 - val_loss: 1.7981 - val_accuracy: 0.2819\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0128 - accuracy: 0.9996 - val_loss: 1.8158 - val_accuracy: 0.2801\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0124 - accuracy: 0.9996 - val_loss: 1.7816 - val_accuracy: 0.2855\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0121 - accuracy: 0.9996 - val_loss: 1.8195 - val_accuracy: 0.2819\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 25us/sample - loss: 0.0118 - accuracy: 0.9996 - val_loss: 1.8228 - val_accuracy: 0.2819\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.0074 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 22us/sample - loss: 0.0116 - accuracy: 0.9996 - val_loss: 1.8256 - val_accuracy: 0.2819\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 34us/sample - loss: 0.0113 - accuracy: 0.9996 - val_loss: 1.8517 - val_accuracy: 0.2801\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0002s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0002s). Check your callbacks.\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0110 - accuracy: 0.9996 - val_loss: 1.8615 - val_accuracy: 0.2801\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0108 - accuracy: 0.9996 - val_loss: 1.8327 - val_accuracy: 0.2855\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 19us/sample - loss: 0.0105 - accuracy: 0.9996 - val_loss: 1.8734 - val_accuracy: 0.2801\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 20us/sample - loss: 0.0103 - accuracy: 0.9996 - val_loss: 1.8580 - val_accuracy: 0.2837\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0101 - accuracy: 0.9996 - val_loss: 1.8658 - val_accuracy: 0.2837\n",
      "Train on 2255 samples, validate on 564 samples\n",
      "  64/2255 [..............................] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
      "Training stopped as the loss reached below 0.01\n",
      "2255/2255 [==============================] - 0s 18us/sample - loss: 0.0099 - accuracy: 0.9996 - val_loss: 1.8623 - val_accuracy: 0.2855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_10048\\1060379516.py:67: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  prediction_scores[i][j] = 1 / (1 + np.exp(-T))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 99us/sample - loss: 0.4809 - accuracy: 0.7951 - val_loss: 1.6772 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 31us/sample - loss: 0.4644 - accuracy: 0.8049 - val_loss: 1.4004 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.4602 - accuracy: 0.8049 - val_loss: 1.5203 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.4571 - accuracy: 0.8049 - val_loss: 1.5759 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.4543 - accuracy: 0.8049 - val_loss: 1.5176 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.4506 - accuracy: 0.8049 - val_loss: 1.5529 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.4468 - accuracy: 0.8049 - val_loss: 1.6466 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.4409 - accuracy: 0.8049 - val_loss: 1.4746 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.4353 - accuracy: 0.8049 - val_loss: 1.3663 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.4294 - accuracy: 0.8049 - val_loss: 1.3728 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.4200 - accuracy: 0.8049 - val_loss: 1.4810 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.4613 - accuracy: 0.7812WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.4120 - accuracy: 0.8071 - val_loss: 1.3287 - val_accuracy: 0.0053\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.4038 - accuracy: 0.8080 - val_loss: 1.5339 - val_accuracy: 0.0000e+00\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.3944 - accuracy: 0.8102 - val_loss: 1.3843 - val_accuracy: 0.0071\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.3825 - accuracy: 0.8106 - val_loss: 1.3380 - val_accuracy: 0.0141\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.3725 - accuracy: 0.8173 - val_loss: 1.4184 - val_accuracy: 0.0106\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.3623 - accuracy: 0.8199 - val_loss: 1.3484 - val_accuracy: 0.0265\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.3508 - accuracy: 0.8323 - val_loss: 1.2049 - val_accuracy: 0.0830\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.3400 - accuracy: 0.8434 - val_loss: 1.2610 - val_accuracy: 0.0760\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.3288 - accuracy: 0.8553 - val_loss: 1.2607 - val_accuracy: 0.0830\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.3181 - accuracy: 0.8704 - val_loss: 1.4121 - val_accuracy: 0.0424\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.3077 - accuracy: 0.8783 - val_loss: 1.2600 - val_accuracy: 0.0936\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.2964 - accuracy: 0.8934 - val_loss: 1.3576 - val_accuracy: 0.0689\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.2865 - accuracy: 0.8916 - val_loss: 1.2394 - val_accuracy: 0.1095\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.2762 - accuracy: 0.9013 - val_loss: 1.3423 - val_accuracy: 0.0866\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.2665 - accuracy: 0.9058 - val_loss: 1.2379 - val_accuracy: 0.1325\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.2570 - accuracy: 0.9080 - val_loss: 1.1999 - val_accuracy: 0.1502\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.2474 - accuracy: 0.9173 - val_loss: 1.2111 - val_accuracy: 0.1519\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.3699 - accuracy: 0.8125WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.2385 - accuracy: 0.9190 - val_loss: 1.2637 - val_accuracy: 0.1396\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.1906 - accuracy: 0.9375WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.2293 - accuracy: 0.9235 - val_loss: 1.0961 - val_accuracy: 0.2350\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.2215 - accuracy: 0.9279 - val_loss: 1.1437 - val_accuracy: 0.2085\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.2135 - accuracy: 0.9327 - val_loss: 1.1503 - val_accuracy: 0.2173\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.2057 - accuracy: 0.9407 - val_loss: 1.1930 - val_accuracy: 0.2014\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.1987 - accuracy: 0.9412 - val_loss: 1.0602 - val_accuracy: 0.2986\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.1918 - accuracy: 0.9478 - val_loss: 1.1561 - val_accuracy: 0.2403\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.1847 - accuracy: 0.9487 - val_loss: 1.1626 - val_accuracy: 0.2456\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.1785 - accuracy: 0.9566 - val_loss: 1.1582 - val_accuracy: 0.2527\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.1731 - accuracy: 0.9580 - val_loss: 1.1089 - val_accuracy: 0.2968\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.1667 - accuracy: 0.9575 - val_loss: 1.1059 - val_accuracy: 0.3039\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.1613 - accuracy: 0.9606 - val_loss: 1.1282 - val_accuracy: 0.2951\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.1562 - accuracy: 0.9659 - val_loss: 1.1985 - val_accuracy: 0.2686\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.1508 - accuracy: 0.9628 - val_loss: 1.1404 - val_accuracy: 0.3074\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 25us/sample - loss: 0.1461 - accuracy: 0.9686 - val_loss: 1.1639 - val_accuracy: 0.2986\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 24us/sample - loss: 0.1414 - accuracy: 0.9699 - val_loss: 1.0989 - val_accuracy: 0.3445\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 22us/sample - loss: 0.1373 - accuracy: 0.9686 - val_loss: 1.0570 - val_accuracy: 0.3728\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 29us/sample - loss: 0.1332 - accuracy: 0.9730 - val_loss: 1.1629 - val_accuracy: 0.3163\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.1286 - accuracy: 0.9752 - val_loss: 1.1144 - val_accuracy: 0.3445\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.1247 - accuracy: 0.9748 - val_loss: 1.0413 - val_accuracy: 0.4064\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.1219 - accuracy: 0.9792 - val_loss: 1.1326 - val_accuracy: 0.3516\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.1176 - accuracy: 0.9792 - val_loss: 1.2194 - val_accuracy: 0.3057\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.1139 - accuracy: 0.9788 - val_loss: 1.1011 - val_accuracy: 0.3869\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.1111 - accuracy: 0.9810 - val_loss: 1.1373 - val_accuracy: 0.3746\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.1077 - accuracy: 0.9823 - val_loss: 1.1118 - val_accuracy: 0.3922\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 39us/sample - loss: 0.1046 - accuracy: 0.9836 - val_loss: 1.1585 - val_accuracy: 0.3675\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.1016 - accuracy: 0.9841 - val_loss: 1.1623 - val_accuracy: 0.3746\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0988 - accuracy: 0.9858 - val_loss: 1.1569 - val_accuracy: 0.3781\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.1321 - accuracy: 0.9688WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.0963 - accuracy: 0.9841 - val_loss: 1.1698 - val_accuracy: 0.3763\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.0935 - accuracy: 0.9863 - val_loss: 1.1949 - val_accuracy: 0.3640\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0910 - accuracy: 0.9876 - val_loss: 1.1915 - val_accuracy: 0.3763\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0895 - accuracy: 0.9863 - val_loss: 1.2314 - val_accuracy: 0.3516\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0862 - accuracy: 0.9885 - val_loss: 1.2029 - val_accuracy: 0.3781\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0842 - accuracy: 0.9876 - val_loss: 1.1696 - val_accuracy: 0.3993\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0821 - accuracy: 0.9876 - val_loss: 1.1408 - val_accuracy: 0.4152\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0798 - accuracy: 0.9894 - val_loss: 1.2445 - val_accuracy: 0.3675\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0783 - accuracy: 0.9885 - val_loss: 1.1186 - val_accuracy: 0.4382\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0763 - accuracy: 0.9889 - val_loss: 1.1469 - val_accuracy: 0.4293\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0740 - accuracy: 0.9889 - val_loss: 1.2159 - val_accuracy: 0.3958\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0724 - accuracy: 0.9903 - val_loss: 1.1815 - val_accuracy: 0.4117\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0707 - accuracy: 0.9903 - val_loss: 1.1552 - val_accuracy: 0.4311\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0690 - accuracy: 0.9903 - val_loss: 1.2489 - val_accuracy: 0.3887\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0679 - accuracy: 0.9903 - val_loss: 1.2038 - val_accuracy: 0.4152\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0662 - accuracy: 0.9916 - val_loss: 1.2541 - val_accuracy: 0.3922\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0642 - accuracy: 0.9907 - val_loss: 1.1570 - val_accuracy: 0.4346\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.0781 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0627 - accuracy: 0.9907 - val_loss: 1.2225 - val_accuracy: 0.4134\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0613 - accuracy: 0.9907 - val_loss: 1.2314 - val_accuracy: 0.4170\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.0510 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0599 - accuracy: 0.9912 - val_loss: 1.2349 - val_accuracy: 0.4187\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.0781 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0586 - accuracy: 0.9907 - val_loss: 1.3059 - val_accuracy: 0.3887\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.0501 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0575 - accuracy: 0.9920 - val_loss: 1.1172 - val_accuracy: 0.4594\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0560 - accuracy: 0.9916 - val_loss: 1.2507 - val_accuracy: 0.4187\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0548 - accuracy: 0.9920 - val_loss: 1.2614 - val_accuracy: 0.4187\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0536 - accuracy: 0.9916 - val_loss: 1.1883 - val_accuracy: 0.4452\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0527 - accuracy: 0.9929 - val_loss: 1.2063 - val_accuracy: 0.4435\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.0374 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0512 - accuracy: 0.9934 - val_loss: 1.3217 - val_accuracy: 0.4011\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0502 - accuracy: 0.9925 - val_loss: 1.2010 - val_accuracy: 0.4452\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0492 - accuracy: 0.9929 - val_loss: 1.2709 - val_accuracy: 0.4205\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0482 - accuracy: 0.9938 - val_loss: 1.2994 - val_accuracy: 0.4117\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0471 - accuracy: 0.9938 - val_loss: 1.2005 - val_accuracy: 0.4470\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0464 - accuracy: 0.9947 - val_loss: 1.2514 - val_accuracy: 0.4382\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0454 - accuracy: 0.9938 - val_loss: 1.2775 - val_accuracy: 0.4293\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0448 - accuracy: 0.9938 - val_loss: 1.2271 - val_accuracy: 0.4452\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0436 - accuracy: 0.9938 - val_loss: 1.2817 - val_accuracy: 0.4293\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0426 - accuracy: 0.9942 - val_loss: 1.2485 - val_accuracy: 0.4417\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0418 - accuracy: 0.9938 - val_loss: 1.2243 - val_accuracy: 0.4488\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0412 - accuracy: 0.9942 - val_loss: 1.2146 - val_accuracy: 0.4523\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0402 - accuracy: 0.9942 - val_loss: 1.2618 - val_accuracy: 0.4417\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0395 - accuracy: 0.9942 - val_loss: 1.1949 - val_accuracy: 0.4664\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0388 - accuracy: 0.9942 - val_loss: 1.2644 - val_accuracy: 0.4417\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0379 - accuracy: 0.9947 - val_loss: 1.2910 - val_accuracy: 0.4364\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0372 - accuracy: 0.9947 - val_loss: 1.2526 - val_accuracy: 0.4505\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.0412 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0366 - accuracy: 0.9951 - val_loss: 1.2918 - val_accuracy: 0.4417\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0358 - accuracy: 0.9947 - val_loss: 1.3095 - val_accuracy: 0.4399\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.0351 - accuracy: 0.9947 - val_loss: 1.2515 - val_accuracy: 0.4558\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 22us/sample - loss: 0.0346 - accuracy: 0.9947 - val_loss: 1.2590 - val_accuracy: 0.4558\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 22us/sample - loss: 0.0340 - accuracy: 0.9947 - val_loss: 1.2661 - val_accuracy: 0.4576\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0334 - accuracy: 0.9942 - val_loss: 1.2690 - val_accuracy: 0.4576\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 22us/sample - loss: 0.0332 - accuracy: 0.9951 - val_loss: 1.3237 - val_accuracy: 0.4417\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 35us/sample - loss: 0.0321 - accuracy: 0.9947 - val_loss: 1.2848 - val_accuracy: 0.4558\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0316 - accuracy: 0.9947 - val_loss: 1.3140 - val_accuracy: 0.4452\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0310 - accuracy: 0.9947 - val_loss: 1.3259 - val_accuracy: 0.4452\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0306 - accuracy: 0.9947 - val_loss: 1.3217 - val_accuracy: 0.4488\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0300 - accuracy: 0.9947 - val_loss: 1.2439 - val_accuracy: 0.4753\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0296 - accuracy: 0.9951 - val_loss: 1.2733 - val_accuracy: 0.4629\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0292 - accuracy: 0.9951 - val_loss: 1.3723 - val_accuracy: 0.4382\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0285 - accuracy: 0.9947 - val_loss: 1.2729 - val_accuracy: 0.4682\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0280 - accuracy: 0.9951 - val_loss: 1.3031 - val_accuracy: 0.4558\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0275 - accuracy: 0.9947 - val_loss: 1.2924 - val_accuracy: 0.4629\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0270 - accuracy: 0.9947 - val_loss: 1.2971 - val_accuracy: 0.4647\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0266 - accuracy: 0.9947 - val_loss: 1.3448 - val_accuracy: 0.4541\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 21us/sample - loss: 0.0261 - accuracy: 0.9956 - val_loss: 1.3333 - val_accuracy: 0.4558\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0259 - accuracy: 0.9947 - val_loss: 1.2625 - val_accuracy: 0.4806\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.0342 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0253 - accuracy: 0.9956 - val_loss: 1.3429 - val_accuracy: 0.4523\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.0226 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0002s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0250 - accuracy: 0.9947 - val_loss: 1.3112 - val_accuracy: 0.4647\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0245 - accuracy: 0.9956 - val_loss: 1.3729 - val_accuracy: 0.4523\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0241 - accuracy: 0.9947 - val_loss: 1.2936 - val_accuracy: 0.4735\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0239 - accuracy: 0.9956 - val_loss: 1.3073 - val_accuracy: 0.4700\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0233 - accuracy: 0.9951 - val_loss: 1.3463 - val_accuracy: 0.4629\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0230 - accuracy: 0.9951 - val_loss: 1.3662 - val_accuracy: 0.4576\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0229 - accuracy: 0.9951 - val_loss: 1.2587 - val_accuracy: 0.4947\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0223 - accuracy: 0.9951 - val_loss: 1.3105 - val_accuracy: 0.4735\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0220 - accuracy: 0.9956 - val_loss: 1.3931 - val_accuracy: 0.4523\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0216 - accuracy: 0.9956 - val_loss: 1.3339 - val_accuracy: 0.4682\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0212 - accuracy: 0.9956 - val_loss: 1.3673 - val_accuracy: 0.4629\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0209 - accuracy: 0.9956 - val_loss: 1.3309 - val_accuracy: 0.4700\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0205 - accuracy: 0.9960 - val_loss: 1.4072 - val_accuracy: 0.4523\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0204 - accuracy: 0.9960 - val_loss: 1.3896 - val_accuracy: 0.4594\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0200 - accuracy: 0.9951 - val_loss: 1.3739 - val_accuracy: 0.4629\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0196 - accuracy: 0.9960 - val_loss: 1.3185 - val_accuracy: 0.4859\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0193 - accuracy: 0.9956 - val_loss: 1.3355 - val_accuracy: 0.4788\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0190 - accuracy: 0.9969 - val_loss: 1.4052 - val_accuracy: 0.4594\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0190 - accuracy: 0.9956 - val_loss: 1.3428 - val_accuracy: 0.4753\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0185 - accuracy: 0.9956 - val_loss: 1.2698 - val_accuracy: 0.5053\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0183 - accuracy: 0.9965 - val_loss: 1.4307 - val_accuracy: 0.4541\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0181 - accuracy: 0.9960 - val_loss: 1.2414 - val_accuracy: 0.5159\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 20us/sample - loss: 0.0175 - accuracy: 0.9973 - val_loss: 1.4518 - val_accuracy: 0.4558\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0175 - accuracy: 0.9969 - val_loss: 1.3116 - val_accuracy: 0.5018\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 33us/sample - loss: 0.0170 - accuracy: 0.9973 - val_loss: 1.4059 - val_accuracy: 0.4664\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0168 - accuracy: 0.9969 - val_loss: 1.3134 - val_accuracy: 0.5000\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0165 - accuracy: 0.9973 - val_loss: 1.3684 - val_accuracy: 0.4788\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0164 - accuracy: 0.9965 - val_loss: 1.3255 - val_accuracy: 0.5000\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0160 - accuracy: 0.9969 - val_loss: 1.3212 - val_accuracy: 0.5018\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0159 - accuracy: 0.9969 - val_loss: 1.3076 - val_accuracy: 0.5053\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0156 - accuracy: 0.9969 - val_loss: 1.3182 - val_accuracy: 0.5035\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0155 - accuracy: 0.9973 - val_loss: 1.2823 - val_accuracy: 0.5159\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0151 - accuracy: 0.9973 - val_loss: 1.3779 - val_accuracy: 0.4823\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0149 - accuracy: 0.9978 - val_loss: 1.3368 - val_accuracy: 0.5000\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0146 - accuracy: 0.9973 - val_loss: 1.3506 - val_accuracy: 0.4965\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0145 - accuracy: 0.9978 - val_loss: 1.3941 - val_accuracy: 0.4806\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0144 - accuracy: 0.9978 - val_loss: 1.4026 - val_accuracy: 0.4806\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0140 - accuracy: 0.9978 - val_loss: 1.3117 - val_accuracy: 0.5124\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0138 - accuracy: 0.9973 - val_loss: 1.3962 - val_accuracy: 0.4841\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0135 - accuracy: 0.9973 - val_loss: 1.3643 - val_accuracy: 0.4965\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0135 - accuracy: 0.9978 - val_loss: 1.3076 - val_accuracy: 0.5159\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0131 - accuracy: 0.9978 - val_loss: 1.3575 - val_accuracy: 0.5053\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0129 - accuracy: 0.9978 - val_loss: 1.4153 - val_accuracy: 0.4823\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0128 - accuracy: 0.9978 - val_loss: 1.3354 - val_accuracy: 0.5141\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0125 - accuracy: 0.9978 - val_loss: 1.3809 - val_accuracy: 0.4929\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0123 - accuracy: 0.9978 - val_loss: 1.3381 - val_accuracy: 0.5141\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0121 - accuracy: 0.9978 - val_loss: 1.3500 - val_accuracy: 0.5106\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0119 - accuracy: 0.9978 - val_loss: 1.3728 - val_accuracy: 0.5035\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0118 - accuracy: 0.9982 - val_loss: 1.3555 - val_accuracy: 0.5088\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0116 - accuracy: 0.9978 - val_loss: 1.3601 - val_accuracy: 0.5106\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0114 - accuracy: 0.9982 - val_loss: 1.3337 - val_accuracy: 0.5177\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0112 - accuracy: 0.9982 - val_loss: 1.3657 - val_accuracy: 0.5088\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0111 - accuracy: 0.9982 - val_loss: 1.3926 - val_accuracy: 0.5035\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0109 - accuracy: 0.9982 - val_loss: 1.3546 - val_accuracy: 0.5141\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0107 - accuracy: 0.9982 - val_loss: 1.3689 - val_accuracy: 0.5106\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0106 - accuracy: 0.9982 - val_loss: 1.2835 - val_accuracy: 0.5389\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 19us/sample - loss: 0.0103 - accuracy: 0.9982 - val_loss: 1.4452 - val_accuracy: 0.4894\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0103 - accuracy: 0.9982 - val_loss: 1.3117 - val_accuracy: 0.5353\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "2260/2260 [==============================] - 0s 32us/sample - loss: 0.0100 - accuracy: 0.9982 - val_loss: 1.3661 - val_accuracy: 0.5177\n",
      "Train on 2260 samples, validate on 566 samples\n",
      "  64/2260 [..............................] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
      "Training stopped as the loss reached below 0.01\n",
      "2260/2260 [==============================] - 0s 18us/sample - loss: 0.0099 - accuracy: 0.9982 - val_loss: 1.3940 - val_accuracy: 0.5071\n"
     ]
    }
   ],
   "source": [
    "estimates = HNS_and_estimates(df2, M=2)\n",
    "ef_patient = estimates[1][0]\n",
    "ef_doctor = estimates[1][1]\n",
    "beta_age_p = estimates[1][2]\n",
    "beta_age_d = estimates[1][3]\n",
    "beta_sex_p = estimates[1][4]\n",
    "beta_sex_d = estimates[1][5]\n",
    "beta_distance = estimates[1][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métrique Top-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = graph_formation(n_patients=2000,\n",
    "                    n_doctors=30,\n",
    "                    max_number_connections=2,\n",
    "                    z=0.5,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.1,\n",
    "                    beta_age_d_graph=0.1,\n",
    "                    beta_sex_p_graph=0.1,\n",
    "                    beta_sex_d_graph=0.1,\n",
    "                    beta_distance_graph=-1,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = 5\n",
    "                    )\n",
    "theoric_betas = [0.1, 0.1, 0.1, 0.1, -0.5]\n",
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hns = hard_negative_sampling_first_step(df2, nb_Y=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hns['j'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 65us/sample - loss: 0.4969 - accuracy: 0.8240 - val_loss: 1.8365 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.4599 - accuracy: 0.8290 - val_loss: 1.6899 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.4581 - accuracy: 0.8290 - val_loss: 1.7811 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "  64/4749 [..............................] - ETA: 0s - loss: 0.3985 - accuracy: 0.8594WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0004s). Check your callbacks.\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.4542 - accuracy: 0.8290 - val_loss: 1.8062 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.4480 - accuracy: 0.8290 - val_loss: 1.7231 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.4396 - accuracy: 0.8290 - val_loss: 1.6852 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "  64/4749 [..............................] - ETA: 0s - loss: 0.5302 - accuracy: 0.7969WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.4284 - accuracy: 0.8290 - val_loss: 1.7038 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.4145 - accuracy: 0.8290 - val_loss: 1.5842 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.3997 - accuracy: 0.8290 - val_loss: 1.5891 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "  64/4749 [..............................] - ETA: 0s - loss: 0.4378 - accuracy: 0.8281WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0002s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0002s). Check your callbacks.\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.3815 - accuracy: 0.8290 - val_loss: 1.5567 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.3631 - accuracy: 0.8292 - val_loss: 1.6996 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.3445 - accuracy: 0.8313 - val_loss: 1.4749 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 26us/sample - loss: 0.3251 - accuracy: 0.8385 - val_loss: 1.5793 - val_accuracy: 0.0000e+00\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 27us/sample - loss: 0.3054 - accuracy: 0.8511 - val_loss: 1.4985 - val_accuracy: 0.0025\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.2857 - accuracy: 0.8741 - val_loss: 1.4646 - val_accuracy: 0.0076\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.2676 - accuracy: 0.8945 - val_loss: 1.4902 - val_accuracy: 0.0143\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.2506 - accuracy: 0.9105 - val_loss: 1.5269 - val_accuracy: 0.0202\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.2330 - accuracy: 0.9210 - val_loss: 1.4723 - val_accuracy: 0.0421\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.2167 - accuracy: 0.9305 - val_loss: 1.4594 - val_accuracy: 0.0606\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.2028 - accuracy: 0.9396 - val_loss: 1.4203 - val_accuracy: 0.0926\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.1874 - accuracy: 0.9482 - val_loss: 1.3687 - val_accuracy: 0.1347\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.1748 - accuracy: 0.9530 - val_loss: 1.3687 - val_accuracy: 0.1498\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.1627 - accuracy: 0.9600 - val_loss: 1.4694 - val_accuracy: 0.1347\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.1513 - accuracy: 0.9640 - val_loss: 1.5125 - val_accuracy: 0.1364\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.1410 - accuracy: 0.9672 - val_loss: 1.4387 - val_accuracy: 0.1768\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.1321 - accuracy: 0.9709 - val_loss: 1.3931 - val_accuracy: 0.2003\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.1235 - accuracy: 0.9802 - val_loss: 1.5551 - val_accuracy: 0.1633\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.1145 - accuracy: 0.9817 - val_loss: 1.4287 - val_accuracy: 0.2197\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.1077 - accuracy: 0.9846 - val_loss: 1.5156 - val_accuracy: 0.1987\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.1001 - accuracy: 0.9869 - val_loss: 1.4589 - val_accuracy: 0.2323\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0939 - accuracy: 0.9897 - val_loss: 1.4596 - val_accuracy: 0.2391\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0879 - accuracy: 0.9931 - val_loss: 1.5993 - val_accuracy: 0.2113\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0825 - accuracy: 0.9941 - val_loss: 1.5432 - val_accuracy: 0.2374\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0773 - accuracy: 0.9945 - val_loss: 1.6008 - val_accuracy: 0.2315\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0726 - accuracy: 0.9952 - val_loss: 1.6336 - val_accuracy: 0.2306\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0682 - accuracy: 0.9966 - val_loss: 1.6433 - val_accuracy: 0.2382\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0640 - accuracy: 0.9979 - val_loss: 1.6908 - val_accuracy: 0.2348\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0605 - accuracy: 0.9979 - val_loss: 1.5911 - val_accuracy: 0.2576\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0568 - accuracy: 0.9977 - val_loss: 1.6125 - val_accuracy: 0.2584\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0539 - accuracy: 0.9992 - val_loss: 1.6971 - val_accuracy: 0.2492\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0506 - accuracy: 0.9996 - val_loss: 1.7079 - val_accuracy: 0.2508\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0478 - accuracy: 0.9996 - val_loss: 1.7202 - val_accuracy: 0.2534\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0452 - accuracy: 0.9994 - val_loss: 1.7076 - val_accuracy: 0.2618\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0427 - accuracy: 0.9996 - val_loss: 1.7104 - val_accuracy: 0.2660\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0406 - accuracy: 0.9996 - val_loss: 1.8238 - val_accuracy: 0.2517\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0382 - accuracy: 0.9996 - val_loss: 1.7779 - val_accuracy: 0.2626\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0362 - accuracy: 0.9996 - val_loss: 1.7080 - val_accuracy: 0.2744\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0345 - accuracy: 0.9996 - val_loss: 1.8219 - val_accuracy: 0.2635\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0326 - accuracy: 0.9996 - val_loss: 1.9183 - val_accuracy: 0.2517\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0309 - accuracy: 0.9996 - val_loss: 1.8676 - val_accuracy: 0.2635\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 21us/sample - loss: 0.0293 - accuracy: 0.9996 - val_loss: 1.8267 - val_accuracy: 0.2719\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0279 - accuracy: 0.9996 - val_loss: 1.8748 - val_accuracy: 0.2677\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0265 - accuracy: 0.9996 - val_loss: 1.8224 - val_accuracy: 0.2753\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0253 - accuracy: 0.9996 - val_loss: 1.8822 - val_accuracy: 0.2727\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0240 - accuracy: 0.9996 - val_loss: 1.9245 - val_accuracy: 0.2710\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 26us/sample - loss: 0.0228 - accuracy: 0.9996 - val_loss: 1.8895 - val_accuracy: 0.2736\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0217 - accuracy: 0.9996 - val_loss: 1.9818 - val_accuracy: 0.2702\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0207 - accuracy: 0.9996 - val_loss: 1.9882 - val_accuracy: 0.2702\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0198 - accuracy: 0.9996 - val_loss: 2.0890 - val_accuracy: 0.2618\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "  64/4749 [..............................] - ETA: 0s - loss: 0.0241 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0188 - accuracy: 0.9996 - val_loss: 2.1435 - val_accuracy: 0.2576\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0180 - accuracy: 0.9996 - val_loss: 2.0328 - val_accuracy: 0.2719\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0171 - accuracy: 0.9996 - val_loss: 2.0342 - val_accuracy: 0.2736\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0164 - accuracy: 0.9996 - val_loss: 2.0572 - val_accuracy: 0.2727\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0157 - accuracy: 0.9996 - val_loss: 2.0804 - val_accuracy: 0.2727\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0149 - accuracy: 0.9996 - val_loss: 2.0947 - val_accuracy: 0.2736\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0143 - accuracy: 0.9996 - val_loss: 2.1392 - val_accuracy: 0.2694\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0136 - accuracy: 0.9996 - val_loss: 2.1588 - val_accuracy: 0.2694\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0131 - accuracy: 0.9996 - val_loss: 2.1574 - val_accuracy: 0.2702\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "  64/4749 [..............................] - ETA: 0s - loss: 0.0119 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_begin` time: 0.0003s). Check your callbacks.\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0125 - accuracy: 0.9996 - val_loss: 2.0935 - val_accuracy: 0.2786\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0120 - accuracy: 0.9996 - val_loss: 2.1398 - val_accuracy: 0.2769\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0114 - accuracy: 0.9996 - val_loss: 2.1724 - val_accuracy: 0.2719\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0109 - accuracy: 0.9996 - val_loss: 2.2741 - val_accuracy: 0.2660\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0105 - accuracy: 0.9996 - val_loss: 2.2717 - val_accuracy: 0.2668\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "4749/4749 [==============================] - 0s 20us/sample - loss: 0.0100 - accuracy: 0.9996 - val_loss: 2.1982 - val_accuracy: 0.2769\n",
      "Train on 4749 samples, validate on 1188 samples\n",
      "3200/4749 [===================>..........] - ETA: 0s - loss: 0.0097 - accuracy: 0.9994\n",
      "Training stopped as the loss reached below 0.01\n",
      "4749/4749 [==============================] - 0s 19us/sample - loss: 0.0096 - accuracy: 0.9996 - val_loss: 2.3575 - val_accuracy: 0.2618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_10048\\1060379516.py:67: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  prediction_scores[i][j] = 1 / (1 + np.exp(-T))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 69us/sample - loss: 0.9397 - accuracy: 0.8292 - val_loss: 1.9520 - val_accuracy: 0.0000e+00\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.4753 - accuracy: 0.8292 - val_loss: 1.6351 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.4679 - accuracy: 0.8292 - val_loss: 1.7411 - val_accuracy: 0.0000e+00\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.4632 - accuracy: 0.8292 - val_loss: 1.5839 - val_accuracy: 0.0000e+00\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.4582 - accuracy: 0.8292 - val_loss: 1.6038 - val_accuracy: 0.0000e+00\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.4491 - accuracy: 0.8292 - val_loss: 1.7368 - val_accuracy: 0.0000e+00\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "  64/4748 [..............................] - ETA: 0s - loss: 0.4436 - accuracy: 0.8281WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.4381 - accuracy: 0.8292 - val_loss: 1.6035 - val_accuracy: 0.0000e+00\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "  64/4748 [..............................] - ETA: 0s - loss: 0.4487 - accuracy: 0.8125WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0004s vs `on_train_batch_end` time: 0.0004s). Check your callbacks.\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.4248 - accuracy: 0.8292 - val_loss: 1.4903 - val_accuracy: 0.0000e+00\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.4102 - accuracy: 0.8292 - val_loss: 1.5061 - val_accuracy: 0.0000e+00\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.3939 - accuracy: 0.8292 - val_loss: 1.4825 - val_accuracy: 0.0017\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.3757 - accuracy: 0.8302 - val_loss: 1.4942 - val_accuracy: 0.0034\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 21us/sample - loss: 0.3569 - accuracy: 0.8336 - val_loss: 1.4041 - val_accuracy: 0.0126\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.3397 - accuracy: 0.8389 - val_loss: 1.4654 - val_accuracy: 0.0143\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 21us/sample - loss: 0.3206 - accuracy: 0.8463 - val_loss: 1.4502 - val_accuracy: 0.0194\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.3030 - accuracy: 0.8549 - val_loss: 1.4110 - val_accuracy: 0.0286\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.2860 - accuracy: 0.8635 - val_loss: 1.3913 - val_accuracy: 0.0370\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.2697 - accuracy: 0.8749 - val_loss: 1.3051 - val_accuracy: 0.0640\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.2528 - accuracy: 0.8833 - val_loss: 1.2516 - val_accuracy: 0.1019\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 27us/sample - loss: 0.2387 - accuracy: 0.9031 - val_loss: 1.2345 - val_accuracy: 0.1288\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 21us/sample - loss: 0.2243 - accuracy: 0.9187 - val_loss: 1.3604 - val_accuracy: 0.0985\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.2112 - accuracy: 0.9267 - val_loss: 1.2271 - val_accuracy: 0.1709\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.1996 - accuracy: 0.9374 - val_loss: 1.3576 - val_accuracy: 0.1431\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.1876 - accuracy: 0.9414 - val_loss: 1.3519 - val_accuracy: 0.1633\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.1767 - accuracy: 0.9528 - val_loss: 1.3115 - val_accuracy: 0.1919\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.1674 - accuracy: 0.9606 - val_loss: 1.3847 - val_accuracy: 0.1768\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.1578 - accuracy: 0.9634 - val_loss: 1.2500 - val_accuracy: 0.2374\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.1496 - accuracy: 0.9671 - val_loss: 1.2827 - val_accuracy: 0.2357\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "  64/4748 [..............................] - ETA: 0s - loss: 0.0891 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0005s). Check your callbacks.\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.1415 - accuracy: 0.9751 - val_loss: 1.3440 - val_accuracy: 0.2231\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.1346 - accuracy: 0.9754 - val_loss: 1.3219 - val_accuracy: 0.2492\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.1275 - accuracy: 0.9796 - val_loss: 1.3955 - val_accuracy: 0.2315\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.1213 - accuracy: 0.9802 - val_loss: 1.3316 - val_accuracy: 0.2702\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.1152 - accuracy: 0.9813 - val_loss: 1.2320 - val_accuracy: 0.3350\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.1098 - accuracy: 0.9838 - val_loss: 1.2643 - val_accuracy: 0.3300\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.1044 - accuracy: 0.9861 - val_loss: 1.3986 - val_accuracy: 0.2761\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0995 - accuracy: 0.9859 - val_loss: 1.3231 - val_accuracy: 0.3266\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0950 - accuracy: 0.9863 - val_loss: 1.3039 - val_accuracy: 0.3535\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0910 - accuracy: 0.9895 - val_loss: 1.5090 - val_accuracy: 0.2668\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0871 - accuracy: 0.9895 - val_loss: 1.4355 - val_accuracy: 0.3047\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0833 - accuracy: 0.9888 - val_loss: 1.3419 - val_accuracy: 0.3670\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0796 - accuracy: 0.9907 - val_loss: 1.3623 - val_accuracy: 0.3687\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0761 - accuracy: 0.9918 - val_loss: 1.3867 - val_accuracy: 0.3662\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0730 - accuracy: 0.9924 - val_loss: 1.4684 - val_accuracy: 0.3359\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0701 - accuracy: 0.9926 - val_loss: 1.3938 - val_accuracy: 0.3813\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0671 - accuracy: 0.9937 - val_loss: 1.5228 - val_accuracy: 0.3316\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 21us/sample - loss: 0.0645 - accuracy: 0.9937 - val_loss: 1.5041 - val_accuracy: 0.3485\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 21us/sample - loss: 0.0620 - accuracy: 0.9945 - val_loss: 1.4187 - val_accuracy: 0.3965\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0597 - accuracy: 0.9945 - val_loss: 1.3440 - val_accuracy: 0.4318\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0578 - accuracy: 0.9945 - val_loss: 1.5059 - val_accuracy: 0.3763\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0554 - accuracy: 0.9949 - val_loss: 1.6015 - val_accuracy: 0.3443\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0533 - accuracy: 0.9956 - val_loss: 1.5537 - val_accuracy: 0.3737\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0513 - accuracy: 0.9958 - val_loss: 1.4635 - val_accuracy: 0.4158\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0494 - accuracy: 0.9958 - val_loss: 1.5401 - val_accuracy: 0.3864\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0476 - accuracy: 0.9956 - val_loss: 1.5320 - val_accuracy: 0.3998\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0458 - accuracy: 0.9966 - val_loss: 1.4362 - val_accuracy: 0.4352\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0444 - accuracy: 0.9968 - val_loss: 1.4888 - val_accuracy: 0.4251\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0428 - accuracy: 0.9971 - val_loss: 1.5334 - val_accuracy: 0.4200\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0412 - accuracy: 0.9975 - val_loss: 1.4407 - val_accuracy: 0.4503\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0401 - accuracy: 0.9975 - val_loss: 1.5627 - val_accuracy: 0.4200\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0387 - accuracy: 0.9971 - val_loss: 1.6288 - val_accuracy: 0.4007\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0372 - accuracy: 0.9975 - val_loss: 1.6947 - val_accuracy: 0.3838\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0364 - accuracy: 0.9979 - val_loss: 1.6535 - val_accuracy: 0.4024\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0350 - accuracy: 0.9979 - val_loss: 1.4844 - val_accuracy: 0.4579\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0340 - accuracy: 0.9987 - val_loss: 1.6465 - val_accuracy: 0.4184\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0327 - accuracy: 0.9989 - val_loss: 1.7433 - val_accuracy: 0.3880\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0318 - accuracy: 0.9979 - val_loss: 1.5803 - val_accuracy: 0.4419\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0309 - accuracy: 0.9979 - val_loss: 1.6879 - val_accuracy: 0.4209\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0303 - accuracy: 0.9975 - val_loss: 1.6266 - val_accuracy: 0.4327\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0289 - accuracy: 0.9983 - val_loss: 1.6917 - val_accuracy: 0.4234\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0282 - accuracy: 0.9987 - val_loss: 1.6505 - val_accuracy: 0.4386\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 26us/sample - loss: 0.0271 - accuracy: 0.9987 - val_loss: 1.6723 - val_accuracy: 0.4360\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0263 - accuracy: 0.9992 - val_loss: 1.6059 - val_accuracy: 0.4554\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0256 - accuracy: 0.9987 - val_loss: 1.7212 - val_accuracy: 0.4285\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0249 - accuracy: 0.9989 - val_loss: 1.7530 - val_accuracy: 0.4242\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0241 - accuracy: 0.9987 - val_loss: 1.8032 - val_accuracy: 0.4150\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0235 - accuracy: 0.9987 - val_loss: 1.7828 - val_accuracy: 0.4251\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0227 - accuracy: 0.9989 - val_loss: 1.6069 - val_accuracy: 0.4705\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0221 - accuracy: 0.9989 - val_loss: 1.6474 - val_accuracy: 0.4621\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0215 - accuracy: 0.9992 - val_loss: 1.7921 - val_accuracy: 0.4310\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0208 - accuracy: 0.9992 - val_loss: 1.6634 - val_accuracy: 0.4655\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 34us/sample - loss: 0.0204 - accuracy: 0.9992 - val_loss: 1.7080 - val_accuracy: 0.4579\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 22us/sample - loss: 0.0198 - accuracy: 0.9992 - val_loss: 1.7660 - val_accuracy: 0.4478\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 22us/sample - loss: 0.0192 - accuracy: 0.9989 - val_loss: 1.7230 - val_accuracy: 0.4596\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 21us/sample - loss: 0.0187 - accuracy: 0.9992 - val_loss: 1.8951 - val_accuracy: 0.4242\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 21us/sample - loss: 0.0181 - accuracy: 0.9996 - val_loss: 1.8609 - val_accuracy: 0.4301\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0176 - accuracy: 0.9994 - val_loss: 1.6893 - val_accuracy: 0.4731\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0171 - accuracy: 0.9996 - val_loss: 1.6487 - val_accuracy: 0.4832\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0167 - accuracy: 0.9996 - val_loss: 1.8446 - val_accuracy: 0.4444\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 29us/sample - loss: 0.0163 - accuracy: 0.9996 - val_loss: 1.8259 - val_accuracy: 0.4537\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 26us/sample - loss: 0.0159 - accuracy: 0.9994 - val_loss: 1.8630 - val_accuracy: 0.4461\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 21us/sample - loss: 0.0154 - accuracy: 0.9994 - val_loss: 1.7120 - val_accuracy: 0.4798\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0151 - accuracy: 0.9994 - val_loss: 1.7843 - val_accuracy: 0.4680\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0146 - accuracy: 0.9996 - val_loss: 1.8563 - val_accuracy: 0.4588\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0142 - accuracy: 0.9994 - val_loss: 1.8203 - val_accuracy: 0.4680\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0139 - accuracy: 0.9994 - val_loss: 1.7908 - val_accuracy: 0.4722\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0134 - accuracy: 0.9996 - val_loss: 1.7710 - val_accuracy: 0.4790\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 37us/sample - loss: 0.0132 - accuracy: 0.9994 - val_loss: 1.8368 - val_accuracy: 0.4680\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 21us/sample - loss: 0.0131 - accuracy: 0.9998 - val_loss: 1.9110 - val_accuracy: 0.4596\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0125 - accuracy: 0.9996 - val_loss: 1.9482 - val_accuracy: 0.4545\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0122 - accuracy: 0.9996 - val_loss: 1.8378 - val_accuracy: 0.4731\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0120 - accuracy: 0.9996 - val_loss: 1.8241 - val_accuracy: 0.4773\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "  64/4748 [..............................] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0116 - accuracy: 0.9996 - val_loss: 1.8793 - val_accuracy: 0.4697\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 27us/sample - loss: 0.0113 - accuracy: 0.9996 - val_loss: 1.9430 - val_accuracy: 0.4655\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0109 - accuracy: 0.9998 - val_loss: 1.8565 - val_accuracy: 0.4773\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 19us/sample - loss: 0.0108 - accuracy: 0.9996 - val_loss: 1.8587 - val_accuracy: 0.4790\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0105 - accuracy: 0.9996 - val_loss: 1.8718 - val_accuracy: 0.4790\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0107 - accuracy: 0.9996 - val_loss: 2.0329 - val_accuracy: 0.4537\n",
      "Train on 4748 samples, validate on 1188 samples\n",
      "3200/4748 [===================>..........] - ETA: 0s - loss: 0.0101 - accuracy: 0.9991\n",
      "Training stopped as the loss reached below 0.01\n",
      "4748/4748 [==============================] - 0s 20us/sample - loss: 0.0100 - accuracy: 0.9994 - val_loss: 2.0165 - val_accuracy: 0.4621\n"
     ]
    }
   ],
   "source": [
    "estimates = HNS_and_estimates(df2, M=3, show_print=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11534129,  0.11898759,  0.61684245,  0.8476461 ,  0.1513416 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates[1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N(estimates, theoric_betas, initial_dataframe, N):\n",
    "    \"\"\"Étant donné les estimations obtenues par la fonction HNS_and_estimates, la base de données initiale et le paramètre N de la métrique Top-N, top_N calcule le \"score\" de la prédiction.\n",
    "    En se basant sur les connexions qui n'ont pas eu lieu, les probas de connexion théoriques/estimées sont calculées puis comparées entre elles (pour chaque patient, on compte\n",
    "    les N docteurs recommandés par notre système qui sont dans la liste des N docteurs qui ont les probas théoriques les plus élevées de se connecter au patient)\n",
    "\n",
    "    Args:\n",
    "        estimates (_type_): L'output obtenu par HNS_and_estimates\n",
    "        initial_dataframe (_type_): Le dataframe initial, généré par la fonction graph_formation\n",
    "        theoric_betas: Contient la liste des valeurs théoriques des Beta utilisées pour générer le dataframe [beta_age_p, beta_age_d, beta_sex_p, beta_sex_d, beta_distance]\n",
    "        N (_type_): le paramètre de la métrique Top-N (le nombre de docteurs qu'on cherche à recommander)\n",
    "    \"\"\"\n",
    "    # Certaines colonnes sont normalisées (optionnel si c'est déjà effectué)\n",
    "    df = initial_dataframe.copy()\n",
    "    df['age_p_normed'] = ( df['age_p'] - df['age_p'].mean() ) / df['age_p'].std()\n",
    "    df['age_d_normed'] = ( df['age_d'] - df['age_d'].mean() ) / df['age_d'].std()\n",
    "    df['sex_p_normed'] = ( df['sex_p'] - df['sex_p'].mean() ) / df['sex_p'].std()\n",
    "    df['sex_d_normed'] = ( df['sex_d'] - df['sex_d'].mean() ) / df['sex_d'].std()\n",
    "    \n",
    "    n_patients = estimates[1][0].shape[0]\n",
    "    # n_doctors = estimates[1][1].shape[0]\n",
    "    embedding_dim = int(len(initial_dataframe.columns[8:])/2)\n",
    "    beta_age_p_hat = estimates[1][2]\n",
    "    beta_age_d_hat = estimates[1][3]\n",
    "    beta_sex_p_hat = estimates[1][4]\n",
    "    beta_sex_d_hat = estimates[1][5]\n",
    "    beta_distance_hat = estimates[1][6]\n",
    "    beta_age_p = theoric_betas[0]\n",
    "    beta_age_d = theoric_betas[1]\n",
    "    beta_sex_p = theoric_betas[2]\n",
    "    beta_sex_d = theoric_betas[3]\n",
    "    beta_distance = theoric_betas[4]\n",
    "    negative_connections = df[df['y'] == 0]\n",
    "    # recommendations = np.zeros((1000, 2, N)) # Utile pour stocker les N recommandations théoriques/estimées\n",
    "    accuracy = 0\n",
    "    for i in range(n_patients):\n",
    "    # Récupérer les différentes estimations à partir d'estimates\n",
    "        ef_patient_hat = estimates[1][0][i]\n",
    "        # Récupérer les valeurs théoriques via initial_dataframe\n",
    "        ef_patient = df[df['i'] == i].iloc[0][8 : 8 + embedding_dim].to_list()\n",
    "        patient_age_normed = df[df['i'] == i]['age_p_normed'].iloc[0]\n",
    "        patient_sex_normed = df[df['i'] == i]['sex_p_normed'].iloc[0]\n",
    "        # Calculer les probas estimées/théoriques avec les docteurs pour lesquels il n'y a pas de connexion\n",
    "        # On récupère la liste des docteurs avec lesquelles il n'y a pas de connexion\n",
    "        patient_df = negative_connections[negative_connections['i'] == i]\n",
    "        doctors_without_connection = patient_df['j'].to_list()\n",
    "        scores = np.zeros((2, len(doctors_without_connection)))\n",
    "        for j, doctor in enumerate(doctors_without_connection):\n",
    "            distance = patient_df[patient_df['j'] == doctor]['distance'].iloc[0]\n",
    "            ef_doctor_hat = estimates[1][1][doctor]\n",
    "            ef_doctor = initial_dataframe[initial_dataframe['j'] == doctor].iloc[0][-embedding_dim:].to_list()\n",
    "            doctor_age_normed = df[df['j'] == doctor]['age_d_normed'].iloc[0]\n",
    "            doctor_sex_normed =df[df['j'] == doctor]['sex_d_normed'].iloc[0]\n",
    "            T = np.dot(ef_patient, ef_doctor) + beta_age_p * patient_age_normed + beta_age_d * doctor_age_normed \\\n",
    "                + beta_sex_p * patient_sex_normed + beta_sex_d * doctor_sex_normed + beta_distance * distance\n",
    "            scores[0][j] = 1 / (1 + np.exp(-T))\n",
    "            T_hat = np.dot(ef_patient_hat, ef_doctor_hat) + beta_age_p_hat * patient_age_normed + beta_age_d_hat * doctor_age_normed \\\n",
    "                + beta_sex_p_hat * patient_sex_normed + beta_sex_d_hat * doctor_sex_normed + beta_distance_hat * distance\n",
    "            scores[1][j] = 1 / (1 + np.exp(-T_hat))\n",
    "        # Garder les N probas estimées/théoriques les plus grandes\n",
    "        highest_theoretical_scores = np.argsort(scores[0])[-N:]\n",
    "        highest_estimated_scores = np.argsort(scores[1])[-N:]\n",
    "        # Compter le nombre de docteurs recommandés qui sont dans les docteurs avec la proba de connexion la plus élevée\n",
    "        # Il suffit de compter le nombre d'éléments similaires dans highest_theoritical_scores et highest_estimated_scores, sans chercher à retrouver les indices des docteurs recommandés\n",
    "        # (Il est possible de retrouver les indices avec [doctors_without_connection[i] for i in highest_theoretical_scores] et\n",
    "        # [doctors_without_connection[i] for i in highest_theoretical_scores] puis les stocker dans recommendations défini au dessus)\n",
    "        nb_right_predictions = len(set(highest_estimated_scores).intersection(set(highest_theoretical_scores)))\n",
    "        accuracy += nb_right_predictions / N\n",
    "\n",
    "    return accuracy/n_patients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_10048\\3330631541.py:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  scores[1][j] = 1 / (1 + np.exp(-T_hat))\n"
     ]
    }
   ],
   "source": [
    "accuracy = top_N(estimates, theoric_betas, df2, N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cette partie de code est optionnel (la descente de gradient est faite à la main et va beaucoup moins vite que le code ci-dessus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descente de gradient alternée à la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_ui(df):\n",
    "    # à partir du dataset y = 0 et y = 1, obtenir la matrice de connexion\n",
    "    # df2 = df.copy()\n",
    "    patients = df['i'].unique()\n",
    "    n_patients = len(patients)\n",
    "    n_doctors = len(df2['j'].unique())\n",
    "    R = np.zeros((n_patients, n_doctors))\n",
    "    for i in patients:\n",
    "        for j in df[df['i'] == i]['j']:\n",
    "            connection = df[df['i'] == i][df[df['i'] == i]['j'] == j]['y']\n",
    "            # if len(connection) > 0:\n",
    "            if connection[connection.index[0]] == 1:\n",
    "                R[i][j] = 1\n",
    "    return R\n",
    "\n",
    "def alpha(R):\n",
    "    return np.unique(R.flatten(), return_counts = True)[1][0] / np.unique(R.flatten(), return_counts = True)[1][1] # Nombre de 0 sur nombre de 1\n",
    "\n",
    "def descente_gradient(df, max_iter, eps = 0.01, lr_patients=0.001, lr_doctors=0.001, reg_term=0.2):\n",
    "    to_be_normalized_columns = ['age_p', 'age_d', 'sex_p', 'sex_d', 'distance']\n",
    "    df[to_be_normalized_columns] = ( df[to_be_normalized_columns] - df[to_be_normalized_columns].mean() ) / df[to_be_normalized_columns].std()\n",
    "    patients = df['i'].unique()\n",
    "    doctors = df['j'].unique()\n",
    "    n_patients = len(patients)\n",
    "    n_doctors = len(doctors)\n",
    "    nb_latent_factors = len([s for s in df.columns if \"ef_p\" in s])\n",
    "    R = r_ui(df)\n",
    "    alpha_term = alpha(R)\n",
    "    # We initialise randomly the fixed effects, betas and the bias\n",
    "    X = np.random.uniform(-1, 1, size = (n_patients, nb_latent_factors))\n",
    "    Y = np.random.uniform(-1, 1, size = (n_doctors, nb_latent_factors))\n",
    "    beta_age_doctor = np.random.uniform(-0.5, 0.5)\n",
    "    beta_age_patient = np.random.uniform(-0.5, 0.5)\n",
    "    beta_sex_doctor = np.random.uniform(-0.5, 0.5)\n",
    "    beta_sex_patient = np.random.uniform(-0.5, 0.5)\n",
    "    beta_distance = np.random.uniform(-0.5, 0.5)\n",
    "    grad_X = np.zeros((n_patients, nb_latent_factors)) # Matrix containin fixed effects gradients of patients\n",
    "    grad_Y = np.zeros((n_doctors, nb_latent_factors)) # Matrix containing fixed effects gradients of doctors\n",
    "    grad_beta_age_doctor = 0\n",
    "    grad_beta_sex_doctor = 0\n",
    "    grad_beta_age_patient = 0\n",
    "    grad_beta_sex_patient = 0\n",
    "    grad_beta_distance = 0\n",
    "    sum_squared_sgd_ef_patients = [0 for _ in range(n_patients)]\n",
    "    sum_squared_sgd_ef_doctors = [0 for _ in range(n_doctors)]\n",
    "    sum_squared_sgd_beta_age_doctor = 0\n",
    "    sum_squared_sgd_beta_sex_doctor = 0\n",
    "    sum_squared_sgd_beta_age_patient = 0\n",
    "    sum_squared_sgd_beta_sex_patient = 0\n",
    "    sum_squared_sgd_beta_distance = 0\n",
    "\n",
    "    k = 0\n",
    "    first_loop = True # Allows the while loop to run the first time\n",
    "    while ( np.linalg.norm(grad_X) > eps or np.linalg.norm(grad_Y) > eps or first_loop == True ) and \\\n",
    "    k < max_iter : # Alternate gradient descent\n",
    "        first_loop = False\n",
    "        t_0 = time.time()\n",
    "        k += 1\n",
    "        grad_beta_age_doctor = 0\n",
    "        grad_beta_sex_doctor = 0\n",
    "        for j in doctors: # We fix fixed effects / bias of the patients and update the one of the doctors\n",
    "            grad_Y[j, :] = 0 # We reinitialize at each loop the gradients\n",
    "            for i in df[df['j'] == j]['i']:\n",
    "\n",
    "                sex_patient = df[df['i'] == i]['sex_p'].iloc[0]\n",
    "                sex_doctor = df[df['j'] == j]['sex_d'].iloc[0]\n",
    "                age_patient = df[df['i'] == i]['age_p'].iloc[0]\n",
    "                age_doctor = df[df['j'] == j]['age_d'].iloc[0]\n",
    "                distance = df[df['i'] == i][df[df['i'] == i]['j'] == j]['distance'].iloc[0]\n",
    "                # Following terms are defined to optimize calculation time\n",
    "                exp_term = (1 + alpha_term*R[i][j]) / (1 + np.exp(-(np.dot(X[i, :], Y[j, :]) + \\\n",
    "                age_patient * beta_age_patient + sex_patient * beta_sex_patient + age_doctor * beta_age_doctor + sex_doctor * beta_sex_doctor + distance * beta_distance )))\n",
    "                alpha_r = alpha_term*R[i][j]\n",
    "                \n",
    "                grad_Y[j, :] += -alpha_r*X[i, :] + X[i, :] * exp_term + reg_term*Y[j, :]\n",
    "                \n",
    "                grad_beta_age_doctor += age_doctor*(-alpha_r + exp_term)\n",
    "\n",
    "                grad_beta_sex_doctor += sex_doctor*(-alpha_r + exp_term)\n",
    "\n",
    "\n",
    "            # Sum of terms in the denominator for Adaboost\n",
    "            sum_squared_sgd_ef_doctors[j] += (np.dot(grad_Y[j, :], grad_Y[j, :])) \n",
    "            Y[j, :] -= lr_doctors*grad_Y[j, :]/np.sqrt(sum_squared_sgd_ef_doctors[j])\n",
    "\n",
    "        sum_squared_sgd_beta_age_doctor += grad_beta_age_doctor**2\n",
    "        sum_squared_sgd_beta_sex_doctor += grad_beta_sex_doctor**2\n",
    "        beta_age_doctor -= lr_doctors*grad_beta_age_doctor/np.sqrt(sum_squared_sgd_beta_age_doctor)\n",
    "        beta_sex_doctor -= lr_doctors*grad_beta_sex_doctor/np.sqrt(sum_squared_sgd_beta_sex_doctor)\n",
    "\n",
    "\n",
    "        grad_beta_age_patient = 0 # We reinitialize at each loop the gradients\n",
    "        grad_beta_sex_patient = 0\n",
    "        for i in patients: # We now update fixed effects / bias of the patients using our updated fixed effects / bias of doctors\n",
    "            grad_X[i, :] = 0 # We reinitialize at each loop the gradients\n",
    "            for j in df[df['i'] == i]['j']:\n",
    "\n",
    "                sex_patient = df[df['i'] == i]['sex_p'].iloc[0]\n",
    "                sex_doctor = df[df['j'] == j]['sex_d'].iloc[0]\n",
    "                age_patient = df[df['i'] == i]['age_p'].iloc[0]\n",
    "                age_doctor = df[df['j'] == j]['age_d'].iloc[0]\n",
    "                distance = df[df['i'] == i][df[df['i'] == i]['j'] == j]['distance'].iloc[0]\n",
    "                # Following terms are defined to optimize calculation time\n",
    "                exp_term = (1 + alpha_term*R[i][j]) / (1 + np.exp(-(np.dot(X[i, :], Y[j, :]) + \\\n",
    "                age_patient * beta_age_patient + sex_patient * beta_sex_patient + age_doctor * beta_age_doctor + sex_doctor * beta_sex_doctor + distance * beta_distance )))\n",
    "                alpha_r = alpha_term*R[i][j]\n",
    "                \n",
    "                grad_X[i, :] += -alpha_r * Y[j, :] + Y[j, :] * exp_term + reg_term*X[i, :]\n",
    "\n",
    "                grad_beta_age_patient += age_patient * (-alpha_r + exp_term)\n",
    "\n",
    "                grad_beta_sex_patient += sex_patient * (-alpha_r + exp_term)\n",
    "                \n",
    "            sum_squared_sgd_ef_patients[i] += (np.dot(grad_X[i, :], grad_X[i, :])) # Denominator term for Adaboost\n",
    "            X[i, :] -= lr_patients*grad_X[i, :]/np.sqrt(sum_squared_sgd_ef_patients[i])\n",
    "\n",
    "        sum_squared_sgd_beta_age_patient += grad_beta_age_patient**2\n",
    "        sum_squared_sgd_beta_sex_patient += grad_beta_sex_patient**2\n",
    "        beta_age_patient -= lr_patients*grad_beta_age_patient/np.sqrt(sum_squared_sgd_beta_age_patient)\n",
    "        beta_sex_patient -= lr_patients*grad_beta_age_patient/np.sqrt(sum_squared_sgd_beta_sex_patient)\n",
    "\n",
    "        grad_beta_distance = 0\n",
    "        for i in patients: # We finally update the beta_distance as it is dependent on patients and doctors parameters.\n",
    "            for j in df[df['i'] == i]['j']:\n",
    "\n",
    "                sex_patient = df[df['i'] == i]['sex_p'].iloc[0]\n",
    "                sex_doctor = df[df['j'] == j]['sex_d'].iloc[0]\n",
    "                age_patient = df[df['i'] == i]['age_p'].iloc[0]\n",
    "                age_doctor = df[df['j'] == j]['age_d'].iloc[0]\n",
    "                distance = df[df['i'] == i][df[df['i'] == i]['j'] == j]['distance'].iloc[0]\n",
    "                exp_term = (1 + alpha_term*R[i][j]) / (1 + np.exp(-(np.dot(X[i, :], Y[j, :]) + \\\n",
    "                age_patient * beta_age_patient + sex_patient * beta_sex_patient + age_doctor * beta_age_doctor + sex_doctor * beta_sex_doctor + distance * beta_distance )))\n",
    "                alpha_r = alpha_term*R[i][j]\n",
    "                \n",
    "                grad_beta_distance += distance*(-alpha_r + exp_term)\n",
    "            \n",
    "        sum_squared_sgd_beta_distance += grad_beta_distance**2\n",
    "        beta_distance -= ((lr_patients + lr_doctors)/2)*grad_beta_distance/np.sqrt(sum_squared_sgd_beta_distance)\n",
    "        t_1 = time.time()\n",
    "\n",
    "        print(k, \"-th iteration took\", t_1 - t_0, \"to execute.\")\n",
    "    beta = [beta_age_doctor, beta_sex_doctor, beta_age_patient, beta_sex_patient, beta_distance]\n",
    "    grad_beta = [grad_beta_age_doctor, grad_beta_sex_doctor, grad_beta_age_patient, grad_beta_sex_patient, grad_beta_distance]\n",
    "    return ([X, Y, beta], [grad_X, grad_Y, grad_beta])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = graph_formation(n_patients=100,\n",
    "                    n_doctors=30,\n",
    "                    max_number_connections=4,\n",
    "                    z=0.5,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.01,\n",
    "                    beta_age_d_graph=0.01,\n",
    "                    beta_sex_p_graph=0.1,\n",
    "                    beta_sex_d_graph=0.1,\n",
    "                    beta_distance_graph=-10,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = 5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = df.copy()\n",
    "positive_connections = dataframe[dataframe['y'] == 1]\n",
    "negative_connections = dataframe[dataframe['y'] == 0]\n",
    "len(negative_connections['i'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.69"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre de non connexions par patient en moyenne\n",
    "negative_connections['i'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M doit être inférieur ou égal à cette valeur\n",
    "# C'est le plus petit nombre de docteurs auquel un patient i n'est pas connecté\n",
    "negative_connections['i'].value_counts().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i\n",
       "50    4\n",
       "22    4\n",
       "59    3\n",
       "60    3\n",
       "86    2\n",
       "     ..\n",
       "32    1\n",
       "31    1\n",
       "30    1\n",
       "29    1\n",
       "99    1\n",
       "Name: count, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On a bien plus de connexions négatives que de connexions positives\n",
    "positive_connections['i'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.31"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre de connexions par patient en moyenne\n",
    "positive_connections['i'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = graph_formation_hns(n_patients=1000,\n",
    "                    n_doctors=300,\n",
    "                    max_number_connections=5,\n",
    "                    z=0.5,\n",
    "                    M=30,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.01,\n",
    "                    beta_age_d_graph=0.01,\n",
    "                    beta_sex_p_graph=0.1,\n",
    "                    beta_sex_d_graph=0.1,\n",
    "                    beta_distance_graph=-10,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = 5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>y</th>\n",
       "      <th>age_p</th>\n",
       "      <th>age_d</th>\n",
       "      <th>sex_p</th>\n",
       "      <th>sex_d</th>\n",
       "      <th>distance</th>\n",
       "      <th>ef_p_0</th>\n",
       "      <th>ef_p_1</th>\n",
       "      <th>ef_p_2</th>\n",
       "      <th>ef_p_3</th>\n",
       "      <th>ef_p_4</th>\n",
       "      <th>ef_d_0</th>\n",
       "      <th>ef_d_1</th>\n",
       "      <th>ef_d_2</th>\n",
       "      <th>ef_d_3</th>\n",
       "      <th>ef_d_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>89</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.367899</td>\n",
       "      <td>-0.682953</td>\n",
       "      <td>-0.815052</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>-0.576334</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>-0.168551</td>\n",
       "      <td>0.065324</td>\n",
       "      <td>0.815440</td>\n",
       "      <td>0.461285</td>\n",
       "      <td>0.983074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>44</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.291169</td>\n",
       "      <td>-0.682953</td>\n",
       "      <td>-0.815052</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>-0.576334</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>0.040532</td>\n",
       "      <td>-0.352408</td>\n",
       "      <td>0.731672</td>\n",
       "      <td>-0.114341</td>\n",
       "      <td>-0.670008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>45</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.093386</td>\n",
       "      <td>-0.682953</td>\n",
       "      <td>-0.815052</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>-0.576334</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>-0.094269</td>\n",
       "      <td>-0.821354</td>\n",
       "      <td>-0.235760</td>\n",
       "      <td>0.282684</td>\n",
       "      <td>-0.743585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>47</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386003</td>\n",
       "      <td>-0.682953</td>\n",
       "      <td>-0.815052</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>-0.576334</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>-0.971572</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>-0.689428</td>\n",
       "      <td>-0.252345</td>\n",
       "      <td>0.567239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>88</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.343987</td>\n",
       "      <td>-0.682953</td>\n",
       "      <td>-0.815052</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>-0.576334</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>-0.391391</td>\n",
       "      <td>-0.904439</td>\n",
       "      <td>0.291136</td>\n",
       "      <td>-0.168596</td>\n",
       "      <td>0.922401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5898</th>\n",
       "      <td>995</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.209940</td>\n",
       "      <td>-0.551933</td>\n",
       "      <td>0.228154</td>\n",
       "      <td>0.829068</td>\n",
       "      <td>-0.545308</td>\n",
       "      <td>-0.901676</td>\n",
       "      <td>0.082150</td>\n",
       "      <td>-0.882055</td>\n",
       "      <td>0.704478</td>\n",
       "      <td>-0.410692</td>\n",
       "      <td>-0.355338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>996</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>93</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.253955</td>\n",
       "      <td>-0.489046</td>\n",
       "      <td>0.817064</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>-0.820247</td>\n",
       "      <td>0.267251</td>\n",
       "      <td>-0.488486</td>\n",
       "      <td>0.236925</td>\n",
       "      <td>-0.282689</td>\n",
       "      <td>0.105945</td>\n",
       "      <td>0.081801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>997</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.015671</td>\n",
       "      <td>0.428782</td>\n",
       "      <td>0.346513</td>\n",
       "      <td>0.196287</td>\n",
       "      <td>-0.351358</td>\n",
       "      <td>0.671217</td>\n",
       "      <td>-0.040852</td>\n",
       "      <td>0.196027</td>\n",
       "      <td>-0.999387</td>\n",
       "      <td>-0.907583</td>\n",
       "      <td>-0.385915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5901</th>\n",
       "      <td>998</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100773</td>\n",
       "      <td>-0.043227</td>\n",
       "      <td>-0.840932</td>\n",
       "      <td>0.074328</td>\n",
       "      <td>0.421382</td>\n",
       "      <td>-0.467160</td>\n",
       "      <td>-0.709199</td>\n",
       "      <td>0.663311</td>\n",
       "      <td>0.445632</td>\n",
       "      <td>-0.092643</td>\n",
       "      <td>-0.999014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>999</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.131638</td>\n",
       "      <td>0.875532</td>\n",
       "      <td>0.077772</td>\n",
       "      <td>-0.580259</td>\n",
       "      <td>0.935813</td>\n",
       "      <td>-0.591349</td>\n",
       "      <td>-0.040852</td>\n",
       "      <td>0.196027</td>\n",
       "      <td>-0.999387</td>\n",
       "      <td>-0.907583</td>\n",
       "      <td>-0.385915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5903 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        i    j  y  age_p  age_d  sex_p  sex_d  distance    ef_p_0    ef_p_1  \\\n",
       "0       0   30  1     86     89     -1     -1  0.367899 -0.682953 -0.815052   \n",
       "1       0  145  1     86     44     -1      1  0.291169 -0.682953 -0.815052   \n",
       "2       0  156  1     86     45     -1      1  0.093386 -0.682953 -0.815052   \n",
       "3       0  262  1     86     47     -1      1  0.386003 -0.682953 -0.815052   \n",
       "4       0  289  1     86     88     -1      1  0.343987 -0.682953 -0.815052   \n",
       "...   ...  ... ..    ...    ...    ...    ...       ...       ...       ...   \n",
       "5898  995  275  0     80     67      1      1  0.209940 -0.551933  0.228154   \n",
       "5899  996  132  0     83     93     -1      1  0.253955 -0.489046  0.817064   \n",
       "5900  997  173  0      2     28     -1     -1  0.015671  0.428782  0.346513   \n",
       "5901  998   97  0      3     56      1      1  0.100773 -0.043227 -0.840932   \n",
       "5902  999  173  0     29     28      1     -1  0.131638  0.875532  0.077772   \n",
       "\n",
       "        ef_p_2    ef_p_3    ef_p_4    ef_d_0    ef_d_1    ef_d_2    ef_d_3  \\\n",
       "0     0.010601 -0.576334  0.109618 -0.168551  0.065324  0.815440  0.461285   \n",
       "1     0.010601 -0.576334  0.109618  0.040532 -0.352408  0.731672 -0.114341   \n",
       "2     0.010601 -0.576334  0.109618 -0.094269 -0.821354 -0.235760  0.282684   \n",
       "3     0.010601 -0.576334  0.109618 -0.971572  0.410381 -0.689428 -0.252345   \n",
       "4     0.010601 -0.576334  0.109618 -0.391391 -0.904439  0.291136 -0.168596   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5898  0.829068 -0.545308 -0.901676  0.082150 -0.882055  0.704478 -0.410692   \n",
       "5899  0.061119 -0.820247  0.267251 -0.488486  0.236925 -0.282689  0.105945   \n",
       "5900  0.196287 -0.351358  0.671217 -0.040852  0.196027 -0.999387 -0.907583   \n",
       "5901  0.074328  0.421382 -0.467160 -0.709199  0.663311  0.445632 -0.092643   \n",
       "5902 -0.580259  0.935813 -0.591349 -0.040852  0.196027 -0.999387 -0.907583   \n",
       "\n",
       "        ef_d_4  \n",
       "0     0.983074  \n",
       "1    -0.670008  \n",
       "2    -0.743585  \n",
       "3     0.567239  \n",
       "4     0.922401  \n",
       "...        ...  \n",
       "5898 -0.355338  \n",
       "5899  0.081801  \n",
       "5900 -0.385915  \n",
       "5901 -0.999014  \n",
       "5902 -0.385915  \n",
       "\n",
       "[5903 rows x 18 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "1    4946\n",
       "0     957\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparaison Y = 0 / Y = 1, pas assez d'échantillonnage négatif ?\n",
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -th iteration took 24.55609917640686 to execute.\n",
      "2 -th iteration took 26.22185707092285 to execute.\n",
      "3 -th iteration took 25.664959192276 to execute.\n",
      "4 -th iteration took 23.428571701049805 to execute.\n",
      "5 -th iteration took 24.576223611831665 to execute.\n",
      "6 -th iteration took 24.129536867141724 to execute.\n",
      "7 -th iteration took 24.11001944541931 to execute.\n",
      "8 -th iteration took 24.294201135635376 to execute.\n",
      "9 -th iteration took 24.16663360595703 to execute.\n",
      "10 -th iteration took 24.205181121826172 to execute.\n",
      "11 -th iteration took 24.013393878936768 to execute.\n",
      "12 -th iteration took 24.050031900405884 to execute.\n",
      "13 -th iteration took 24.973216772079468 to execute.\n",
      "14 -th iteration took 24.470908880233765 to execute.\n",
      "15 -th iteration took 24.423006772994995 to execute.\n",
      "16 -th iteration took 23.795681953430176 to execute.\n",
      "17 -th iteration took 24.83116054534912 to execute.\n",
      "18 -th iteration took 25.092320680618286 to execute.\n",
      "19 -th iteration took 25.979527711868286 to execute.\n",
      "20 -th iteration took 26.26982069015503 to execute.\n",
      "21 -th iteration took 26.041643142700195 to execute.\n",
      "22 -th iteration took 24.13598608970642 to execute.\n",
      "23 -th iteration took 24.60260772705078 to execute.\n",
      "24 -th iteration took 24.932884693145752 to execute.\n",
      "25 -th iteration took 25.130613088607788 to execute.\n",
      "26 -th iteration took 25.325184106826782 to execute.\n",
      "27 -th iteration took 25.452670335769653 to execute.\n",
      "28 -th iteration took 24.259822607040405 to execute.\n",
      "29 -th iteration took 23.60060691833496 to execute.\n",
      "30 -th iteration took 23.818846225738525 to execute.\n",
      "31 -th iteration took 24.34464406967163 to execute.\n",
      "32 -th iteration took 24.301928758621216 to execute.\n",
      "33 -th iteration took 23.14770770072937 to execute.\n",
      "34 -th iteration took 23.14901304244995 to execute.\n",
      "35 -th iteration took 25.214627504348755 to execute.\n",
      "36 -th iteration took 25.165579080581665 to execute.\n",
      "37 -th iteration took 24.082488298416138 to execute.\n",
      "38 -th iteration took 25.33447003364563 to execute.\n",
      "39 -th iteration took 24.486979007720947 to execute.\n",
      "40 -th iteration took 24.66359233856201 to execute.\n",
      "41 -th iteration took 24.701664686203003 to execute.\n",
      "42 -th iteration took 24.84695529937744 to execute.\n",
      "43 -th iteration took 23.986178159713745 to execute.\n",
      "44 -th iteration took 25.846885204315186 to execute.\n",
      "45 -th iteration took 24.149953842163086 to execute.\n",
      "46 -th iteration took 25.639819622039795 to execute.\n",
      "47 -th iteration took 26.208609342575073 to execute.\n",
      "48 -th iteration took 26.178250551223755 to execute.\n",
      "49 -th iteration took 24.919225215911865 to execute.\n",
      "50 -th iteration took 25.217904090881348 to execute.\n",
      "51 -th iteration took 23.754377841949463 to execute.\n",
      "52 -th iteration took 25.18455743789673 to execute.\n",
      "53 -th iteration took 24.903316736221313 to execute.\n",
      "54 -th iteration took 24.601991891860962 to execute.\n",
      "55 -th iteration took 24.32833981513977 to execute.\n",
      "56 -th iteration took 25.18531584739685 to execute.\n",
      "57 -th iteration took 24.026367902755737 to execute.\n",
      "58 -th iteration took 24.521310091018677 to execute.\n",
      "59 -th iteration took 24.93058705329895 to execute.\n",
      "60 -th iteration took 24.20749020576477 to execute.\n",
      "61 -th iteration took 23.49236512184143 to execute.\n",
      "62 -th iteration took 25.147790670394897 to execute.\n",
      "63 -th iteration took 24.04516315460205 to execute.\n",
      "64 -th iteration took 25.09157657623291 to execute.\n",
      "65 -th iteration took 24.619941234588623 to execute.\n",
      "66 -th iteration took 25.490046739578247 to execute.\n",
      "67 -th iteration took 24.26006817817688 to execute.\n",
      "68 -th iteration took 25.216858625411987 to execute.\n",
      "69 -th iteration took 24.52005171775818 to execute.\n",
      "70 -th iteration took 25.4677996635437 to execute.\n",
      "71 -th iteration took 24.77733302116394 to execute.\n",
      "72 -th iteration took 24.4067223072052 to execute.\n",
      "73 -th iteration took 24.591670274734497 to execute.\n",
      "74 -th iteration took 25.242889165878296 to execute.\n",
      "75 -th iteration took 25.413061380386353 to execute.\n",
      "76 -th iteration took 23.750635385513306 to execute.\n",
      "77 -th iteration took 23.94399619102478 to execute.\n",
      "78 -th iteration took 24.760543823242188 to execute.\n",
      "79 -th iteration took 23.710671424865723 to execute.\n",
      "80 -th iteration took 24.778351068496704 to execute.\n",
      "81 -th iteration took 24.303751468658447 to execute.\n",
      "82 -th iteration took 24.020217657089233 to execute.\n",
      "83 -th iteration took 24.263299703598022 to execute.\n",
      "84 -th iteration took 22.775520086288452 to execute.\n",
      "85 -th iteration took 24.086087465286255 to execute.\n",
      "86 -th iteration took 24.184792041778564 to execute.\n",
      "87 -th iteration took 24.29128336906433 to execute.\n",
      "88 -th iteration took 24.383270740509033 to execute.\n",
      "89 -th iteration took 24.82006549835205 to execute.\n",
      "90 -th iteration took 24.32274055480957 to execute.\n",
      "91 -th iteration took 25.0598201751709 to execute.\n",
      "92 -th iteration took 23.75376319885254 to execute.\n",
      "93 -th iteration took 25.275885820388794 to execute.\n",
      "94 -th iteration took 23.66220736503601 to execute.\n",
      "95 -th iteration took 24.62480854988098 to execute.\n",
      "96 -th iteration took 23.95264434814453 to execute.\n",
      "97 -th iteration took 24.287663459777832 to execute.\n",
      "98 -th iteration took 25.012372255325317 to execute.\n",
      "99 -th iteration took 23.1621732711792 to execute.\n",
      "100 -th iteration took 25.13355302810669 to execute.\n"
     ]
    }
   ],
   "source": [
    "estimation = descente_gradient(df = df,\n",
    "                               eps = 0.1,\n",
    "                               lr_patients=0.01,\n",
    "                               lr_doctors=0.01,\n",
    "                               max_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.86510073e-01, -2.93828795e+00, -4.01088596e+00,\n",
       "         3.32367652e+00, -2.42663156e+00],\n",
       "       [-6.86205939e+00,  5.33422064e+00,  2.26828245e+00,\n",
       "        -4.09973338e+00, -6.24010126e+00],\n",
       "       [ 2.10215838e+00, -8.88230774e+00, -2.36393694e+00,\n",
       "        -4.92893894e-01,  3.59808490e+00],\n",
       "       [-1.41164891e+01,  1.00682957e+01,  5.39026517e+00,\n",
       "         9.08717800e-03, -1.21585999e+01],\n",
       "       [-5.45473010e+00, -8.26662145e+00,  2.52056527e+00,\n",
       "         5.63085716e+00, -3.68921928e+00],\n",
       "       [ 1.13120530e+01, -8.05152227e+00,  4.87896954e+00,\n",
       "         9.97776168e+00, -1.49861909e+01],\n",
       "       [ 7.67342151e+00,  1.23532720e+00, -4.37892911e+00,\n",
       "        -8.56023614e+00,  1.04389170e+01],\n",
       "       [ 3.28167147e+00, -7.74976258e+00, -1.16264563e+01,\n",
       "        -1.93535674e+00, -7.49534011e+00],\n",
       "       [-6.92277611e+00, -5.44984994e-01, -8.88486620e-01,\n",
       "        -1.39881267e+00, -8.36237379e+00],\n",
       "       [ 1.16263014e+01, -1.58366721e+00,  9.85928106e+00,\n",
       "        -6.39932648e+00,  8.35666639e+00],\n",
       "       [-6.29952448e+00, -1.64503882e+00,  7.30453927e+00,\n",
       "         3.55019215e+00, -8.63654520e+00],\n",
       "       [-2.82905956e+00,  6.29599132e+00,  1.26253390e-01,\n",
       "        -4.28026559e+00, -1.03401184e+01],\n",
       "       [-5.35122256e+00,  2.39148500e-01, -4.07083542e-01,\n",
       "        -8.16447285e+00, -3.69803789e+00],\n",
       "       [ 4.28813690e+00,  2.22374740e-01, -2.14563730e+00,\n",
       "        -6.93784637e+00, -1.54658660e+00],\n",
       "       [ 6.16407207e-01,  5.37315719e+00,  5.07108816e+00,\n",
       "        -4.44469764e+00, -1.71739181e+00],\n",
       "       [ 7.86118185e+00,  8.56017965e+00, -5.12411103e+00,\n",
       "         2.38142243e+00,  7.30213466e+00],\n",
       "       [ 5.55698490e+00,  1.42025851e+00,  5.96152787e+00,\n",
       "         5.73616448e+00, -6.01481820e+00],\n",
       "       [ 2.45676172e+00, -3.33622087e+00, -3.28494869e+00,\n",
       "        -1.55052681e+00,  5.96491746e+00],\n",
       "       [-5.22682405e+00,  4.55245303e+00,  2.37641607e+00,\n",
       "         4.83742875e+00,  4.05379270e+00],\n",
       "       [ 9.08043151e-01, -4.16635995e+00,  6.63357806e+00,\n",
       "         2.18771838e+00,  7.09258471e+00],\n",
       "       [ 1.37294726e+00, -2.54186177e-01, -2.75868185e+00,\n",
       "         4.46920666e+00, -5.51835031e+00],\n",
       "       [ 6.03808975e+00,  1.55371089e+00,  2.52618379e+00,\n",
       "        -1.00306356e-01, -1.72078801e+00],\n",
       "       [-7.54681640e+00, -8.02246586e+00,  1.07130838e+01,\n",
       "         1.60997032e+01, -6.36796165e+00],\n",
       "       [ 4.06244528e+00, -8.64966191e+00,  1.03371953e+01,\n",
       "        -5.75706891e+00, -9.69108377e+00],\n",
       "       [-2.46480667e+00, -5.15281777e-02, -1.65051509e+00,\n",
       "        -4.99641391e+00, -4.57023240e+00],\n",
       "       [ 6.00766521e-01,  2.86174491e+00,  3.37397636e+00,\n",
       "        -5.37331861e+00, -2.54590344e+00],\n",
       "       [ 3.41579002e+00,  1.17352543e+01, -1.15000679e+01,\n",
       "        -5.79217240e+00,  2.57558348e+00],\n",
       "       [-9.90689105e+00,  2.27681428e+00,  1.37074612e+00,\n",
       "         7.55247047e-01, -2.33828031e+00],\n",
       "       [-4.40964223e+00,  1.82845407e+00,  2.98715706e+00,\n",
       "         1.71635688e+00, -5.31710402e+00],\n",
       "       [-4.82073971e+00,  3.75161034e-01, -1.70219666e+00,\n",
       "        -7.27915673e+00,  2.31941929e+00],\n",
       "       [ 5.01599743e-01,  1.06682036e+01,  6.57983689e+00,\n",
       "        -4.16329000e+00, -1.39040126e+01],\n",
       "       [-8.20580085e+00, -8.64720090e+00, -1.04958511e+01,\n",
       "         3.74882458e+00,  2.20821947e+00],\n",
       "       [ 4.56489541e+00,  5.25988179e+00,  2.92501757e+00,\n",
       "        -1.09673313e+00, -4.58051201e+00],\n",
       "       [ 3.05399856e+00,  3.03611107e+00, -1.08459751e+00,\n",
       "         2.98250499e+00,  4.56094893e-01],\n",
       "       [ 9.95575381e+00,  3.96764808e+00,  2.83457628e+00,\n",
       "        -1.33023761e+00, -4.62276138e-01],\n",
       "       [-5.61419501e+00,  4.20876832e+00,  1.14046913e+00,\n",
       "        -9.08055939e-01, -1.04931728e+01],\n",
       "       [ 2.24327803e+00,  3.05045592e+00,  1.86814175e+00,\n",
       "         5.59896197e-01, -4.11790426e+00],\n",
       "       [ 1.19665745e+01, -2.03333063e+00, -4.88412269e-01,\n",
       "         9.60803773e+00,  6.52865151e+00],\n",
       "       [ 2.18523497e+00, -6.81317195e+00,  7.56652054e-01,\n",
       "         4.63519544e+00,  3.31030961e+00],\n",
       "       [ 1.34820015e+01, -2.52916391e+00, -1.49044957e+00,\n",
       "        -1.01814323e+01,  1.05959006e+01],\n",
       "       [ 1.06009982e+01,  1.88363130e+00,  7.29266670e+00,\n",
       "        -5.44263245e+00, -8.10320921e+00],\n",
       "       [ 3.72163709e+00, -3.86658727e-01,  3.97376204e+00,\n",
       "        -4.54149658e+00, -5.19397835e+00],\n",
       "       [-3.23784372e+00, -1.73084426e+00, -5.69382881e+00,\n",
       "        -6.23011293e+00, -8.21583104e+00],\n",
       "       [ 1.19666472e+01,  6.19867509e+00, -3.18122784e+00,\n",
       "        -1.37228263e+01,  6.68779174e+00],\n",
       "       [-1.59709371e+00,  2.26585742e+00, -8.21126070e+00,\n",
       "        -2.30648397e+00, -7.41688325e-01],\n",
       "       [-4.69306892e+00,  3.89814096e+00,  2.35075689e+00,\n",
       "         5.59765602e+00, -2.80945615e+00],\n",
       "       [-3.23170955e+00, -2.81651367e+00, -5.79307749e+00,\n",
       "        -4.67724624e+00,  3.98796377e+00],\n",
       "       [ 2.35068279e-01,  3.85783809e+00,  7.46611144e-01,\n",
       "         2.07066910e+00,  3.40265470e+00],\n",
       "       [-3.74447206e-01, -4.88067755e+00,  1.69586500e+00,\n",
       "         1.27923109e+00,  2.13262207e+00],\n",
       "       [ 1.47918026e+01,  4.57708384e+00, -5.29951276e+00,\n",
       "        -6.22444384e+00, -2.24853330e+00],\n",
       "       [-6.00721131e-02, -3.07995312e+00, -7.50028560e-01,\n",
       "         4.04276781e+00,  8.08108693e+00],\n",
       "       [ 1.23619928e+01, -9.32859310e+00, -7.73326292e+00,\n",
       "         9.07401852e+00, -7.85615765e+00],\n",
       "       [ 5.89780678e+00, -4.97233125e+00, -4.21117114e+00,\n",
       "         4.50066427e+00, -2.94151074e-01],\n",
       "       [-1.17873025e+01, -2.06802302e+00,  3.30677011e+00,\n",
       "         1.22261740e+01, -4.57801304e+00],\n",
       "       [-5.13701724e+00,  2.48300386e+00, -3.76431381e+00,\n",
       "        -6.14849807e+00, -5.06188441e+00],\n",
       "       [-4.59101788e-01,  4.10207034e+00, -4.58134331e+00,\n",
       "        -4.11186176e+00, -3.47435329e+00],\n",
       "       [-7.79068563e-01, -2.96137861e-01, -5.78748622e-01,\n",
       "         4.39234672e+00,  5.32454281e-01],\n",
       "       [-5.03837096e+00,  1.42010576e+00,  5.24519340e+00,\n",
       "        -4.65887333e-01, -2.02373839e-01],\n",
       "       [ 6.17899053e-01,  1.79668085e+00, -3.46259122e+00,\n",
       "        -2.77234759e+00, -1.02397782e+00],\n",
       "       [ 2.80846112e+00, -1.26182795e+01, -3.60483127e+00,\n",
       "         1.09487473e+01, -4.80349576e-01],\n",
       "       [-5.12081573e+00, -1.00989931e+01, -4.94196359e+00,\n",
       "         1.39771687e-01, -1.73926975e+00],\n",
       "       [-3.44339027e+00,  4.97912355e-01,  6.71045366e-01,\n",
       "         1.13721207e+00,  3.45069527e+00],\n",
       "       [-4.83927213e+00, -6.86857488e-01,  1.17018835e+00,\n",
       "        -5.27203809e-01, -8.67182418e+00],\n",
       "       [ 1.32795303e+00,  1.06525402e+00, -2.15519952e+00,\n",
       "         8.09519303e-01,  2.92319559e+00],\n",
       "       [-9.85151802e+00,  8.18218453e+00, -5.01452165e+00,\n",
       "         6.34783956e-01, -2.83554733e+00],\n",
       "       [ 7.02903800e+00, -2.70866912e+00,  2.00624273e+00,\n",
       "        -3.83962082e+00, -5.19014146e+00],\n",
       "       [-9.42600049e+00, -1.06099040e+01,  1.61261104e+01,\n",
       "        -1.25121462e+01,  9.40152561e+00],\n",
       "       [ 1.12416313e+01,  2.71416818e+00,  8.25203220e-01,\n",
       "        -3.72963285e+00,  4.99558746e+00],\n",
       "       [ 4.33447658e+00,  4.84548634e+00, -1.44924261e+00,\n",
       "         1.46232507e+00, -6.13695062e+00],\n",
       "       [-3.55081758e-01, -8.60771117e+00,  5.46689085e+00,\n",
       "         5.79349075e+00, -8.68454563e+00],\n",
       "       [ 1.07897001e+01, -1.33907634e+01, -2.73519447e-01,\n",
       "        -5.39460039e+00,  6.06576504e+00],\n",
       "       [ 5.70899239e+00, -4.88439503e+00, -1.11607092e+00,\n",
       "         5.14633466e+00, -5.58782186e+00],\n",
       "       [ 1.08718611e+00,  7.26430193e-01, -4.02329668e-01,\n",
       "         1.22425801e+00,  2.18536416e-01],\n",
       "       [-1.67394389e-01, -8.48100436e+00, -3.70817528e+00,\n",
       "         3.35953792e+00,  3.96064925e+00],\n",
       "       [ 2.62562399e+00, -9.95556634e+00,  2.81431694e+00,\n",
       "         5.35700173e+00, -4.50237403e+00],\n",
       "       [ 6.16130692e+00, -3.10595997e+00,  6.04925236e+00,\n",
       "         5.74445999e+00, -4.21483888e+00],\n",
       "       [-6.44837917e+00, -1.43306769e+01, -2.80355975e+00,\n",
       "         1.15784281e+01,  1.71463579e+00],\n",
       "       [ 1.79704062e+00, -3.28077021e+00, -2.85749524e+00,\n",
       "         1.06799139e+00,  6.85982249e-01],\n",
       "       [-7.47835149e+00, -1.06318894e+00, -4.70401933e-01,\n",
       "         1.56308491e+00, -5.79896232e+00],\n",
       "       [ 1.57503647e+00,  3.45497618e+00, -3.53593275e+00,\n",
       "         3.36268184e+00,  4.40095066e+00],\n",
       "       [-2.62186100e+00, -1.30067515e+01,  6.19691431e+00,\n",
       "         8.21959524e+00, -7.71590358e-01],\n",
       "       [ 2.28737065e+00,  4.04633885e-01,  5.11453173e+00,\n",
       "        -1.51704661e+01, -1.59282861e+00],\n",
       "       [ 2.91563045e-01, -9.18180507e+00,  2.11331110e+00,\n",
       "         4.03712175e+00, -4.46904267e+00],\n",
       "       [ 1.95831669e+00,  3.55714775e+00, -1.73723939e+00,\n",
       "        -5.80411430e+00, -4.83670453e-01],\n",
       "       [-1.20998157e+01, -5.23897786e+00,  6.10526107e+00,\n",
       "         5.41196372e-01, -5.68871642e+00],\n",
       "       [ 1.29015003e+00,  2.67372655e+00, -6.04987644e+00,\n",
       "        -3.06012803e+00,  3.01239101e+00],\n",
       "       [ 3.98397765e+00,  4.63101851e+00,  1.94118615e+00,\n",
       "         2.67468424e+00, -4.18846329e-01],\n",
       "       [-4.76414036e+00, -1.41947373e-01,  5.12741018e+00,\n",
       "         2.11437370e+00,  1.60064782e+00],\n",
       "       [ 1.04876309e+00, -3.06029828e+00, -8.25941175e-01,\n",
       "         4.00308242e+00,  2.71413493e+00],\n",
       "       [-3.95411715e+00, -2.17174876e+00, -5.85562270e-01,\n",
       "        -2.71478821e+00,  1.87274903e+00],\n",
       "       [-5.06476745e-01, -4.94469668e+00, -3.39646478e+00,\n",
       "         3.27313425e+00, -3.38006191e+00],\n",
       "       [ 2.43126640e+00, -4.61246840e+00, -2.49860508e+00,\n",
       "         9.63148285e+00, -2.30965472e+00],\n",
       "       [-1.88994674e+01, -1.21778964e+01,  1.52595998e+01,\n",
       "        -6.29691029e+00,  1.46311157e+01],\n",
       "       [ 1.17368624e+01, -1.37415002e+01,  9.68711375e-01,\n",
       "         1.08259900e+01, -1.57139780e+01],\n",
       "       [ 4.90749050e+00, -5.23508961e+00,  6.26053687e+00,\n",
       "        -4.27531171e+00,  4.39406059e-01],\n",
       "       [-7.92274098e+00,  1.34375725e+01,  2.52662513e+00,\n",
       "        -6.62353401e+00,  1.01395302e+00],\n",
       "       [-4.09859001e+00, -8.44691703e+00, -1.19894963e+01,\n",
       "         8.89973479e+00,  1.50504599e+01],\n",
       "       [-6.51152771e+00, -1.96191468e+00, -2.07297133e-01,\n",
       "        -2.76968662e+00,  2.47184592e+00],\n",
       "       [-7.37874929e+00, -3.64997797e+00, -2.88154374e-01,\n",
       "        -5.35758544e+00,  1.37094917e+01],\n",
       "       [ 3.08307261e+00, -1.08536127e+01,  3.04030091e-01,\n",
       "         4.86268267e+00, -1.15837476e+01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimation[1][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
