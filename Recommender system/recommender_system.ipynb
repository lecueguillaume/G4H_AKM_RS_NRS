{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Ignore warnings below\n",
    "# Optimisation via tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Dot, Embedding, Add, Flatten, Activation, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "pd.options.mode.chained_assignment = None  # default='warn' # Remove copy on slice warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph formation without HNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_formation(n_patients,\n",
    "                    n_doctors,\n",
    "                    max_number_connections,\n",
    "                    z=1.4,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.01,\n",
    "                    beta_age_d_graph=0.01,\n",
    "                    beta_sex_p_graph=0.5,\n",
    "                    beta_sex_d_graph=0.5,\n",
    "                    beta_distance_graph=-0.5,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = None\n",
    "                   ):\n",
    "    \"\"\"Génère un dataframe contenant l'ensemble des connexions/inexistantes entre les patients/docteurs selon un modèle logistique.\n",
    "\n",
    "    Args:\n",
    "        n_patients (int): Le nombre de patients\n",
    "        n_doctors (int): Le nombre de docteurs\n",
    "        max_number_connections (int): Le nombre max de connexions par patient que l'on autorise\n",
    "        z (float, optional): Le seuil de distance maximale pour lequel on autorise connexion. Defaults to 1.4.\n",
    "        type_distance (str, optional): Permet de définir la manière dont on génère la distance (par défaut, on génère des points dans [0, 1] x [0, 1]),\n",
    "        sinon on utilise les CODGEO de l'INSEE. Defaults to \"default\".\n",
    "        beta_age_p_graph (float, optional): le paramètre du modèle associé à l'âge des patients. Defaults to 0.01.\n",
    "        beta_age_d_graph (float, optional): le paramètre du modèle associé à l'âge des docteurs. Defaults to 0.01.\n",
    "        beta_sex_p_graph (float, optional): le paramètre du modèle associé au sexe des patients. Defaults to 0.5.\n",
    "        beta_sex_d_graph (float, optional): le paramètre du modèle associé au sexe des docteurs. Defaults to 0.5.\n",
    "        beta_distance_graph (float, optional): le paramètre du modèle associé à la distance. Defaults to -0.5.\n",
    "        alpha_law_graph (tuple, optional): les bornes de la loi uniforme pour la génération de l'effet fixe des patients. Defaults to (-1, 1).\n",
    "        psi_law_graph (tuple, optional): les bornes de la loi uniforme pour la génération de l'effet fixe des docteurs. Defaults to (-1, 1).\n",
    "        nb_latent_factors (_type_, optional): Le nombre de vecteurs latents, par défaut, on les génère aléatoirement. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Le dataframe voulu.\n",
    "    \"\"\"\n",
    "    coor_patients = []\n",
    "    coor_doctors = []\n",
    "    alpha_graph = []\n",
    "    psi_graph = []\n",
    "    rng = np.random.default_rng(None)\n",
    "    D = np.zeros([n_patients, n_doctors], dtype = np.ndarray)\n",
    "\n",
    "    if nb_latent_factors == None: # We pick a random number of latent factors\n",
    "        random_number = random.randint(1, 10)\n",
    "        for i in range(n_patients):\n",
    "            \n",
    "            # We generate the FE for the graph formation model\n",
    "            alpha_graph.append( np.random.uniform(alpha_law_graph[0], alpha_law_graph[1], size = random_number) )\n",
    "\n",
    "        for j in range(n_doctors):\n",
    "\n",
    "            # We generate the FE for the graph formation model\n",
    "            psi_graph.append( np.random.uniform(psi_law_graph[0], psi_law_graph[1], size = random_number ) )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        for i in range(n_patients):\n",
    "            \n",
    "            # We generate the FE for the graph formation model\n",
    "            alpha_graph.append( np.random.uniform(alpha_law_graph[0], alpha_law_graph[1], size = nb_latent_factors) )\n",
    "\n",
    "        for j in range(n_doctors):\n",
    "\n",
    "            # We generate the FE for the graph formation model\n",
    "            psi_graph.append( np.random.uniform(psi_law_graph[0], psi_law_graph[1], size = nb_latent_factors ) )\n",
    "\n",
    "    # Generate distance matrix\n",
    "    if type_distance == 'default':\n",
    "        for i in range(n_patients):\n",
    "            # Generate the coordinates of the patients\n",
    "            coor_patients.append( np.random.uniform(0, 1, 2) )\n",
    "            for j in range(n_doctors):\n",
    "                if i == 0: # We ensure each coordinate is generated once for each doctor\n",
    "                    # Generate the coordinates of the doctors\n",
    "                    coor_doctors.append( np.random.uniform(0, 1, 2) )\n",
    "                d = np.sqrt(np.power((coor_patients[i][0] - coor_doctors[j][0]), 2) + np.power((coor_patients[i][1] - coor_doctors[j][1]), 2))\n",
    "                D[i][j] = d\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Assign randomly a CODGEO, DEP, or REG to patients and doctors\n",
    "        dist_matrix = pd.read_csv('../Data/' + type_distance + '.csv')\n",
    "        \n",
    "        del dist_matrix[dist_matrix.columns[0]]\n",
    "        dist_matrix.index = dist_matrix.columns\n",
    "        for i in range(len(dist_matrix)):\n",
    "            dist_matrix.iloc[i, i] = 0\n",
    "        arr = dist_matrix.columns.values\n",
    "        for i, col in enumerate(arr):\n",
    "            arr[i] = int(float(arr[i]))\n",
    "        dist_matrix.columns = arr\n",
    "        dist_matrix.index = arr\n",
    "\n",
    "        # Generate code for patient and doctor\n",
    "        code_patient = []\n",
    "        code_doctor = []\n",
    "        for i in range(n_patients):\n",
    "            random_code = np.random.choice(dist_matrix.columns.values)\n",
    "            code_patient.append( random_code )\n",
    "        for j in range(n_doctors):\n",
    "            random_code = np.random.choice(dist_matrix.columns.values)\n",
    "            code_doctor.append( random_code )\n",
    "        for i in range(n_patients):\n",
    "            for j in range(n_doctors):\n",
    "                D[i, j] = dist_matrix.loc[code_patient[i], code_doctor[j]]\n",
    "\n",
    "    # D_normed = ( D - D.mean() ) / D.std()\n",
    "    # Random draws of ages for patients and doctors\n",
    "    sim_patient_age = rng.integers(low = 1, high = 99, size = n_patients)\n",
    "    sim_patient_age_normed = ( sim_patient_age - sim_patient_age.mean() ) / sim_patient_age.std()\n",
    "    sim_doctor_age = rng.integers(low = 26, high = 99, size = n_doctors)\n",
    "    sim_doctor_age_normed = ( sim_doctor_age - sim_doctor_age.mean() ) / sim_doctor_age.std()\n",
    "\n",
    "    # Random draws of genders of patients and doctors\n",
    "    sim_patient_gender = np.random.choice(np.array([0, 1]), n_patients)\n",
    "    sim_doctor_gender = np.random.choice(np.array([0, 1]), n_doctors)\n",
    "    sim_patient_gender_normed = ( sim_patient_gender - sim_patient_gender.mean() ) / sim_patient_gender.std()\n",
    "    sim_doctor_gender_normed = ( sim_doctor_gender - sim_doctor_gender.mean() ) / sim_doctor_gender.std()\n",
    "\n",
    "    # Compile ids\n",
    "    id_p = np.repeat(range(n_patients), n_doctors)\n",
    "    id_d = np.tile(range(n_doctors), n_patients)\n",
    "\n",
    "    # Compile observed features\n",
    "    age_p_data = np.repeat(sim_patient_age, n_doctors)\n",
    "    age_d_data = np.tile(sim_doctor_age, n_patients)\n",
    "    sex_p_data = np.repeat(sim_patient_gender, n_doctors)\n",
    "    sex_d_data = np.tile(sim_doctor_gender, n_patients)\n",
    "    if type_distance != 'default':\n",
    "        code_patient_data = np.repeat(code_patient, n_doctors)\n",
    "        code_doctor_data = np.tile(code_doctor, n_patients)\n",
    "    # # P is the matrix with all the connection probabilities\n",
    "    # P = np.zeros((n_patients, n_doctors))\n",
    "    # Generate the identifier matrix A based on the distance\n",
    "    A = np.zeros([n_patients, n_doctors], dtype = np.ndarray)\n",
    "    for i in range(n_patients):\n",
    "        for j in range(n_doctors):\n",
    "            if D[i][j] <= z: # if patient i and doctor j are too far away, there is no relation\n",
    "                T = np.dot(alpha_graph[i], psi_graph[j]) + beta_age_p_graph * sim_patient_age_normed[i] + beta_age_d_graph * sim_doctor_age_normed[j] \\\n",
    "                + beta_sex_p_graph * sim_patient_gender_normed[i] + beta_sex_d_graph * sim_doctor_gender_normed[j] + beta_distance_graph * D[i][j]\n",
    "                p = 1 / (1 + np.exp(-T))\n",
    "                # P[i][j] = p\n",
    "                A[i][j] = np.random.binomial(1, p)\n",
    "    \n",
    "    # Compile relations between doctors and patients\n",
    "    relation = A.flatten()\n",
    "\n",
    "    # Merge all columns into a dataframe\n",
    "    dataframe = pd.DataFrame(data={'i': id_p, 'j': id_d, 'y' : relation, 'age_p': age_p_data, 'age_d': age_d_data, \n",
    "                           'sex_p': sex_p_data, 'sex_d': sex_d_data\n",
    "                            })\n",
    "    dataframe['distance'] = D[dataframe['i'], dataframe['j']].astype(float)\n",
    "\n",
    "    # Now, we bound the number of connections (1 <= connections <= max_number_connections)\n",
    "    # First, we detect the patients who have 0 connection.\n",
    "    number_of_connections = dataframe.groupby('i').agg({'y': 'sum'})\n",
    "    zero_connection = number_of_connections[number_of_connections['y'] == 0].index\n",
    "    for patient in zero_connection:\n",
    "        # If patient has zero connection, we connect him with the nearest doctor (even if the threshold z isn't respected)\n",
    "        min_index = dataframe[dataframe['i'] == patient]['distance'].idxmin()\n",
    "        doctor_to_connect = dataframe.loc[min_index, 'j']\n",
    "        dataframe.loc[(dataframe['i'] == patient) & (dataframe['j'] == doctor_to_connect), 'y'] = 1\n",
    "        \n",
    "    # We also detect the doctors who have 0 connection.\n",
    "    number_of_connections = dataframe.groupby('j').agg({'y': 'sum'})\n",
    "    zero_connection = number_of_connections[number_of_connections['y'] == 0].index\n",
    "    for doctor in zero_connection:\n",
    "        # If doctor has zero connection, we connect him with the nearest patient (even if the threshold z isn't respected)\n",
    "        min_index = dataframe[dataframe['j'] == doctor]['distance'].idxmin()\n",
    "        patient_to_connect = dataframe.loc[min_index, 'i']\n",
    "        dataframe.loc[(dataframe['j'] == doctor) & (dataframe['j'] == patient_to_connect), 'y'] = 1\n",
    "    \n",
    "    # Then, we detect the patients who have more than max_number_connections. We choose the remaining connections by doctor's popularities (possible to choose randomly).\n",
    "    # Note : on ne s'assure pas ici qu'un docteur pourra finir par avoir 0 connexion (cas très peu probable).\n",
    "    number_of_connections = dataframe.groupby('i').agg({'y': 'sum'})\n",
    "    too_much_connection = number_of_connections[number_of_connections['y'] > max_number_connections].index\n",
    "    for patient in too_much_connection:\n",
    "        # We keep the connections with the most popular doctors\n",
    "        patient_df = dataframe[dataframe['i'] == patient]\n",
    "        connected_doctors = patient_df[patient_df['y'] == 1]['j'].values\n",
    "        most_popular_doctors = dataframe[dataframe['j'].isin(connected_doctors)].groupby('j').agg({'y': 'sum'}).sort_values('y', ascending=False)\n",
    "        not_kept_doctors = most_popular_doctors.index[max_number_connections:].values\n",
    "        for doctor in not_kept_doctors:\n",
    "            dataframe.loc[(dataframe['i'] == patient) & (dataframe['j'] == doctor), 'y'] = 0\n",
    "\n",
    "    # Create a dataframe of fixed effects for patients and doctors, then concatenate it with all the data\n",
    "    k = len(alpha_graph[0]) # Number of latent factors\n",
    "    ef_patient = pd.DataFrame(np.zeros((n_patients*n_doctors, k)))\n",
    "    ef_doctor = pd.DataFrame(np.zeros((n_patients*n_doctors, k)))\n",
    "    for i in range(k):\n",
    "        ef_patient_element = []\n",
    "        ef_doctor_element = []\n",
    "        ef_patient.rename(columns = {i :f'ef_p_{i}'}, inplace = True)\n",
    "        ef_doctor.rename(columns = {i :f'ef_d_{i}'}, inplace = True)\n",
    "        for j in range(n_patients):\n",
    "            \n",
    "            ef_patient_element += list(np.repeat(alpha_graph[j][i], n_doctors))\n",
    "\n",
    "        for j in range(n_doctors):\n",
    "            ef_doctor_element.append(psi_graph[j][i])\n",
    "        \n",
    "        ef_patient[f'ef_p_{i}'] = ef_patient_element\n",
    "        ef_doctor[f'ef_d_{i}'] = np.tile(ef_doctor_element, n_patients)\n",
    "    dataframe = pd.concat([dataframe, ef_patient, ef_doctor], axis = 1)\n",
    "    dataframe = dataframe.reset_index().drop(['index'], axis = 1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = graph_formation(n_patients=500,\n",
    "                    n_doctors=10,\n",
    "                    max_number_connections=2,\n",
    "                    z=0.5,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.1,\n",
    "                    beta_age_d_graph=0.1,\n",
    "                    beta_sex_p_graph=0.1,\n",
    "                    beta_sex_d_graph=0.1,\n",
    "                    beta_distance_graph=-1,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = 5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    4213\n",
       "1     787\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>y</th>\n",
       "      <th>age_p</th>\n",
       "      <th>age_d</th>\n",
       "      <th>sex_p</th>\n",
       "      <th>sex_d</th>\n",
       "      <th>distance</th>\n",
       "      <th>ef_p_0</th>\n",
       "      <th>ef_p_1</th>\n",
       "      <th>ef_p_2</th>\n",
       "      <th>ef_p_3</th>\n",
       "      <th>ef_p_4</th>\n",
       "      <th>ef_d_0</th>\n",
       "      <th>ef_d_1</th>\n",
       "      <th>ef_d_2</th>\n",
       "      <th>ef_d_3</th>\n",
       "      <th>ef_d_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.450207</td>\n",
       "      <td>0.850586</td>\n",
       "      <td>0.565199</td>\n",
       "      <td>-0.735455</td>\n",
       "      <td>0.961413</td>\n",
       "      <td>-0.309081</td>\n",
       "      <td>0.966415</td>\n",
       "      <td>0.849090</td>\n",
       "      <td>0.578151</td>\n",
       "      <td>0.040308</td>\n",
       "      <td>-0.062714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.214586</td>\n",
       "      <td>0.850586</td>\n",
       "      <td>0.565199</td>\n",
       "      <td>-0.735455</td>\n",
       "      <td>0.961413</td>\n",
       "      <td>-0.309081</td>\n",
       "      <td>0.983086</td>\n",
       "      <td>-0.262288</td>\n",
       "      <td>-0.653916</td>\n",
       "      <td>0.509942</td>\n",
       "      <td>0.382328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196875</td>\n",
       "      <td>0.850586</td>\n",
       "      <td>0.565199</td>\n",
       "      <td>-0.735455</td>\n",
       "      <td>0.961413</td>\n",
       "      <td>-0.309081</td>\n",
       "      <td>-0.846554</td>\n",
       "      <td>-0.465794</td>\n",
       "      <td>-0.958816</td>\n",
       "      <td>0.123867</td>\n",
       "      <td>-0.527545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.547644</td>\n",
       "      <td>0.850586</td>\n",
       "      <td>0.565199</td>\n",
       "      <td>-0.735455</td>\n",
       "      <td>0.961413</td>\n",
       "      <td>-0.309081</td>\n",
       "      <td>-0.401191</td>\n",
       "      <td>-0.906268</td>\n",
       "      <td>-0.105887</td>\n",
       "      <td>-0.649086</td>\n",
       "      <td>0.603314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.502395</td>\n",
       "      <td>0.850586</td>\n",
       "      <td>0.565199</td>\n",
       "      <td>-0.735455</td>\n",
       "      <td>0.961413</td>\n",
       "      <td>-0.309081</td>\n",
       "      <td>-0.760111</td>\n",
       "      <td>-0.650895</td>\n",
       "      <td>0.123246</td>\n",
       "      <td>0.978786</td>\n",
       "      <td>-0.576945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i  j  y  age_p  age_d  sex_p  sex_d  distance    ef_p_0    ef_p_1  \\\n",
       "0  0  0  0     31     72      0      1  0.450207  0.850586  0.565199   \n",
       "1  0  1  1     31     94      0      1  0.214586  0.850586  0.565199   \n",
       "2  0  2  1     31     60      0      1  0.196875  0.850586  0.565199   \n",
       "3  0  3  0     31     87      0      0  0.547644  0.850586  0.565199   \n",
       "4  0  4  0     31     89      0      0  0.502395  0.850586  0.565199   \n",
       "\n",
       "     ef_p_2    ef_p_3    ef_p_4    ef_d_0    ef_d_1    ef_d_2    ef_d_3  \\\n",
       "0 -0.735455  0.961413 -0.309081  0.966415  0.849090  0.578151  0.040308   \n",
       "1 -0.735455  0.961413 -0.309081  0.983086 -0.262288 -0.653916  0.509942   \n",
       "2 -0.735455  0.961413 -0.309081 -0.846554 -0.465794 -0.958816  0.123867   \n",
       "3 -0.735455  0.961413 -0.309081 -0.401191 -0.906268 -0.105887 -0.649086   \n",
       "4 -0.735455  0.961413 -0.309081 -0.760111 -0.650895  0.123246  0.978786   \n",
       "\n",
       "     ef_d_4  \n",
       "0 -0.062714  \n",
       "1  0.382328  \n",
       "2 -0.527545  \n",
       "3  0.603314  \n",
       "4 -0.576945  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF + Hard Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negative_sampling_first_step(dataframe,\n",
    "                                      nb_Y = 1,\n",
    "                                     ):\n",
    "    \"\"\"à partir de notre dataframe, on doit pouvoir générer des Y = 0 pertinents. Pour cela, il faut effectuer un entrainement initial sur notre dataframe pour \n",
    "    avoir une première estimation des paramètres de notre modèle (EF, Betas) et ensuite effectuer ce HNS.\n",
    "\n",
    "    Args:\n",
    "        dataframe (_type_): Le dataframe initial sur lequel on applique la première étape de notre Hard Negative Sampling.\n",
    "        nb_Y (int, optional): Le nombre de Y=0 que l'on souhaite échantillonner. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Renvoie le dataframe échantillonné.\n",
    "    \"\"\"\n",
    "    df = dataframe.copy()\n",
    "    # Hard Negative Sampling : \n",
    "\n",
    "    # On enregistre l'ensemble des Y = 0 avant de les supprimer de notre dataframe\n",
    "    negative_connections = df[df['y'] == 0]\n",
    "    negative_patients = negative_connections['i'].unique()\n",
    "    # On supprime les Y = 0 car on va les générer avec le HNS\n",
    "    df.drop(df[df['y'] == 0].index, inplace = True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    # D'abord, on effectue l'étape 1 du HNS. Il nous faut une première estimation des EFs/Betas. On garde un Y=0 en se basant sur une distribution dépendant de la\n",
    "    # popularité/distance à chaque docteur. Pour chaque patient, on garde en mémoire le(s) Y=0 le plus intéressant.\n",
    "    negative_connections_first_step = negative_connections.copy()\n",
    "    for i in negative_patients:\n",
    "        patient_df = negative_connections_first_step[negative_connections_first_step['i'] == i]\n",
    "        distance = np.zeros(len(patient_df)) # Array pour stocker les distances. Il faut faire attention, indice de l'array =/= indice \"réel\" du docteur\n",
    "        popularity = np.zeros(len(patient_df)) # Array pour stocker les popularités\n",
    "        for j, doctor in enumerate(patient_df['j']):\n",
    "            distance[j] = patient_df[patient_df['j'] == doctor]['distance'].iloc[0]\n",
    "            popularity[j] = df[df['j'] == doctor]['y'].sum()\n",
    "        score = popularity / distance\n",
    "        score = score/score.sum()\n",
    "        \n",
    "        # On conserve les échantillons négatifs les plus significatifs (ceux pour lesquels la probabilité de connexion est la plus grande mais n'a pas eu lieu)\n",
    "        best_score_array = score.argsort()[-nb_Y:]\n",
    "        doctors_chosen = patient_df['j'].iloc[best_score_array].to_list()\n",
    "        doctors_to_eliminate = patient_df[patient_df['j'].isin([j for j in patient_df['j'] if j not in doctors_chosen])]\n",
    "        negative_connections_first_step.drop(doctors_to_eliminate.index, inplace = True)\n",
    "    # On a maintenant le Y = 0 désiré pour chaque patient, il faut fusionner les Y = 0 et Y = 1\n",
    "    return pd.concat([df, negative_connections_first_step]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_17104\\2952294950.py:2: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf # Pour les Warnings de dépréciation de méthodes\n",
    "tf.disable_eager_execution()\n",
    "# Pour arrêter la descente lorsque la loss est suffisamment faible\n",
    "class StopTrainingBelowLoss(Callback):\n",
    "    def __init__(self, target_loss):\n",
    "        super(StopTrainingBelowLoss, self).__init__()\n",
    "        self.target_loss = target_loss\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('loss') is not None and logs.get('loss') < self.target_loss:\n",
    "            print(f\"\\nTraining stopped as the loss reached below {self.target_loss}\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Récupérer la valeur de la loss (utile si on ne fixe pas de seuil à atteindre)\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super(LossHistory, self).__init__()\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "def get_estimations(df, nb_epochs=50, initial_weights=None, target_loss=None):\n",
    "    \"\"\"Etant donné un dataframe, get_estimations renvoie l'estimation des effets fixes et des Bêtas en utilisant TensorFlow.keras\n",
    "\n",
    "    Args:\n",
    "        df (_type_): le dataframe pour lequel on souhaite obtenir l'estimation\n",
    "        nb_epochs (int, optional): Le nombre d'itérations pour notre descente de gradient. Defaults to 50.\n",
    "        initial_weights (_type_, optional): Si on a déjà lancé un entraînement et qu'on souhaite le poursuivre, on peut rentrer l'estimation obtenue auparavant\n",
    "        (voir plus bas pour un exemple). Defaults to None.\n",
    "        target_loss (_type_, optional): Si une valeur est indiquée, la descente de gradient ne s'arrête pas tant que la loss n'est pas inférieure\n",
    "        à cette valeur. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: l'ensemble des estimations (voir plus bas pour un exemple)\n",
    "    \"\"\"\n",
    "    # Définition des dimensions\n",
    "    num_patients = df['i'].nunique()\n",
    "    num_doctors = df['j'].nunique()\n",
    "    embedding_dim = int(len(df.columns[8:])/2) # Dimension des vecteurs d'effets fixes\n",
    "\n",
    "    # Créer un LabelEncoder pour les ID des patients et des docteurs, utile si les id ne sont pas nécessairement une suite consécutive d'entiers (1, 2, ..., n)\n",
    "    patient_encoder = LabelEncoder()\n",
    "    i_encoded = patient_encoder.fit_transform(df['i'])\n",
    "\n",
    "    doctor_encoder = LabelEncoder()\n",
    "    j_encoded = doctor_encoder.fit_transform(df['j'])\n",
    "\n",
    "    # Enregistrer les mappings ID -> entier pour retrouver les bons indices associés à chaque patient/docteur si besoin\n",
    "    patient_id_mapping = dict(zip(patient_encoder.classes_, range(num_patients)))\n",
    "    doctor_id_mapping = dict(zip(doctor_encoder.classes_, range(num_doctors)))\n",
    "\n",
    "    y = df['y'].values.astype(np.float32)\n",
    "    df = df.astype(np.float32)\n",
    "    X = [i_encoded, j_encoded, df['age_p'].values, df['age_d'].values, df['sex_p'].values,\\\n",
    "            df['sex_d'].values, df['distance'].values]\n",
    "    \n",
    "    # Entrées du modèle\n",
    "    user_input = Input(shape=(1,))\n",
    "    doctor_input = Input(shape=(1,))\n",
    "    age_patient_input = Input(shape=(1,), name='age_p')\n",
    "    age_doctor_input = Input(shape=(1,), name='age_d')\n",
    "    sex_patient_input = Input(shape=(1,), name='sex_p')\n",
    "    sex_doctor_input = Input(shape=(1,), name='sex_d')\n",
    "    distance_input = Input(shape=(1,))\n",
    "\n",
    "    # Incorporation des utilisateurs et des docteurs dans des espaces latents\n",
    "    user_embedding = Embedding(input_dim=num_patients, output_dim=embedding_dim, input_length=1)(user_input)\n",
    "    doctor_embedding = Embedding(input_dim=num_doctors, output_dim=embedding_dim, input_length=1)(doctor_input)\n",
    "\n",
    "    # Obtention des vecteurs latents des utilisateurs et des docteurs\n",
    "    user_latent = Flatten()(user_embedding)\n",
    "    doctor_latent = Flatten()(doctor_embedding)\n",
    "\n",
    "    # Produit scalaire entre les vecteurs latents des utilisateurs et des docteurs\n",
    "    dot_product = Dot(axes=1)([user_latent, doctor_latent])\n",
    "\n",
    "    # Création d'une couche pour paramétriser les Beta\n",
    "    class CustomLayer(Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(CustomLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.beta_age_patient = self.add_weight(shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.beta_age_doctor = self.add_weight(shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.beta_sexe_patient = self.add_weight(shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.beta_sexe_doctor = self.add_weight(shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.beta_distance = self.add_weight(shape=(1,), initializer='random_normal', trainable=True)\n",
    "            super(CustomLayer, self).build(input_shape)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            age_patient_input, age_doctor_input, sex_patient_input, sex_doctor_input, distance_input = inputs\n",
    "            linear_term = self.beta_age_patient * age_patient_input + \\\n",
    "                        self.beta_age_doctor * age_doctor_input + \\\n",
    "                        self.beta_sexe_patient * sex_patient_input + \\\n",
    "                        self.beta_sexe_doctor * sex_doctor_input + \\\n",
    "                        self.beta_distance * distance_input\n",
    "            return linear_term\n",
    "\n",
    "    # Ajout de la couche personnalisée dans le modèle\n",
    "    linear_term = CustomLayer()([age_patient_input, age_doctor_input, sex_patient_input, sex_doctor_input, distance_input])\n",
    "    output = Add()([dot_product, linear_term])\n",
    "    output = Activation('sigmoid')(output)\n",
    "\n",
    "    # Création du modèle\n",
    "    model = Model(inputs=[user_input, doctor_input, age_patient_input, age_doctor_input, sex_patient_input, sex_doctor_input, distance_input], outputs=output)\n",
    "\n",
    "    # Compilation du modèle\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Chargement des poids initiaux si disponibles (utile pour reprendre l'entraînement)\n",
    "    if initial_weights != None:\n",
    "        model.set_weights(initial_weights)\n",
    "\n",
    "    # Ajout du rappel pour arrêter l'entraînement seulement si la perte atteint un seuil\n",
    "    if target_loss != None:\n",
    "        callbacks = []\n",
    "        callbacks.append(StopTrainingBelowLoss(target_loss))\n",
    "        # Initialisation du nombre d'époques\n",
    "        epoch = 0\n",
    "    # Boucle d'entraînement jusqu'à ce que la perte atteigne le seuil\n",
    "        while True:\n",
    "            # Entraînement d'une époque\n",
    "            model.fit(X, y, epochs=1, batch_size=64, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
    "\n",
    "            # Incrémentation du nombre d'époques\n",
    "            epoch += 1\n",
    "\n",
    "            # Vérification si l'entraînement doit être arrêté\n",
    "            if model.stop_training:\n",
    "                break\n",
    "        return epoch, model.get_weights(), patient_id_mapping, doctor_id_mapping\n",
    "    else:\n",
    "        # Ajout du rappel pour enregistrer la perte\n",
    "        loss_history = LossHistory()\n",
    "        callbacks = [loss_history]\n",
    "        # Entraînement du modèle\n",
    "        model.fit(X, y, epochs=nb_epochs, batch_size=64, validation_split=0.2, verbose=1, callbacks=callbacks)\n",
    "        # model.fit(X, y, epochs=nb_epochs, batch_size=64, validation_split=0.2, verbose=0, callbacks=callbacks) # Pour ne pas afficher le print des epochs\n",
    "        loss_values = loss_history.losses\n",
    "        return loss_values, model.get_weights(), patient_id_mapping, doctor_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrouver l'ensemble des estimations\n",
    "\n",
    "# ef_patient = model.get_weights()[0]\n",
    "# ef_doctor = model.get_weights()[1]\n",
    "# beta_age_p = model.get_weights()[2]\n",
    "# beta_age_d = model.get_weights()[3]\n",
    "# beta_sex_p = model.get_weights()[4]\n",
    "# beta_sex_d = model.get_weights()[5]\n",
    "# beta_distance = model.get_weights()[6]\n",
    "# Beta = [beta_age_p, beta_age_d, beta_sex_p, beta_sex_d, beta_distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\version_utils.py:76: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:635: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_utils_v1.py:50: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 88us/sample - loss: 3.6190 - accuracy: 0.2477 - val_loss: 1.4510 - val_accuracy: 0.4440\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.8266 - accuracy: 0.6357 - val_loss: 0.6452 - val_accuracy: 0.7390\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "2688/4000 [===================>..........] - ETA: 0s - loss: 0.6140 - accuracy: 0.7697"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.6214 - accuracy: 0.7678 - val_loss: 0.5889 - val_accuracy: 0.7720\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.5725 - accuracy: 0.7865 - val_loss: 0.5442 - val_accuracy: 0.7850\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.5270 - accuracy: 0.8167 - val_loss: 0.5038 - val_accuracy: 0.8370\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.4886 - accuracy: 0.8415 - val_loss: 0.4751 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.4597 - accuracy: 0.8420 - val_loss: 0.4569 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.4385 - accuracy: 0.8420 - val_loss: 0.4482 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 33us/sample - loss: 0.4226 - accuracy: 0.8420 - val_loss: 0.4421 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.4099 - accuracy: 0.8420 - val_loss: 0.4393 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.3976 - accuracy: 0.8420 - val_loss: 0.4367 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.3869 - accuracy: 0.8420 - val_loss: 0.4349 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.3760 - accuracy: 0.8422 - val_loss: 0.4346 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.3665 - accuracy: 0.8453 - val_loss: 0.4323 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.3575 - accuracy: 0.8512 - val_loss: 0.4303 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.3489 - accuracy: 0.8545 - val_loss: 0.4293 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 28us/sample - loss: 0.3413 - accuracy: 0.8633 - val_loss: 0.4286 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.3342 - accuracy: 0.8680 - val_loss: 0.4268 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.3280 - accuracy: 0.8737 - val_loss: 0.4258 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.3220 - accuracy: 0.8752 - val_loss: 0.4253 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.3165 - accuracy: 0.8780 - val_loss: 0.4237 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.3112 - accuracy: 0.8790 - val_loss: 0.4232 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.3067 - accuracy: 0.8770 - val_loss: 0.4226 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.3020 - accuracy: 0.8790 - val_loss: 0.4221 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.2980 - accuracy: 0.8783 - val_loss: 0.4205 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.2937 - accuracy: 0.8780 - val_loss: 0.4197 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.2899 - accuracy: 0.8788 - val_loss: 0.4212 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.2854 - accuracy: 0.8792 - val_loss: 0.4179 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.2814 - accuracy: 0.8820 - val_loss: 0.4163 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.2775 - accuracy: 0.8838 - val_loss: 0.4198 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.2740 - accuracy: 0.8823 - val_loss: 0.4209 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 37us/sample - loss: 0.2699 - accuracy: 0.8860 - val_loss: 0.4190 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.2665 - accuracy: 0.8865 - val_loss: 0.4147 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.2623 - accuracy: 0.8865 - val_loss: 0.4142 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.2588 - accuracy: 0.8875 - val_loss: 0.4155 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.2550 - accuracy: 0.8892 - val_loss: 0.4139 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 30us/sample - loss: 0.2506 - accuracy: 0.8885 - val_loss: 0.4111 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.2471 - accuracy: 0.8888 - val_loss: 0.4144 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.2436 - accuracy: 0.8905 - val_loss: 0.4159 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.2396 - accuracy: 0.8907 - val_loss: 0.4208 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.2371 - accuracy: 0.8940 - val_loss: 0.4180 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.2360 - accuracy: 0.9062WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0006s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.2325 - accuracy: 0.8920 - val_loss: 0.4151 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.2288 - accuracy: 0.8960 - val_loss: 0.4140 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.2251 - accuracy: 0.8967 - val_loss: 0.4144 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.2210 - accuracy: 0.9038 - val_loss: 0.4123 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.2173 - accuracy: 0.9020 - val_loss: 0.4034 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.2131 - accuracy: 0.9065 - val_loss: 0.4169 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.2092 - accuracy: 0.9072 - val_loss: 0.4067 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.2057 - accuracy: 0.9100 - val_loss: 0.4092 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.2016 - accuracy: 0.9133 - val_loss: 0.4109 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.1982 - accuracy: 0.9168 - val_loss: 0.4059 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.1943 - accuracy: 0.9205 - val_loss: 0.4145 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.1910 - accuracy: 0.9222 - val_loss: 0.4177 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.1873 - accuracy: 0.9258 - val_loss: 0.4030 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.1835 - accuracy: 0.9315 - val_loss: 0.4085 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.1796 - accuracy: 0.9352 - val_loss: 0.4102 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.1765 - accuracy: 0.9348 - val_loss: 0.4052 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 39us/sample - loss: 0.1730 - accuracy: 0.9425 - val_loss: 0.4292 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.1695 - accuracy: 0.9463 - val_loss: 0.4117 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.1655 - accuracy: 0.9475 - val_loss: 0.4103 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.1622 - accuracy: 0.9503 - val_loss: 0.4116 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.1586 - accuracy: 0.9532 - val_loss: 0.4166 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 31us/sample - loss: 0.1558 - accuracy: 0.9550 - val_loss: 0.4121 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.1520 - accuracy: 0.9563 - val_loss: 0.4038 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.1490 - accuracy: 0.9590 - val_loss: 0.4236 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 30us/sample - loss: 0.1454 - accuracy: 0.9600 - val_loss: 0.4075 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 28us/sample - loss: 0.1427 - accuracy: 0.9625 - val_loss: 0.4086 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 32us/sample - loss: 0.1398 - accuracy: 0.9622 - val_loss: 0.4214 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 28us/sample - loss: 0.1372 - accuracy: 0.9625 - val_loss: 0.4034 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.1336 - accuracy: 0.9657 - val_loss: 0.4275 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 44us/sample - loss: 0.1310 - accuracy: 0.9675 - val_loss: 0.4338 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 29us/sample - loss: 0.1290 - accuracy: 0.9665 - val_loss: 0.4135 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.1253 - accuracy: 0.9695 - val_loss: 0.4152 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 32us/sample - loss: 0.1234 - accuracy: 0.9680 - val_loss: 0.4356 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 32us/sample - loss: 0.1208 - accuracy: 0.9700 - val_loss: 0.4258 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 31us/sample - loss: 0.1179 - accuracy: 0.9705 - val_loss: 0.4239 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 28us/sample - loss: 0.1151 - accuracy: 0.9715 - val_loss: 0.4307 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.1129 - accuracy: 0.9718 - val_loss: 0.4324 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 28us/sample - loss: 0.1104 - accuracy: 0.9728 - val_loss: 0.4134 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 31us/sample - loss: 0.1082 - accuracy: 0.9720 - val_loss: 0.4210 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.1060 - accuracy: 0.9735 - val_loss: 0.4389 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.1037 - accuracy: 0.9747 - val_loss: 0.4270 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 38us/sample - loss: 0.1016 - accuracy: 0.9743 - val_loss: 0.4174 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 31us/sample - loss: 0.0997 - accuracy: 0.9743 - val_loss: 0.4273 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0973 - accuracy: 0.9760 - val_loss: 0.4362 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0957 - accuracy: 0.9765 - val_loss: 0.4389 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.0935 - accuracy: 0.9772 - val_loss: 0.4405 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0916 - accuracy: 0.9783 - val_loss: 0.4572 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0899 - accuracy: 0.9783 - val_loss: 0.4586 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.0881 - accuracy: 0.9787 - val_loss: 0.4280 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0866 - accuracy: 0.9790 - val_loss: 0.4308 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0848 - accuracy: 0.9795 - val_loss: 0.4382 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 28us/sample - loss: 0.0832 - accuracy: 0.9790 - val_loss: 0.4363 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0813 - accuracy: 0.9795 - val_loss: 0.4546 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0799 - accuracy: 0.9795 - val_loss: 0.4462 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0782 - accuracy: 0.9795 - val_loss: 0.4420 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0768 - accuracy: 0.9797 - val_loss: 0.4602 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0753 - accuracy: 0.9800 - val_loss: 0.4677 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0739 - accuracy: 0.9810 - val_loss: 0.4623 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.0726 - accuracy: 0.9810 - val_loss: 0.4695 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0711 - accuracy: 0.9820 - val_loss: 0.4504 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0697 - accuracy: 0.9815 - val_loss: 0.4722 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 30us/sample - loss: 0.0686 - accuracy: 0.9833 - val_loss: 0.4594 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0671 - accuracy: 0.9825 - val_loss: 0.4638 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0661 - accuracy: 0.9837 - val_loss: 0.4891 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.0649 - accuracy: 0.9835 - val_loss: 0.4738 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0634 - accuracy: 0.9847 - val_loss: 0.4886 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 49us/sample - loss: 0.0623 - accuracy: 0.9850 - val_loss: 0.4632 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 29us/sample - loss: 0.0612 - accuracy: 0.9855 - val_loss: 0.4637 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0602 - accuracy: 0.9847 - val_loss: 0.4745 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0589 - accuracy: 0.9862 - val_loss: 0.4904 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0581 - accuracy: 0.9862 - val_loss: 0.4597 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0570 - accuracy: 0.9862 - val_loss: 0.4892 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0557 - accuracy: 0.9865 - val_loss: 0.4807 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.0549 - accuracy: 0.9870 - val_loss: 0.4697 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0537 - accuracy: 0.9877 - val_loss: 0.4893 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0528 - accuracy: 0.9877 - val_loss: 0.4979 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.0518 - accuracy: 0.9868 - val_loss: 0.4579 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0509 - accuracy: 0.9872 - val_loss: 0.4781 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0499 - accuracy: 0.9880 - val_loss: 0.4727 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0495 - accuracy: 0.9893 - val_loss: 0.4900 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0480 - accuracy: 0.9880 - val_loss: 0.5040 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0474 - accuracy: 0.9885 - val_loss: 0.4974 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0461 - accuracy: 0.9883 - val_loss: 0.4686 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0458 - accuracy: 0.9893 - val_loss: 0.5119 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0447 - accuracy: 0.9890 - val_loss: 0.4940 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0439 - accuracy: 0.9895 - val_loss: 0.4964 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0431 - accuracy: 0.9898 - val_loss: 0.5218 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0421 - accuracy: 0.9902 - val_loss: 0.5123 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0414 - accuracy: 0.9900 - val_loss: 0.5098 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 34us/sample - loss: 0.0406 - accuracy: 0.9905 - val_loss: 0.5118 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0398 - accuracy: 0.9902 - val_loss: 0.5150 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0390 - accuracy: 0.9915 - val_loss: 0.5126 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0384 - accuracy: 0.9915 - val_loss: 0.5367 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0380 - accuracy: 0.9915 - val_loss: 0.5034 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0373 - accuracy: 0.9920 - val_loss: 0.5124 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0362 - accuracy: 0.9918 - val_loss: 0.5019 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0003s vs `on_train_batch_end` time: 0.0008s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0358 - accuracy: 0.9923 - val_loss: 0.5163 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0351 - accuracy: 0.9925 - val_loss: 0.5484 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0347 - accuracy: 0.9927 - val_loss: 0.5291 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0338 - accuracy: 0.9925 - val_loss: 0.5405 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0188 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0009s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0333 - accuracy: 0.9927 - val_loss: 0.5321 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0325 - accuracy: 0.9935 - val_loss: 0.5806 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0320 - accuracy: 0.9930 - val_loss: 0.5320 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0313 - accuracy: 0.9945 - val_loss: 0.5309 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0310 - accuracy: 0.9945 - val_loss: 0.5866 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0311 - accuracy: 0.9940 - val_loss: 0.5387 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0297 - accuracy: 0.9952 - val_loss: 0.5518 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0292 - accuracy: 0.9952 - val_loss: 0.5818 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0288 - accuracy: 0.9952 - val_loss: 0.5745 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0283 - accuracy: 0.9952 - val_loss: 0.5564 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 33us/sample - loss: 0.0284 - accuracy: 0.9965 - val_loss: 0.5837 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0271 - accuracy: 0.9952 - val_loss: 0.5560 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0267 - accuracy: 0.9960 - val_loss: 0.5636 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0264 - accuracy: 0.9962 - val_loss: 0.5882 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0261 - accuracy: 0.9967 - val_loss: 0.5961 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0257 - accuracy: 0.9973 - val_loss: 0.5860 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0251 - accuracy: 0.9965 - val_loss: 0.6069 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 20us/sample - loss: 0.0246 - accuracy: 0.9970 - val_loss: 0.6042 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0242 - accuracy: 0.9965 - val_loss: 0.6283 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0235 - accuracy: 0.9973 - val_loss: 0.5408 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0236 - accuracy: 0.9973 - val_loss: 0.5890 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0231 - accuracy: 0.9975 - val_loss: 0.6275 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0230 - accuracy: 0.9970 - val_loss: 0.5981 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0224 - accuracy: 0.9975 - val_loss: 0.5809 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0222 - accuracy: 0.9970 - val_loss: 0.6088 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0215 - accuracy: 0.9975 - val_loss: 0.5836 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0214 - accuracy: 0.9975 - val_loss: 0.5964 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0213 - accuracy: 0.9977 - val_loss: 0.6272 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0208 - accuracy: 0.9975 - val_loss: 0.5981 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.0204 - accuracy: 0.9975 - val_loss: 0.6019 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 33us/sample - loss: 0.0201 - accuracy: 0.9975 - val_loss: 0.6410 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 26us/sample - loss: 0.0197 - accuracy: 0.9975 - val_loss: 0.6372 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0195 - accuracy: 0.9975 - val_loss: 0.6361 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0192 - accuracy: 0.9975 - val_loss: 0.6151 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0360 - accuracy: 0.9844WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0191 - accuracy: 0.9973 - val_loss: 0.6216 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0186 - accuracy: 0.9975 - val_loss: 0.6505 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0184 - accuracy: 0.9977 - val_loss: 0.6658 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0183 - accuracy: 0.9975 - val_loss: 0.6327 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0178 - accuracy: 0.9975 - val_loss: 0.6177 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0178 - accuracy: 0.9975 - val_loss: 0.6319 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0173 - accuracy: 0.9977 - val_loss: 0.6785 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0171 - accuracy: 0.9975 - val_loss: 0.6430 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0169 - accuracy: 0.9975 - val_loss: 0.6476 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0169 - accuracy: 0.9975 - val_loss: 0.6590 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.0166 - accuracy: 0.9975 - val_loss: 0.6750 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0163 - accuracy: 0.9975 - val_loss: 0.6898 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0160 - accuracy: 0.9977 - val_loss: 0.7035 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 27us/sample - loss: 0.0161 - accuracy: 0.9980 - val_loss: 0.7020 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 40us/sample - loss: 0.0156 - accuracy: 0.9977 - val_loss: 0.6865 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 25us/sample - loss: 0.0154 - accuracy: 0.9975 - val_loss: 0.6802 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0153 - accuracy: 0.9977 - val_loss: 0.6624 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0150 - accuracy: 0.9977 - val_loss: 0.6957 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0149 - accuracy: 0.9980 - val_loss: 0.6678 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0150 - accuracy: 0.9977 - val_loss: 0.6818 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0146 - accuracy: 0.9977 - val_loss: 0.7020 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0143 - accuracy: 0.9977 - val_loss: 0.6668 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0142 - accuracy: 0.9980 - val_loss: 0.6802 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0140 - accuracy: 0.9977 - val_loss: 0.6823 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.6290 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0138 - accuracy: 0.9975 - val_loss: 0.6737 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "  64/4000 [..............................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_begin` time: 0.0003s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0135 - accuracy: 0.9977 - val_loss: 0.6698 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0139 - accuracy: 0.9983 - val_loss: 0.6957 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0133 - accuracy: 0.9980 - val_loss: 0.7192 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0132 - accuracy: 0.9980 - val_loss: 0.6485 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0131 - accuracy: 0.9980 - val_loss: 0.7547 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0129 - accuracy: 0.9975 - val_loss: 0.7096 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0129 - accuracy: 0.9977 - val_loss: 0.7232 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 32us/sample - loss: 0.0125 - accuracy: 0.9980 - val_loss: 0.6990 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0124 - accuracy: 0.9980 - val_loss: 0.6945 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0123 - accuracy: 0.9980 - val_loss: 0.7370 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0122 - accuracy: 0.9977 - val_loss: 0.7262 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0120 - accuracy: 0.9980 - val_loss: 0.7266 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 24us/sample - loss: 0.0120 - accuracy: 0.9980 - val_loss: 0.7006 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0117 - accuracy: 0.9980 - val_loss: 0.7440 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0116 - accuracy: 0.9980 - val_loss: 0.7282 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.7560 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 22us/sample - loss: 0.0115 - accuracy: 0.9977 - val_loss: 0.7678 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0114 - accuracy: 0.9980 - val_loss: 0.7305 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0111 - accuracy: 0.9980 - val_loss: 0.7591 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0111 - accuracy: 0.9980 - val_loss: 0.7678 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0109 - accuracy: 0.9980 - val_loss: 0.7523 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0108 - accuracy: 0.9980 - val_loss: 0.7419 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0107 - accuracy: 0.9980 - val_loss: 0.7370 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0106 - accuracy: 0.9980 - val_loss: 0.7630 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 23us/sample - loss: 0.0107 - accuracy: 0.9983 - val_loss: 0.7279 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0104 - accuracy: 0.9980 - val_loss: 0.7961 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0105 - accuracy: 0.9980 - val_loss: 0.7606 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 36us/sample - loss: 0.0102 - accuracy: 0.9980 - val_loss: 0.7826 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0101 - accuracy: 0.9980 - val_loss: 0.7447 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0100 - accuracy: 0.9980 - val_loss: 0.7627 - val_accuracy: 0.8450\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "3008/4000 [=====================>........] - ETA: 0s - loss: 0.0086 - accuracy: 0.9983\n",
      "Training stopped as the loss reached below 0.01\n",
      "4000/4000 [==============================] - 0s 21us/sample - loss: 0.0099 - accuracy: 0.9980 - val_loss: 0.7359 - val_accuracy: 0.8450\n"
     ]
    }
   ],
   "source": [
    "results = get_estimations(df, initial_weights=None, target_loss=0.01)\n",
    "# results = get_estimations(df, nb_epochs=50, initial_weights=None, target_loss=None) target_loss l'emporte sur nb_epochs, càd que si on fixe une loss à atteindre, le nombre d'itérations fixé\n",
    "# n'aura plus d'importance car on va faire autant d'itérations que nécessaire pour atteindre une loss faible.\n",
    "# results = get_estimations(df, nb_epochs=50, initial_weights=None, target_loss=0.01) -> on ne fait pas 50 epochs mais autant d'epochs que nécessaire pour avoir une loss <= 0.01\n",
    "\n",
    "ef_patient = results[1][0]\n",
    "ef_doctor = results[1][1]\n",
    "beta_age_p = results[1][2]\n",
    "beta_age_d = results[1][3]\n",
    "beta_sex_p = results[1][4]\n",
    "beta_sex_d = results[1][5]\n",
    "beta_distance = results[1][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = graph_formation(n_patients=1000,\n",
    "                    n_doctors=15,\n",
    "                    max_number_connections=2,\n",
    "                    z=0.5,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.1,\n",
    "                    beta_age_d_graph=0.1,\n",
    "                    beta_sex_p_graph=0.1,\n",
    "                    beta_sex_d_graph=0.1,\n",
    "                    beta_distance_graph=-1,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = 5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HNS_and_estimates(dataframe,\n",
    "                      M,\n",
    "                      nb_Y = 1,\n",
    "                      epochs=50,\n",
    "                      weights=None,\n",
    "                      threshold_loss=0.01,\n",
    "                      ):\n",
    "    \"\"\"Cette fonction effectue le Hard Negative Sampling -HNS) dans sa globalité et renvoie l'estimation finale des effets fixes, betas.\n",
    "\n",
    "    Args:\n",
    "        dataframe (_type_): Le dataframe initial\n",
    "        M (_type_): Le nombre associé au Top-M docteurs (le paramètre de notre HNS permettant de tirer aléatoirement parmi\n",
    "        les M docteurs les plus relevants, cf. papier de recherche associé)\n",
    "        nb_Y (int, optional): Le paramètre Y associé à la première étape du HNS. Defaults to 1.\n",
    "        epochs (int, optional): paramètre de get_estimations. Defaults to 50.\n",
    "        weights (_type_, optional): paramètre de get_estimations. Defaults to None.\n",
    "        threshold_loss (float, optional): paramètre de get_estimations. Defaults to 0.01.\n",
    "\n",
    "    Returns:\n",
    "        _type_: l'ensemble des estimations.\n",
    "    \"\"\"\n",
    "    df = dataframe.copy()\n",
    "    n_patients = dataframe['i'].nunique()\n",
    "    n_doctors = dataframe['j'].nunique()\n",
    "    # On récupère le dataframe ayant subi la première étape du HNS\n",
    "    df_first_hns = hard_negative_sampling_first_step(df, nb_Y)\n",
    "    \n",
    "    # On utilise get_first_estimations qui s'occupe d'effectuer la première estimation des EF/Bêtas.\n",
    "    parameters = get_estimations(df_first_hns, nb_epochs=epochs, initial_weights=weights, target_loss=threshold_loss)\n",
    "\n",
    "    # parameters[2] contient l'ensemble des premières estimations\n",
    "\n",
    "    # Maintenant, on estime les scores en se servant de la première estimation ci-dessus (https://arxiv.org/abs/2302.03472)\n",
    "    alpha_graph_training = parameters[1][0]\n",
    "    psi_graph_training = parameters[1][1]\n",
    "    beta_age_p_graph_training = parameters[1][2]\n",
    "    beta_age_d_graph_training = parameters[1][3]\n",
    "    beta_sex_p_graph_training = parameters[1][4]\n",
    "    beta_sex_d_graph_training = parameters[1][5]\n",
    "    beta_distance_graph_training =  parameters[1][6]\n",
    "    prediction_scores = np.zeros((n_patients, n_doctors))\n",
    "    negative_drawing_list = []\n",
    "    # Les scores estimés se basent sur notre modèle de LMF (https://arxiv.org/abs/2302.03472)\n",
    "    negative_connections = df[df['y'] == 0]\n",
    "    negative_patients = negative_connections['i'].unique()\n",
    "    highest_prediction_scores_indexes = np.zeros((len(negative_patients), M))\n",
    "    # Pour calculer les scores, il faut au préalable récupérer les âges et sexes normalisés.\n",
    "\n",
    "    patient_age_normed = ( df['age_p'] - df['age_p'].mean() ) / df['age_p'].std()\n",
    "    doctor_age_normed = ( df['age_d'] - df['age_d'].mean() ) / df['age_d'].std()\n",
    "    patient_sex_normed = ( df['sex_p'] - df['sex_p'].mean() ) / df['sex_p'].std()\n",
    "    doctor_sex_normed = ( df['sex_d'] - df['sex_d'].mean() ) / df['sex_d'].std()\n",
    "\n",
    "    for i in negative_patients:\n",
    "        patient_df = negative_connections[negative_connections['i'] == i]\n",
    "        # On considère les docteurs pour lesquels le patient i n'a pas de connexions avec\n",
    "        for j in patient_df['j']:\n",
    "            # on récupère la distance entre le patient i et le docteur j\n",
    "            distance = patient_df[patient_df['j'] == j]['distance'].iloc[0]\n",
    "            T = np.dot(alpha_graph_training[i], psi_graph_training[j]) + beta_age_p_graph_training * patient_age_normed[i] + beta_age_d_graph_training * doctor_age_normed[j] \\\n",
    "            + beta_sex_p_graph_training * patient_sex_normed[i] + beta_sex_d_graph_training * doctor_sex_normed[j] + beta_distance_graph_training * distance\n",
    "            prediction_scores[i][j] = 1 / (1 + np.exp(-T))\n",
    "        # on garde les M indices des docteurs avec la plus grande probabilité de connexion pour le patient i dans highest_prediction_scores_indexes\n",
    "        highest_prediction_scores_indexes[i] = np.argsort(prediction_scores[i])[-M:]\n",
    "        # on effectue le negative sampling\n",
    "        negative_drawing_patient = []\n",
    "        # on détermine les y = 0 à rajouter à notre dataframe\n",
    "        negative_drawing = np.random.binomial(1, p=1/M, size=M)\n",
    "        for l in range(M):\n",
    "            if negative_drawing[l] == 1:\n",
    "                doc = highest_prediction_scores_indexes[i][l]\n",
    "                negative_drawing_patient.append(patient_df[patient_df['j'] == doc].iloc[0].to_list())\n",
    "        negative_drawing_list += negative_drawing_patient\n",
    "    # Une fois les connexions négatives récupérées pour chaque patient, on concatène nos Y = 1 et Y = 0\n",
    "    positive_connections = df.drop(df[df['y'] == 0].index)\n",
    "    positive_connections = positive_connections.reset_index(drop=True)\n",
    "    df_second_hns = pd.DataFrame(negative_drawing_list)\n",
    "    df_second_hns.columns = df.columns\n",
    "    df_second_hns = pd.concat([positive_connections, df_second_hns], axis = 0)\n",
    "    df_second_hns = df_second_hns.reset_index(drop=True)\n",
    "    \n",
    "    # On effectue maintenant l'estimation finale des EFs/Betas encore une fois à l'aide de Keras mais cette fois-ci sur notre second dataframe\n",
    "    final_parameters = get_estimations(df_second_hns, nb_epochs=epochs, initial_weights=weights, target_loss=threshold_loss)\n",
    "\n",
    "    return final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 137us/sample - loss: 1.7824 - accuracy: 0.2037 - val_loss: 0.7558 - val_accuracy: 0.1423\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 37us/sample - loss: 0.5447 - accuracy: 0.8012 - val_loss: 1.6888 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "  64/2244 [..............................] - ETA: 0s - loss: 0.4429 - accuracy: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2244/2244 [==============================] - 0s 40us/sample - loss: 0.5180 - accuracy: 0.8048 - val_loss: 1.6218 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 34us/sample - loss: 0.5161 - accuracy: 0.8048 - val_loss: 1.5799 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 34us/sample - loss: 0.5145 - accuracy: 0.8048 - val_loss: 1.5593 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 33us/sample - loss: 0.5125 - accuracy: 0.8048 - val_loss: 1.5679 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 38us/sample - loss: 0.5097 - accuracy: 0.8048 - val_loss: 1.5622 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 51us/sample - loss: 0.5043 - accuracy: 0.8048 - val_loss: 1.5727 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 40us/sample - loss: 0.4987 - accuracy: 0.8048 - val_loss: 1.5685 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 43us/sample - loss: 0.4920 - accuracy: 0.8048 - val_loss: 1.5587 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.4839 - accuracy: 0.8048 - val_loss: 1.5644 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.4750 - accuracy: 0.8048 - val_loss: 1.4800 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.4648 - accuracy: 0.8048 - val_loss: 1.4925 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 24us/sample - loss: 0.4533 - accuracy: 0.8048 - val_loss: 1.4618 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.4420 - accuracy: 0.8048 - val_loss: 1.4251 - val_accuracy: 0.0000e+00\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 24us/sample - loss: 0.4313 - accuracy: 0.8066 - val_loss: 1.4768 - val_accuracy: 0.0018\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.4172 - accuracy: 0.8075 - val_loss: 1.3909 - val_accuracy: 0.0018\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 24us/sample - loss: 0.4047 - accuracy: 0.8097 - val_loss: 1.4632 - val_accuracy: 0.0018\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.3926 - accuracy: 0.8106 - val_loss: 1.3911 - val_accuracy: 0.0071\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.3792 - accuracy: 0.8119 - val_loss: 1.3187 - val_accuracy: 0.0196\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 29us/sample - loss: 0.3671 - accuracy: 0.8235 - val_loss: 1.4780 - val_accuracy: 0.0125\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.3540 - accuracy: 0.8280 - val_loss: 1.3471 - val_accuracy: 0.0285\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.3415 - accuracy: 0.8329 - val_loss: 1.2908 - val_accuracy: 0.0516\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 32us/sample - loss: 0.3292 - accuracy: 0.8520 - val_loss: 1.4113 - val_accuracy: 0.0356\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 30us/sample - loss: 0.3186 - accuracy: 0.8476 - val_loss: 1.2950 - val_accuracy: 0.0765\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.3065 - accuracy: 0.8578 - val_loss: 1.2870 - val_accuracy: 0.0943\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.2957 - accuracy: 0.8908 - val_loss: 1.3155 - val_accuracy: 0.1014\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 29us/sample - loss: 0.2848 - accuracy: 0.8922 - val_loss: 1.2846 - val_accuracy: 0.1281\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.2753 - accuracy: 0.8904 - val_loss: 1.2543 - val_accuracy: 0.1690\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.2657 - accuracy: 0.9118 - val_loss: 1.2701 - val_accuracy: 0.1762\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.2563 - accuracy: 0.9171 - val_loss: 1.2823 - val_accuracy: 0.1868\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.2475 - accuracy: 0.9354 - val_loss: 1.3614 - val_accuracy: 0.1690\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.2393 - accuracy: 0.9269 - val_loss: 1.2674 - val_accuracy: 0.2260\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 58us/sample - loss: 0.2311 - accuracy: 0.9381 - val_loss: 1.2707 - val_accuracy: 0.2402\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 31us/sample - loss: 0.2235 - accuracy: 0.9461 - val_loss: 1.3430 - val_accuracy: 0.2260\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 46us/sample - loss: 0.2160 - accuracy: 0.9456 - val_loss: 1.2652 - val_accuracy: 0.2705\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 26us/sample - loss: 0.2095 - accuracy: 0.9528 - val_loss: 1.3631 - val_accuracy: 0.2349\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.2029 - accuracy: 0.9559 - val_loss: 1.3936 - val_accuracy: 0.2349\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 22us/sample - loss: 0.1969 - accuracy: 0.9563 - val_loss: 1.3625 - val_accuracy: 0.2580\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 26us/sample - loss: 0.1901 - accuracy: 0.9563 - val_loss: 1.3143 - val_accuracy: 0.2900\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.1842 - accuracy: 0.9586 - val_loss: 1.3235 - val_accuracy: 0.2972\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.1791 - accuracy: 0.9617 - val_loss: 1.3832 - val_accuracy: 0.2847\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 24us/sample - loss: 0.1736 - accuracy: 0.9612 - val_loss: 1.3632 - val_accuracy: 0.3025\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 50us/sample - loss: 0.1682 - accuracy: 0.9630 - val_loss: 1.3437 - val_accuracy: 0.3256\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.1634 - accuracy: 0.9635 - val_loss: 1.3549 - val_accuracy: 0.3256\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.1589 - accuracy: 0.9621 - val_loss: 1.3243 - val_accuracy: 0.3399\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 30us/sample - loss: 0.1543 - accuracy: 0.9648 - val_loss: 1.4049 - val_accuracy: 0.3221\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 38us/sample - loss: 0.1499 - accuracy: 0.9670 - val_loss: 1.4421 - val_accuracy: 0.3185\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 32us/sample - loss: 0.1456 - accuracy: 0.9648 - val_loss: 1.3145 - val_accuracy: 0.3559\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.1414 - accuracy: 0.9679 - val_loss: 1.3959 - val_accuracy: 0.3381\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.1372 - accuracy: 0.9675 - val_loss: 1.3641 - val_accuracy: 0.3541\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.1334 - accuracy: 0.9688 - val_loss: 1.4076 - val_accuracy: 0.3470\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.1303 - accuracy: 0.9710 - val_loss: 1.4536 - val_accuracy: 0.3399\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 29us/sample - loss: 0.1262 - accuracy: 0.9706 - val_loss: 1.4441 - val_accuracy: 0.3452\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.1230 - accuracy: 0.9701 - val_loss: 1.3858 - val_accuracy: 0.3665\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 24us/sample - loss: 0.1192 - accuracy: 0.9715 - val_loss: 1.4884 - val_accuracy: 0.3452\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.1159 - accuracy: 0.9710 - val_loss: 1.4341 - val_accuracy: 0.3594\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 60us/sample - loss: 0.1128 - accuracy: 0.9728 - val_loss: 1.3978 - val_accuracy: 0.3754\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 51us/sample - loss: 0.1101 - accuracy: 0.9764 - val_loss: 1.4725 - val_accuracy: 0.3541\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 32us/sample - loss: 0.1068 - accuracy: 0.9733 - val_loss: 1.3793 - val_accuracy: 0.3861\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.1036 - accuracy: 0.9768 - val_loss: 1.4892 - val_accuracy: 0.3612\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 46us/sample - loss: 0.1007 - accuracy: 0.9773 - val_loss: 1.4622 - val_accuracy: 0.3701\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.0978 - accuracy: 0.9786 - val_loss: 1.4710 - val_accuracy: 0.3719\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.0953 - accuracy: 0.9782 - val_loss: 1.4656 - val_accuracy: 0.3826\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 26us/sample - loss: 0.0929 - accuracy: 0.9786 - val_loss: 1.4228 - val_accuracy: 0.3950\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.0900 - accuracy: 0.9835 - val_loss: 1.5014 - val_accuracy: 0.3790\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.0876 - accuracy: 0.9835 - val_loss: 1.4722 - val_accuracy: 0.3879\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 47us/sample - loss: 0.0852 - accuracy: 0.9880 - val_loss: 1.6089 - val_accuracy: 0.3577\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 56us/sample - loss: 0.0825 - accuracy: 0.9831 - val_loss: 1.4285 - val_accuracy: 0.4039\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 72us/sample - loss: 0.0811 - accuracy: 0.9898 - val_loss: 1.5394 - val_accuracy: 0.3808\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 34us/sample - loss: 0.0781 - accuracy: 0.9880 - val_loss: 1.5333 - val_accuracy: 0.3843\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.0760 - accuracy: 0.9880 - val_loss: 1.5434 - val_accuracy: 0.3843\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.0739 - accuracy: 0.9911 - val_loss: 1.5808 - val_accuracy: 0.3808\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 29us/sample - loss: 0.0719 - accuracy: 0.9893 - val_loss: 1.5106 - val_accuracy: 0.4021\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 26us/sample - loss: 0.0700 - accuracy: 0.9933 - val_loss: 1.6327 - val_accuracy: 0.3737\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 75us/sample - loss: 0.0682 - accuracy: 0.9911 - val_loss: 1.5680 - val_accuracy: 0.3968\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 66us/sample - loss: 0.0661 - accuracy: 0.9929 - val_loss: 1.5869 - val_accuracy: 0.3950\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 42us/sample - loss: 0.0644 - accuracy: 0.9924 - val_loss: 1.5757 - val_accuracy: 0.4004\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 50us/sample - loss: 0.0627 - accuracy: 0.9938 - val_loss: 1.5994 - val_accuracy: 0.4004\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 54us/sample - loss: 0.0609 - accuracy: 0.9929 - val_loss: 1.6243 - val_accuracy: 0.3968\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 63us/sample - loss: 0.0595 - accuracy: 0.9938 - val_loss: 1.5743 - val_accuracy: 0.4146\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 56us/sample - loss: 0.0578 - accuracy: 0.9929 - val_loss: 1.5878 - val_accuracy: 0.4128\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 54us/sample - loss: 0.0562 - accuracy: 0.9942 - val_loss: 1.6736 - val_accuracy: 0.3915\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 52us/sample - loss: 0.0549 - accuracy: 0.9938 - val_loss: 1.6285 - val_accuracy: 0.4110\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 53us/sample - loss: 0.0537 - accuracy: 0.9933 - val_loss: 1.5798 - val_accuracy: 0.4217\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 55us/sample - loss: 0.0521 - accuracy: 0.9955 - val_loss: 1.6625 - val_accuracy: 0.4039\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 66us/sample - loss: 0.0506 - accuracy: 0.9951 - val_loss: 1.6642 - val_accuracy: 0.4057\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 85us/sample - loss: 0.0493 - accuracy: 0.9960 - val_loss: 1.6648 - val_accuracy: 0.4093\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 106us/sample - loss: 0.0481 - accuracy: 0.9947 - val_loss: 1.6952 - val_accuracy: 0.4039\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 76us/sample - loss: 0.0469 - accuracy: 0.9955 - val_loss: 1.6278 - val_accuracy: 0.4181\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.0457 - accuracy: 0.9955 - val_loss: 1.7105 - val_accuracy: 0.4057\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.0446 - accuracy: 0.9960 - val_loss: 1.7225 - val_accuracy: 0.4057\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 63us/sample - loss: 0.0435 - accuracy: 0.9964 - val_loss: 1.7376 - val_accuracy: 0.4039\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 41us/sample - loss: 0.0424 - accuracy: 0.9964 - val_loss: 1.7234 - val_accuracy: 0.4075\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 36us/sample - loss: 0.0414 - accuracy: 0.9964 - val_loss: 1.7487 - val_accuracy: 0.4057\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 36us/sample - loss: 0.0404 - accuracy: 0.9964 - val_loss: 1.7502 - val_accuracy: 0.4057\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 41us/sample - loss: 0.0395 - accuracy: 0.9964 - val_loss: 1.7497 - val_accuracy: 0.4057\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 52us/sample - loss: 0.0386 - accuracy: 0.9964 - val_loss: 1.7353 - val_accuracy: 0.4146\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 35us/sample - loss: 0.0376 - accuracy: 0.9969 - val_loss: 1.7842 - val_accuracy: 0.4057\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 29us/sample - loss: 0.0368 - accuracy: 0.9964 - val_loss: 1.7981 - val_accuracy: 0.4039\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 29us/sample - loss: 0.0359 - accuracy: 0.9964 - val_loss: 1.7418 - val_accuracy: 0.4199\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 29us/sample - loss: 0.0351 - accuracy: 0.9964 - val_loss: 1.8197 - val_accuracy: 0.4057\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 26us/sample - loss: 0.0343 - accuracy: 0.9964 - val_loss: 1.7719 - val_accuracy: 0.4164\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.0335 - accuracy: 0.9964 - val_loss: 1.7955 - val_accuracy: 0.4110\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 26us/sample - loss: 0.0327 - accuracy: 0.9969 - val_loss: 1.8257 - val_accuracy: 0.4110\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.0321 - accuracy: 0.9969 - val_loss: 1.8713 - val_accuracy: 0.4057\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 26us/sample - loss: 0.0314 - accuracy: 0.9964 - val_loss: 1.8384 - val_accuracy: 0.4110\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 35us/sample - loss: 0.0307 - accuracy: 0.9969 - val_loss: 1.8325 - val_accuracy: 0.4128\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 31us/sample - loss: 0.0300 - accuracy: 0.9973 - val_loss: 1.8636 - val_accuracy: 0.4110\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 29us/sample - loss: 0.0294 - accuracy: 0.9964 - val_loss: 1.8246 - val_accuracy: 0.4164\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.0288 - accuracy: 0.9991 - val_loss: 1.8546 - val_accuracy: 0.4128\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 35us/sample - loss: 0.0281 - accuracy: 0.9973 - val_loss: 1.8453 - val_accuracy: 0.4164\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 45us/sample - loss: 0.0275 - accuracy: 0.9982 - val_loss: 1.8957 - val_accuracy: 0.4093\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 44us/sample - loss: 0.0269 - accuracy: 0.9969 - val_loss: 1.8935 - val_accuracy: 0.4128\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 38us/sample - loss: 0.0263 - accuracy: 0.9978 - val_loss: 1.8936 - val_accuracy: 0.4128\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 49us/sample - loss: 0.0258 - accuracy: 0.9978 - val_loss: 1.8817 - val_accuracy: 0.4164\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 70us/sample - loss: 0.0252 - accuracy: 0.9978 - val_loss: 1.9000 - val_accuracy: 0.4146\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 44us/sample - loss: 0.0247 - accuracy: 0.9982 - val_loss: 1.9225 - val_accuracy: 0.4110\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 26us/sample - loss: 0.0242 - accuracy: 0.9973 - val_loss: 1.8905 - val_accuracy: 0.4199\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.0237 - accuracy: 0.9996 - val_loss: 1.9511 - val_accuracy: 0.4093\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 26us/sample - loss: 0.0232 - accuracy: 0.9991 - val_loss: 1.9153 - val_accuracy: 0.4181\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 36us/sample - loss: 0.0228 - accuracy: 0.9991 - val_loss: 1.9513 - val_accuracy: 0.4110\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 32us/sample - loss: 0.0224 - accuracy: 0.9982 - val_loss: 1.8920 - val_accuracy: 0.4270\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 37us/sample - loss: 0.0219 - accuracy: 0.9987 - val_loss: 1.9624 - val_accuracy: 0.4128\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 83us/sample - loss: 0.0214 - accuracy: 0.9996 - val_loss: 2.0039 - val_accuracy: 0.4075\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 35us/sample - loss: 0.0210 - accuracy: 0.9987 - val_loss: 1.9432 - val_accuracy: 0.4217\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 32us/sample - loss: 0.0206 - accuracy: 0.9991 - val_loss: 1.9964 - val_accuracy: 0.4110\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 45us/sample - loss: 0.0203 - accuracy: 0.9996 - val_loss: 2.0422 - val_accuracy: 0.4039\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.0198 - accuracy: 0.9982 - val_loss: 1.9839 - val_accuracy: 0.4181\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.0193 - accuracy: 1.0000 - val_loss: 2.0701 - val_accuracy: 0.4004\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 41us/sample - loss: 0.0190 - accuracy: 0.9991 - val_loss: 2.0092 - val_accuracy: 0.4164\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 31us/sample - loss: 0.0186 - accuracy: 1.0000 - val_loss: 2.0424 - val_accuracy: 0.4075\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 30us/sample - loss: 0.0183 - accuracy: 0.9996 - val_loss: 1.9855 - val_accuracy: 0.4253\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 35us/sample - loss: 0.0179 - accuracy: 1.0000 - val_loss: 2.0658 - val_accuracy: 0.4075\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 47us/sample - loss: 0.0176 - accuracy: 1.0000 - val_loss: 2.0759 - val_accuracy: 0.4075\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 32us/sample - loss: 0.0172 - accuracy: 1.0000 - val_loss: 2.0499 - val_accuracy: 0.4164\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 24us/sample - loss: 0.0168 - accuracy: 1.0000 - val_loss: 2.0708 - val_accuracy: 0.4110\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 22us/sample - loss: 0.0166 - accuracy: 1.0000 - val_loss: 2.0412 - val_accuracy: 0.4199\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 22us/sample - loss: 0.0164 - accuracy: 1.0000 - val_loss: 2.0760 - val_accuracy: 0.4199\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 22us/sample - loss: 0.0160 - accuracy: 1.0000 - val_loss: 2.0619 - val_accuracy: 0.4253\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.0157 - accuracy: 1.0000 - val_loss: 2.0678 - val_accuracy: 0.4235\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 32us/sample - loss: 0.0153 - accuracy: 1.0000 - val_loss: 2.0629 - val_accuracy: 0.4253\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 28us/sample - loss: 0.0150 - accuracy: 1.0000 - val_loss: 2.0749 - val_accuracy: 0.4217\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 27us/sample - loss: 0.0147 - accuracy: 1.0000 - val_loss: 2.0661 - val_accuracy: 0.4253\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.0147 - accuracy: 1.0000 - val_loss: 2.0827 - val_accuracy: 0.4253\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 22us/sample - loss: 0.0141 - accuracy: 1.0000 - val_loss: 2.1039 - val_accuracy: 0.4217\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 22us/sample - loss: 0.0139 - accuracy: 1.0000 - val_loss: 2.1585 - val_accuracy: 0.4075\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 22us/sample - loss: 0.0136 - accuracy: 1.0000 - val_loss: 2.1372 - val_accuracy: 0.4146\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 22us/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 2.1021 - val_accuracy: 0.4270\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.0131 - accuracy: 1.0000 - val_loss: 2.1304 - val_accuracy: 0.4181\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 2.1338 - val_accuracy: 0.4217\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 26us/sample - loss: 0.0126 - accuracy: 1.0000 - val_loss: 2.1294 - val_accuracy: 0.4235\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 32us/sample - loss: 0.0123 - accuracy: 1.0000 - val_loss: 2.1860 - val_accuracy: 0.4075\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 2.1832 - val_accuracy: 0.4128\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 29us/sample - loss: 0.0119 - accuracy: 1.0000 - val_loss: 2.1878 - val_accuracy: 0.4128\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 47us/sample - loss: 0.0116 - accuracy: 1.0000 - val_loss: 2.1794 - val_accuracy: 0.4146\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.0115 - accuracy: 1.0000 - val_loss: 2.1496 - val_accuracy: 0.4235\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 24us/sample - loss: 0.0112 - accuracy: 1.0000 - val_loss: 2.1958 - val_accuracy: 0.4146\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 23us/sample - loss: 0.0110 - accuracy: 1.0000 - val_loss: 2.1937 - val_accuracy: 0.4199\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.0108 - accuracy: 1.0000 - val_loss: 2.2212 - val_accuracy: 0.4128\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 30us/sample - loss: 0.0106 - accuracy: 1.0000 - val_loss: 2.2276 - val_accuracy: 0.4128\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 29us/sample - loss: 0.0104 - accuracy: 1.0000 - val_loss: 2.1712 - val_accuracy: 0.4235\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 25us/sample - loss: 0.0106 - accuracy: 1.0000 - val_loss: 2.2166 - val_accuracy: 0.4235\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2244/2244 [==============================] - 0s 24us/sample - loss: 0.0101 - accuracy: 1.0000 - val_loss: 2.2394 - val_accuracy: 0.4164\n",
      "Train on 2244 samples, validate on 562 samples\n",
      "2112/2244 [===========================>..] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
      "Training stopped as the loss reached below 0.01\n",
      "2244/2244 [==============================] - 0s 47us/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 2.2123 - val_accuracy: 0.4199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_17104\\2838799353.py:62: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  prediction_scores[i][j] = 1 / (1 + np.exp(-T))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 125us/sample - loss: 0.5494 - accuracy: 0.8099 - val_loss: 1.5514 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.4915 - accuracy: 0.8099 - val_loss: 1.3624 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.4833 - accuracy: 0.8099 - val_loss: 1.4076 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 30us/sample - loss: 0.4808 - accuracy: 0.8099 - val_loss: 1.3831 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 30us/sample - loss: 0.4759 - accuracy: 0.8099 - val_loss: 1.5452 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 29us/sample - loss: 0.4741 - accuracy: 0.8099 - val_loss: 1.4510 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 28us/sample - loss: 0.4697 - accuracy: 0.8099 - val_loss: 1.4637 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.4640 - accuracy: 0.8099 - val_loss: 1.3225 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 28us/sample - loss: 0.4568 - accuracy: 0.8099 - val_loss: 1.4352 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 30us/sample - loss: 0.4492 - accuracy: 0.8099 - val_loss: 1.5392 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 28us/sample - loss: 0.4418 - accuracy: 0.8099 - val_loss: 1.4439 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.4312 - accuracy: 0.8099 - val_loss: 1.3806 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.4220 - accuracy: 0.8099 - val_loss: 1.3201 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.4108 - accuracy: 0.8099 - val_loss: 1.4192 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.3993 - accuracy: 0.8112 - val_loss: 1.2803 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.3881 - accuracy: 0.8112 - val_loss: 1.3304 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.3773 - accuracy: 0.8175 - val_loss: 1.2938 - val_accuracy: 0.0000e+00\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 29us/sample - loss: 0.3633 - accuracy: 0.8193 - val_loss: 1.1493 - val_accuracy: 0.0108\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.3530 - accuracy: 0.8372 - val_loss: 1.2659 - val_accuracy: 0.0036\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.3384 - accuracy: 0.8399 - val_loss: 1.2515 - val_accuracy: 0.0143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.3262 - accuracy: 0.8511 - val_loss: 1.2083 - val_accuracy: 0.0233\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.3138 - accuracy: 0.8623 - val_loss: 1.3133 - val_accuracy: 0.0179\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.3019 - accuracy: 0.8691 - val_loss: 1.1870 - val_accuracy: 0.0520\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.2901 - accuracy: 0.8700 - val_loss: 1.0728 - val_accuracy: 0.1272\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.2798 - accuracy: 0.8825 - val_loss: 1.1045 - val_accuracy: 0.1201\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.2662 - accuracy: 0.8901 - val_loss: 1.1917 - val_accuracy: 0.0932\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.2560 - accuracy: 0.8955 - val_loss: 1.2032 - val_accuracy: 0.1057\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.2458 - accuracy: 0.9072 - val_loss: 1.1196 - val_accuracy: 0.1720\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.2352 - accuracy: 0.9013 - val_loss: 0.9972 - val_accuracy: 0.3011\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.2258 - accuracy: 0.9296 - val_loss: 1.1566 - val_accuracy: 0.1828\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.2169 - accuracy: 0.9251 - val_loss: 1.1241 - val_accuracy: 0.2240\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.2076 - accuracy: 0.9404 - val_loss: 1.0727 - val_accuracy: 0.2796\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.1983 - accuracy: 0.9484 - val_loss: 1.1049 - val_accuracy: 0.2724\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.1904 - accuracy: 0.9484 - val_loss: 1.0593 - val_accuracy: 0.3118\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 22us/sample - loss: 0.1825 - accuracy: 0.9561 - val_loss: 1.0392 - val_accuracy: 0.3369\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.1751 - accuracy: 0.9628 - val_loss: 1.0714 - val_accuracy: 0.3190\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.1681 - accuracy: 0.9655 - val_loss: 1.0742 - val_accuracy: 0.3351\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.1616 - accuracy: 0.9695 - val_loss: 1.0018 - val_accuracy: 0.4068\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.1551 - accuracy: 0.9713 - val_loss: 1.0330 - val_accuracy: 0.3925\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.1489 - accuracy: 0.9789 - val_loss: 1.1492 - val_accuracy: 0.3082\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.1442 - accuracy: 0.9762 - val_loss: 1.1439 - val_accuracy: 0.3244\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.1386 - accuracy: 0.9785 - val_loss: 1.1106 - val_accuracy: 0.3495\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.1330 - accuracy: 0.9771 - val_loss: 1.0143 - val_accuracy: 0.4373\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 22us/sample - loss: 0.1275 - accuracy: 0.9834 - val_loss: 0.9946 - val_accuracy: 0.4534\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 29us/sample - loss: 0.1228 - accuracy: 0.9843 - val_loss: 1.1018 - val_accuracy: 0.3853\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.1187 - accuracy: 0.9861 - val_loss: 1.0806 - val_accuracy: 0.4014\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.1142 - accuracy: 0.9874 - val_loss: 1.0311 - val_accuracy: 0.4480\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 59us/sample - loss: 0.1101 - accuracy: 0.9879 - val_loss: 1.0021 - val_accuracy: 0.4892\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.1062 - accuracy: 0.9874 - val_loss: 1.0146 - val_accuracy: 0.4839\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.1032 - accuracy: 0.9870 - val_loss: 1.0388 - val_accuracy: 0.4642\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0990 - accuracy: 0.9901 - val_loss: 1.0710 - val_accuracy: 0.4444\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 44us/sample - loss: 0.0959 - accuracy: 0.9892 - val_loss: 1.0751 - val_accuracy: 0.4444\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.0925 - accuracy: 0.9901 - val_loss: 1.0136 - val_accuracy: 0.5054\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.0895 - accuracy: 0.9901 - val_loss: 1.0065 - val_accuracy: 0.5179\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0869 - accuracy: 0.9901 - val_loss: 1.0117 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0838 - accuracy: 0.9919 - val_loss: 1.1113 - val_accuracy: 0.4427\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0813 - accuracy: 0.9924 - val_loss: 1.0771 - val_accuracy: 0.4767\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0787 - accuracy: 0.9915 - val_loss: 1.0572 - val_accuracy: 0.4982\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.0764 - accuracy: 0.9919 - val_loss: 1.0202 - val_accuracy: 0.5233\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.0741 - accuracy: 0.9915 - val_loss: 1.0706 - val_accuracy: 0.4964\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "  64/2230 [..............................] - ETA: 0s - loss: 0.0549 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0006s vs `on_train_batch_end` time: 0.0007s). Check your callbacks.\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.0718 - accuracy: 0.9924 - val_loss: 1.0602 - val_accuracy: 0.5036\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0696 - accuracy: 0.9937 - val_loss: 1.0518 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0678 - accuracy: 0.9919 - val_loss: 1.1394 - val_accuracy: 0.4606\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 48us/sample - loss: 0.0656 - accuracy: 0.9937 - val_loss: 1.0364 - val_accuracy: 0.5305\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0639 - accuracy: 0.9955 - val_loss: 1.1391 - val_accuracy: 0.4677\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0620 - accuracy: 0.9933 - val_loss: 1.0294 - val_accuracy: 0.5412\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0603 - accuracy: 0.9942 - val_loss: 1.1100 - val_accuracy: 0.4928\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0589 - accuracy: 0.9951 - val_loss: 1.0652 - val_accuracy: 0.5215\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0572 - accuracy: 0.9955 - val_loss: 1.0903 - val_accuracy: 0.5108\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.0556 - accuracy: 0.9951 - val_loss: 1.0840 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.0541 - accuracy: 0.9960 - val_loss: 1.0874 - val_accuracy: 0.5161\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0527 - accuracy: 0.9960 - val_loss: 1.1009 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0513 - accuracy: 0.9969 - val_loss: 1.1302 - val_accuracy: 0.5000\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0500 - accuracy: 0.9969 - val_loss: 1.0671 - val_accuracy: 0.5412\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0487 - accuracy: 0.9969 - val_loss: 1.1017 - val_accuracy: 0.5161\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.0475 - accuracy: 0.9982 - val_loss: 1.1152 - val_accuracy: 0.5090\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 30us/sample - loss: 0.0463 - accuracy: 0.9973 - val_loss: 1.0575 - val_accuracy: 0.5591\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 31us/sample - loss: 0.0451 - accuracy: 0.9973 - val_loss: 1.1296 - val_accuracy: 0.5072\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 47us/sample - loss: 0.0444 - accuracy: 0.9973 - val_loss: 1.1985 - val_accuracy: 0.4821\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 32us/sample - loss: 0.0434 - accuracy: 0.9973 - val_loss: 1.1764 - val_accuracy: 0.4910\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 38us/sample - loss: 0.0420 - accuracy: 0.9969 - val_loss: 1.1250 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 38us/sample - loss: 0.0410 - accuracy: 0.9973 - val_loss: 1.1117 - val_accuracy: 0.5305\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 35us/sample - loss: 0.0399 - accuracy: 0.9973 - val_loss: 1.1157 - val_accuracy: 0.5305\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 32us/sample - loss: 0.0389 - accuracy: 0.9973 - val_loss: 1.1017 - val_accuracy: 0.5412\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 35us/sample - loss: 0.0381 - accuracy: 0.9982 - val_loss: 1.1961 - val_accuracy: 0.4982\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 29us/sample - loss: 0.0371 - accuracy: 0.9973 - val_loss: 1.1211 - val_accuracy: 0.5341\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0363 - accuracy: 0.9973 - val_loss: 1.1299 - val_accuracy: 0.5323\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 42us/sample - loss: 0.0354 - accuracy: 0.9978 - val_loss: 1.1676 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 31us/sample - loss: 0.0348 - accuracy: 0.9973 - val_loss: 1.1418 - val_accuracy: 0.5287\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 28us/sample - loss: 0.0340 - accuracy: 0.9978 - val_loss: 1.1167 - val_accuracy: 0.5484\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 35us/sample - loss: 0.0331 - accuracy: 0.9991 - val_loss: 1.1720 - val_accuracy: 0.5215\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 29us/sample - loss: 0.0324 - accuracy: 0.9987 - val_loss: 1.2083 - val_accuracy: 0.5000\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 28us/sample - loss: 0.0316 - accuracy: 0.9982 - val_loss: 1.1466 - val_accuracy: 0.5341\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 44us/sample - loss: 0.0309 - accuracy: 0.9991 - val_loss: 1.1804 - val_accuracy: 0.5215\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.0303 - accuracy: 0.9973 - val_loss: 1.1063 - val_accuracy: 0.5663\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0296 - accuracy: 0.9991 - val_loss: 1.1909 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0290 - accuracy: 0.9991 - val_loss: 1.1964 - val_accuracy: 0.5125\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 55us/sample - loss: 0.0284 - accuracy: 0.9987 - val_loss: 1.2182 - val_accuracy: 0.5054\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.0277 - accuracy: 0.9991 - val_loss: 1.1452 - val_accuracy: 0.5448\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0272 - accuracy: 0.9991 - val_loss: 1.1465 - val_accuracy: 0.5448\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0266 - accuracy: 0.9996 - val_loss: 1.1542 - val_accuracy: 0.5412\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0260 - accuracy: 0.9996 - val_loss: 1.1979 - val_accuracy: 0.5251\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0255 - accuracy: 0.9991 - val_loss: 1.1624 - val_accuracy: 0.5412\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0249 - accuracy: 0.9996 - val_loss: 1.2211 - val_accuracy: 0.5161\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0243 - accuracy: 0.9996 - val_loss: 1.1746 - val_accuracy: 0.5376\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0238 - accuracy: 0.9996 - val_loss: 1.1860 - val_accuracy: 0.5358\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0233 - accuracy: 0.9996 - val_loss: 1.2182 - val_accuracy: 0.5233\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0228 - accuracy: 0.9996 - val_loss: 1.2030 - val_accuracy: 0.5251\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.0224 - accuracy: 0.9996 - val_loss: 1.2133 - val_accuracy: 0.5251\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0222 - accuracy: 1.0000 - val_loss: 1.1659 - val_accuracy: 0.5538\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0215 - accuracy: 1.0000 - val_loss: 1.2057 - val_accuracy: 0.5287\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 60us/sample - loss: 0.0210 - accuracy: 1.0000 - val_loss: 1.2338 - val_accuracy: 0.5197\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 42us/sample - loss: 0.0206 - accuracy: 0.9996 - val_loss: 1.2184 - val_accuracy: 0.5269\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 31us/sample - loss: 0.0201 - accuracy: 1.0000 - val_loss: 1.2532 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 33us/sample - loss: 0.0197 - accuracy: 1.0000 - val_loss: 1.2016 - val_accuracy: 0.5430\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 33us/sample - loss: 0.0194 - accuracy: 1.0000 - val_loss: 1.2071 - val_accuracy: 0.5358\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0190 - accuracy: 0.9996 - val_loss: 1.2226 - val_accuracy: 0.5323\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0185 - accuracy: 0.9996 - val_loss: 1.2373 - val_accuracy: 0.5269\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.0181 - accuracy: 1.0000 - val_loss: 1.2700 - val_accuracy: 0.5108\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0178 - accuracy: 1.0000 - val_loss: 1.2313 - val_accuracy: 0.5323\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0174 - accuracy: 1.0000 - val_loss: 1.2470 - val_accuracy: 0.5269\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0171 - accuracy: 1.0000 - val_loss: 1.2462 - val_accuracy: 0.5269\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0167 - accuracy: 1.0000 - val_loss: 1.2820 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0164 - accuracy: 1.0000 - val_loss: 1.2821 - val_accuracy: 0.5161\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0160 - accuracy: 1.0000 - val_loss: 1.2773 - val_accuracy: 0.5179\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 32us/sample - loss: 0.0157 - accuracy: 1.0000 - val_loss: 1.2673 - val_accuracy: 0.5233\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 30us/sample - loss: 0.0154 - accuracy: 1.0000 - val_loss: 1.2944 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 22us/sample - loss: 0.0150 - accuracy: 1.0000 - val_loss: 1.2828 - val_accuracy: 0.5197\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0147 - accuracy: 1.0000 - val_loss: 1.2858 - val_accuracy: 0.5179\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 28us/sample - loss: 0.0146 - accuracy: 1.0000 - val_loss: 1.2964 - val_accuracy: 0.5161\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.0142 - accuracy: 1.0000 - val_loss: 1.2210 - val_accuracy: 0.5556\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 31us/sample - loss: 0.0139 - accuracy: 1.0000 - val_loss: 1.3269 - val_accuracy: 0.5072\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 26us/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 1.3064 - val_accuracy: 0.5161\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.0133 - accuracy: 1.0000 - val_loss: 1.3154 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.0130 - accuracy: 1.0000 - val_loss: 1.2938 - val_accuracy: 0.5179\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 30us/sample - loss: 0.0127 - accuracy: 1.0000 - val_loss: 1.3143 - val_accuracy: 0.5161\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 62us/sample - loss: 0.0125 - accuracy: 1.0000 - val_loss: 1.2984 - val_accuracy: 0.5179\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 25us/sample - loss: 0.0122 - accuracy: 1.0000 - val_loss: 1.3351 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 1.2849 - val_accuracy: 0.5376\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.0118 - accuracy: 1.0000 - val_loss: 1.2917 - val_accuracy: 0.5341\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 28us/sample - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.3390 - val_accuracy: 0.5125\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 28us/sample - loss: 0.0113 - accuracy: 1.0000 - val_loss: 1.3342 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.0110 - accuracy: 1.0000 - val_loss: 1.3321 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0108 - accuracy: 1.0000 - val_loss: 1.3770 - val_accuracy: 0.5054\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 24us/sample - loss: 0.0105 - accuracy: 1.0000 - val_loss: 1.3368 - val_accuracy: 0.5143\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 40us/sample - loss: 0.0103 - accuracy: 1.0000 - val_loss: 1.3274 - val_accuracy: 0.5215\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "2230/2230 [==============================] - 0s 27us/sample - loss: 0.0101 - accuracy: 1.0000 - val_loss: 1.3499 - val_accuracy: 0.5125\n",
      "Train on 2230 samples, validate on 558 samples\n",
      "  64/2230 [..............................] - ETA: 0s - loss: 0.0183 - accuracy: 1.0000\n",
      "Training stopped as the loss reached below 0.01\n",
      "2230/2230 [==============================] - 0s 23us/sample - loss: 0.0099 - accuracy: 1.0000 - val_loss: 1.3567 - val_accuracy: 0.5125\n"
     ]
    }
   ],
   "source": [
    "estimates = HNS_and_estimates(df2, M=2)\n",
    "ef_patient = estimates[1][0]\n",
    "ef_doctor = estimates[1][1]\n",
    "beta_age_p = estimates[1][2]\n",
    "beta_age_d = estimates[1][3]\n",
    "beta_sex_p = estimates[1][4]\n",
    "beta_sex_d = estimates[1][5]\n",
    "beta_distance = estimates[1][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cette partie de code est optionnel (la descente de gradient est faite à la main et va beaucoup moins vite que le code ci-dessus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de gradient alternée à la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_ui(df):\n",
    "    # à partir du dataset y = 0 et y = 1, obtenir la matrice de connexion\n",
    "    # df2 = df.copy()\n",
    "    patients = df['i'].unique()\n",
    "    n_patients = len(patients)\n",
    "    n_doctors = len(df2['j'].unique())\n",
    "    R = np.zeros((n_patients, n_doctors))\n",
    "    for i in patients:\n",
    "        for j in df[df['i'] == i]['j']:\n",
    "            connection = df[df['i'] == i][df[df['i'] == i]['j'] == j]['y']\n",
    "            # if len(connection) > 0:\n",
    "            if connection[connection.index[0]] == 1:\n",
    "                R[i][j] = 1\n",
    "    return R\n",
    "\n",
    "def alpha(R):\n",
    "    return np.unique(R.flatten(), return_counts = True)[1][0] / np.unique(R.flatten(), return_counts = True)[1][1] # Nombre de 0 sur nombre de 1\n",
    "\n",
    "def descente_gradient(df, max_iter, eps = 0.01, lr_patients=0.001, lr_doctors=0.001, reg_term=0.2):\n",
    "    to_be_normalized_columns = ['age_p', 'age_d', 'sex_p', 'sex_d', 'distance']\n",
    "    df[to_be_normalized_columns] = ( df[to_be_normalized_columns] - df[to_be_normalized_columns].mean() ) / df[to_be_normalized_columns].std()\n",
    "    patients = df['i'].unique()\n",
    "    doctors = df['j'].unique()\n",
    "    n_patients = len(patients)\n",
    "    n_doctors = len(doctors)\n",
    "    nb_latent_factors = len([s for s in df.columns if \"ef_p\" in s])\n",
    "    R = r_ui(df)\n",
    "    alpha_term = alpha(R)\n",
    "    # We initialise randomly the fixed effects, betas and the bias\n",
    "    X = np.random.uniform(-1, 1, size = (n_patients, nb_latent_factors))\n",
    "    Y = np.random.uniform(-1, 1, size = (n_doctors, nb_latent_factors))\n",
    "    beta_age_doctor = np.random.uniform(-0.5, 0.5)\n",
    "    beta_age_patient = np.random.uniform(-0.5, 0.5)\n",
    "    beta_sex_doctor = np.random.uniform(-0.5, 0.5)\n",
    "    beta_sex_patient = np.random.uniform(-0.5, 0.5)\n",
    "    beta_distance = np.random.uniform(-0.5, 0.5)\n",
    "    grad_X = np.zeros((n_patients, nb_latent_factors)) # Matrix containin fixed effects gradients of patients\n",
    "    grad_Y = np.zeros((n_doctors, nb_latent_factors)) # Matrix containing fixed effects gradients of doctors\n",
    "    grad_beta_age_doctor = 0\n",
    "    grad_beta_sex_doctor = 0\n",
    "    grad_beta_age_patient = 0\n",
    "    grad_beta_sex_patient = 0\n",
    "    grad_beta_distance = 0\n",
    "    sum_squared_sgd_ef_patients = [0 for _ in range(n_patients)]\n",
    "    sum_squared_sgd_ef_doctors = [0 for _ in range(n_doctors)]\n",
    "    sum_squared_sgd_beta_age_doctor = 0\n",
    "    sum_squared_sgd_beta_sex_doctor = 0\n",
    "    sum_squared_sgd_beta_age_patient = 0\n",
    "    sum_squared_sgd_beta_sex_patient = 0\n",
    "    sum_squared_sgd_beta_distance = 0\n",
    "\n",
    "    k = 0\n",
    "    first_loop = True # Allows the while loop to run the first time\n",
    "    while ( np.linalg.norm(grad_X) > eps or np.linalg.norm(grad_Y) > eps or first_loop == True ) and \\\n",
    "    k < max_iter : # Alternate gradient descent\n",
    "        first_loop = False\n",
    "        t_0 = time.time()\n",
    "        k += 1\n",
    "        grad_beta_age_doctor = 0\n",
    "        grad_beta_sex_doctor = 0\n",
    "        for j in doctors: # We fix fixed effects / bias of the patients and update the one of the doctors\n",
    "            grad_Y[j, :] = 0 # We reinitialize at each loop the gradients\n",
    "            for i in df[df['j'] == j]['i']:\n",
    "\n",
    "                sex_patient = df[df['i'] == i]['sex_p'].iloc[0]\n",
    "                sex_doctor = df[df['j'] == j]['sex_d'].iloc[0]\n",
    "                age_patient = df[df['i'] == i]['age_p'].iloc[0]\n",
    "                age_doctor = df[df['j'] == j]['age_d'].iloc[0]\n",
    "                distance = df[df['i'] == i][df[df['i'] == i]['j'] == j]['distance'].iloc[0]\n",
    "                # Following terms are defined to optimize calculation time\n",
    "                exp_term = (1 + alpha_term*R[i][j]) / (1 + np.exp(-(np.dot(X[i, :], Y[j, :]) + \\\n",
    "                age_patient * beta_age_patient + sex_patient * beta_sex_patient + age_doctor * beta_age_doctor + sex_doctor * beta_sex_doctor + distance * beta_distance )))\n",
    "                alpha_r = alpha_term*R[i][j]\n",
    "                \n",
    "                grad_Y[j, :] += -alpha_r*X[i, :] + X[i, :] * exp_term + reg_term*Y[j, :]\n",
    "                \n",
    "                grad_beta_age_doctor += age_doctor*(-alpha_r + exp_term)\n",
    "\n",
    "                grad_beta_sex_doctor += sex_doctor*(-alpha_r + exp_term)\n",
    "\n",
    "\n",
    "            # Sum of terms in the denominator for Adaboost\n",
    "            sum_squared_sgd_ef_doctors[j] += (np.dot(grad_Y[j, :], grad_Y[j, :])) \n",
    "            Y[j, :] -= lr_doctors*grad_Y[j, :]/np.sqrt(sum_squared_sgd_ef_doctors[j])\n",
    "\n",
    "        sum_squared_sgd_beta_age_doctor += grad_beta_age_doctor**2\n",
    "        sum_squared_sgd_beta_sex_doctor += grad_beta_sex_doctor**2\n",
    "        beta_age_doctor -= lr_doctors*grad_beta_age_doctor/np.sqrt(sum_squared_sgd_beta_age_doctor)\n",
    "        beta_sex_doctor -= lr_doctors*grad_beta_sex_doctor/np.sqrt(sum_squared_sgd_beta_sex_doctor)\n",
    "\n",
    "\n",
    "        grad_beta_age_patient = 0 # We reinitialize at each loop the gradients\n",
    "        grad_beta_sex_patient = 0\n",
    "        for i in patients: # We now update fixed effects / bias of the patients using our updated fixed effects / bias of doctors\n",
    "            grad_X[i, :] = 0 # We reinitialize at each loop the gradients\n",
    "            for j in df[df['i'] == i]['j']:\n",
    "\n",
    "                sex_patient = df[df['i'] == i]['sex_p'].iloc[0]\n",
    "                sex_doctor = df[df['j'] == j]['sex_d'].iloc[0]\n",
    "                age_patient = df[df['i'] == i]['age_p'].iloc[0]\n",
    "                age_doctor = df[df['j'] == j]['age_d'].iloc[0]\n",
    "                distance = df[df['i'] == i][df[df['i'] == i]['j'] == j]['distance'].iloc[0]\n",
    "                # Following terms are defined to optimize calculation time\n",
    "                exp_term = (1 + alpha_term*R[i][j]) / (1 + np.exp(-(np.dot(X[i, :], Y[j, :]) + \\\n",
    "                age_patient * beta_age_patient + sex_patient * beta_sex_patient + age_doctor * beta_age_doctor + sex_doctor * beta_sex_doctor + distance * beta_distance )))\n",
    "                alpha_r = alpha_term*R[i][j]\n",
    "                \n",
    "                grad_X[i, :] += -alpha_r * Y[j, :] + Y[j, :] * exp_term + reg_term*X[i, :]\n",
    "\n",
    "                grad_beta_age_patient += age_patient * (-alpha_r + exp_term)\n",
    "\n",
    "                grad_beta_sex_patient += sex_patient * (-alpha_r + exp_term)\n",
    "                \n",
    "            sum_squared_sgd_ef_patients[i] += (np.dot(grad_X[i, :], grad_X[i, :])) # Denominator term for Adaboost\n",
    "            X[i, :] -= lr_patients*grad_X[i, :]/np.sqrt(sum_squared_sgd_ef_patients[i])\n",
    "\n",
    "        sum_squared_sgd_beta_age_patient += grad_beta_age_patient**2\n",
    "        sum_squared_sgd_beta_sex_patient += grad_beta_sex_patient**2\n",
    "        beta_age_patient -= lr_patients*grad_beta_age_patient/np.sqrt(sum_squared_sgd_beta_age_patient)\n",
    "        beta_sex_patient -= lr_patients*grad_beta_age_patient/np.sqrt(sum_squared_sgd_beta_sex_patient)\n",
    "\n",
    "        grad_beta_distance = 0\n",
    "        for i in patients: # We finally update the beta_distance as it is dependent on patients and doctors parameters.\n",
    "            for j in df[df['i'] == i]['j']:\n",
    "\n",
    "                sex_patient = df[df['i'] == i]['sex_p'].iloc[0]\n",
    "                sex_doctor = df[df['j'] == j]['sex_d'].iloc[0]\n",
    "                age_patient = df[df['i'] == i]['age_p'].iloc[0]\n",
    "                age_doctor = df[df['j'] == j]['age_d'].iloc[0]\n",
    "                distance = df[df['i'] == i][df[df['i'] == i]['j'] == j]['distance'].iloc[0]\n",
    "                exp_term = (1 + alpha_term*R[i][j]) / (1 + np.exp(-(np.dot(X[i, :], Y[j, :]) + \\\n",
    "                age_patient * beta_age_patient + sex_patient * beta_sex_patient + age_doctor * beta_age_doctor + sex_doctor * beta_sex_doctor + distance * beta_distance )))\n",
    "                alpha_r = alpha_term*R[i][j]\n",
    "                \n",
    "                grad_beta_distance += distance*(-alpha_r + exp_term)\n",
    "            \n",
    "        sum_squared_sgd_beta_distance += grad_beta_distance**2\n",
    "        beta_distance -= ((lr_patients + lr_doctors)/2)*grad_beta_distance/np.sqrt(sum_squared_sgd_beta_distance)\n",
    "        t_1 = time.time()\n",
    "\n",
    "        print(k, \"-th iteration took\", t_1 - t_0, \"to execute.\")\n",
    "    beta = [beta_age_doctor, beta_sex_doctor, beta_age_patient, beta_sex_patient, beta_distance]\n",
    "    grad_beta = [grad_beta_age_doctor, grad_beta_sex_doctor, grad_beta_age_patient, grad_beta_sex_patient, grad_beta_distance]\n",
    "    return ([X, Y, beta], [grad_X, grad_Y, grad_beta])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = graph_formation(n_patients=100,\n",
    "                    n_doctors=30,\n",
    "                    max_number_connections=4,\n",
    "                    z=0.5,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.01,\n",
    "                    beta_age_d_graph=0.01,\n",
    "                    beta_sex_p_graph=0.1,\n",
    "                    beta_sex_d_graph=0.1,\n",
    "                    beta_distance_graph=-10,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = 5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = df.copy()\n",
    "positive_connections = dataframe[dataframe['y'] == 1]\n",
    "negative_connections = dataframe[dataframe['y'] == 0]\n",
    "len(negative_connections['i'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.69"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre de non connexions par patient en moyenne\n",
    "negative_connections['i'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M doit être inférieur ou égal à cette valeur\n",
    "# C'est le plus petit nombre de docteurs auquel un patient i n'est pas connecté\n",
    "negative_connections['i'].value_counts().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i\n",
       "50    4\n",
       "22    4\n",
       "59    3\n",
       "60    3\n",
       "86    2\n",
       "     ..\n",
       "32    1\n",
       "31    1\n",
       "30    1\n",
       "29    1\n",
       "99    1\n",
       "Name: count, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On a bien plus de connexions négatives que de connexions positives\n",
    "positive_connections['i'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.31"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre de connexions par patient en moyenne\n",
    "positive_connections['i'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = graph_formation_hns(n_patients=1000,\n",
    "                    n_doctors=300,\n",
    "                    max_number_connections=5,\n",
    "                    z=0.5,\n",
    "                    M=30,\n",
    "                    type_distance=\"default\",\n",
    "                    beta_age_p_graph=0.01,\n",
    "                    beta_age_d_graph=0.01,\n",
    "                    beta_sex_p_graph=0.1,\n",
    "                    beta_sex_d_graph=0.1,\n",
    "                    beta_distance_graph=-10,\n",
    "                    alpha_law_graph=(-1, 1),\n",
    "                    psi_law_graph=(-1, 1),\n",
    "                    nb_latent_factors = 5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>y</th>\n",
       "      <th>age_p</th>\n",
       "      <th>age_d</th>\n",
       "      <th>sex_p</th>\n",
       "      <th>sex_d</th>\n",
       "      <th>distance</th>\n",
       "      <th>ef_p_0</th>\n",
       "      <th>ef_p_1</th>\n",
       "      <th>ef_p_2</th>\n",
       "      <th>ef_p_3</th>\n",
       "      <th>ef_p_4</th>\n",
       "      <th>ef_d_0</th>\n",
       "      <th>ef_d_1</th>\n",
       "      <th>ef_d_2</th>\n",
       "      <th>ef_d_3</th>\n",
       "      <th>ef_d_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>89</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.367899</td>\n",
       "      <td>-0.682953</td>\n",
       "      <td>-0.815052</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>-0.576334</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>-0.168551</td>\n",
       "      <td>0.065324</td>\n",
       "      <td>0.815440</td>\n",
       "      <td>0.461285</td>\n",
       "      <td>0.983074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>44</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.291169</td>\n",
       "      <td>-0.682953</td>\n",
       "      <td>-0.815052</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>-0.576334</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>0.040532</td>\n",
       "      <td>-0.352408</td>\n",
       "      <td>0.731672</td>\n",
       "      <td>-0.114341</td>\n",
       "      <td>-0.670008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>45</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.093386</td>\n",
       "      <td>-0.682953</td>\n",
       "      <td>-0.815052</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>-0.576334</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>-0.094269</td>\n",
       "      <td>-0.821354</td>\n",
       "      <td>-0.235760</td>\n",
       "      <td>0.282684</td>\n",
       "      <td>-0.743585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>47</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386003</td>\n",
       "      <td>-0.682953</td>\n",
       "      <td>-0.815052</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>-0.576334</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>-0.971572</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>-0.689428</td>\n",
       "      <td>-0.252345</td>\n",
       "      <td>0.567239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>88</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.343987</td>\n",
       "      <td>-0.682953</td>\n",
       "      <td>-0.815052</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>-0.576334</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>-0.391391</td>\n",
       "      <td>-0.904439</td>\n",
       "      <td>0.291136</td>\n",
       "      <td>-0.168596</td>\n",
       "      <td>0.922401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5898</th>\n",
       "      <td>995</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.209940</td>\n",
       "      <td>-0.551933</td>\n",
       "      <td>0.228154</td>\n",
       "      <td>0.829068</td>\n",
       "      <td>-0.545308</td>\n",
       "      <td>-0.901676</td>\n",
       "      <td>0.082150</td>\n",
       "      <td>-0.882055</td>\n",
       "      <td>0.704478</td>\n",
       "      <td>-0.410692</td>\n",
       "      <td>-0.355338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>996</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>93</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.253955</td>\n",
       "      <td>-0.489046</td>\n",
       "      <td>0.817064</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>-0.820247</td>\n",
       "      <td>0.267251</td>\n",
       "      <td>-0.488486</td>\n",
       "      <td>0.236925</td>\n",
       "      <td>-0.282689</td>\n",
       "      <td>0.105945</td>\n",
       "      <td>0.081801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>997</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.015671</td>\n",
       "      <td>0.428782</td>\n",
       "      <td>0.346513</td>\n",
       "      <td>0.196287</td>\n",
       "      <td>-0.351358</td>\n",
       "      <td>0.671217</td>\n",
       "      <td>-0.040852</td>\n",
       "      <td>0.196027</td>\n",
       "      <td>-0.999387</td>\n",
       "      <td>-0.907583</td>\n",
       "      <td>-0.385915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5901</th>\n",
       "      <td>998</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100773</td>\n",
       "      <td>-0.043227</td>\n",
       "      <td>-0.840932</td>\n",
       "      <td>0.074328</td>\n",
       "      <td>0.421382</td>\n",
       "      <td>-0.467160</td>\n",
       "      <td>-0.709199</td>\n",
       "      <td>0.663311</td>\n",
       "      <td>0.445632</td>\n",
       "      <td>-0.092643</td>\n",
       "      <td>-0.999014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>999</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.131638</td>\n",
       "      <td>0.875532</td>\n",
       "      <td>0.077772</td>\n",
       "      <td>-0.580259</td>\n",
       "      <td>0.935813</td>\n",
       "      <td>-0.591349</td>\n",
       "      <td>-0.040852</td>\n",
       "      <td>0.196027</td>\n",
       "      <td>-0.999387</td>\n",
       "      <td>-0.907583</td>\n",
       "      <td>-0.385915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5903 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        i    j  y  age_p  age_d  sex_p  sex_d  distance    ef_p_0    ef_p_1  \\\n",
       "0       0   30  1     86     89     -1     -1  0.367899 -0.682953 -0.815052   \n",
       "1       0  145  1     86     44     -1      1  0.291169 -0.682953 -0.815052   \n",
       "2       0  156  1     86     45     -1      1  0.093386 -0.682953 -0.815052   \n",
       "3       0  262  1     86     47     -1      1  0.386003 -0.682953 -0.815052   \n",
       "4       0  289  1     86     88     -1      1  0.343987 -0.682953 -0.815052   \n",
       "...   ...  ... ..    ...    ...    ...    ...       ...       ...       ...   \n",
       "5898  995  275  0     80     67      1      1  0.209940 -0.551933  0.228154   \n",
       "5899  996  132  0     83     93     -1      1  0.253955 -0.489046  0.817064   \n",
       "5900  997  173  0      2     28     -1     -1  0.015671  0.428782  0.346513   \n",
       "5901  998   97  0      3     56      1      1  0.100773 -0.043227 -0.840932   \n",
       "5902  999  173  0     29     28      1     -1  0.131638  0.875532  0.077772   \n",
       "\n",
       "        ef_p_2    ef_p_3    ef_p_4    ef_d_0    ef_d_1    ef_d_2    ef_d_3  \\\n",
       "0     0.010601 -0.576334  0.109618 -0.168551  0.065324  0.815440  0.461285   \n",
       "1     0.010601 -0.576334  0.109618  0.040532 -0.352408  0.731672 -0.114341   \n",
       "2     0.010601 -0.576334  0.109618 -0.094269 -0.821354 -0.235760  0.282684   \n",
       "3     0.010601 -0.576334  0.109618 -0.971572  0.410381 -0.689428 -0.252345   \n",
       "4     0.010601 -0.576334  0.109618 -0.391391 -0.904439  0.291136 -0.168596   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5898  0.829068 -0.545308 -0.901676  0.082150 -0.882055  0.704478 -0.410692   \n",
       "5899  0.061119 -0.820247  0.267251 -0.488486  0.236925 -0.282689  0.105945   \n",
       "5900  0.196287 -0.351358  0.671217 -0.040852  0.196027 -0.999387 -0.907583   \n",
       "5901  0.074328  0.421382 -0.467160 -0.709199  0.663311  0.445632 -0.092643   \n",
       "5902 -0.580259  0.935813 -0.591349 -0.040852  0.196027 -0.999387 -0.907583   \n",
       "\n",
       "        ef_d_4  \n",
       "0     0.983074  \n",
       "1    -0.670008  \n",
       "2    -0.743585  \n",
       "3     0.567239  \n",
       "4     0.922401  \n",
       "...        ...  \n",
       "5898 -0.355338  \n",
       "5899  0.081801  \n",
       "5900 -0.385915  \n",
       "5901 -0.999014  \n",
       "5902 -0.385915  \n",
       "\n",
       "[5903 rows x 18 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "1    4946\n",
       "0     957\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparaison Y = 0 / Y = 1, pas assez d'échantillonnage négatif ?\n",
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -th iteration took 24.55609917640686 to execute.\n",
      "2 -th iteration took 26.22185707092285 to execute.\n",
      "3 -th iteration took 25.664959192276 to execute.\n",
      "4 -th iteration took 23.428571701049805 to execute.\n",
      "5 -th iteration took 24.576223611831665 to execute.\n",
      "6 -th iteration took 24.129536867141724 to execute.\n",
      "7 -th iteration took 24.11001944541931 to execute.\n",
      "8 -th iteration took 24.294201135635376 to execute.\n",
      "9 -th iteration took 24.16663360595703 to execute.\n",
      "10 -th iteration took 24.205181121826172 to execute.\n",
      "11 -th iteration took 24.013393878936768 to execute.\n",
      "12 -th iteration took 24.050031900405884 to execute.\n",
      "13 -th iteration took 24.973216772079468 to execute.\n",
      "14 -th iteration took 24.470908880233765 to execute.\n",
      "15 -th iteration took 24.423006772994995 to execute.\n",
      "16 -th iteration took 23.795681953430176 to execute.\n",
      "17 -th iteration took 24.83116054534912 to execute.\n",
      "18 -th iteration took 25.092320680618286 to execute.\n",
      "19 -th iteration took 25.979527711868286 to execute.\n",
      "20 -th iteration took 26.26982069015503 to execute.\n",
      "21 -th iteration took 26.041643142700195 to execute.\n",
      "22 -th iteration took 24.13598608970642 to execute.\n",
      "23 -th iteration took 24.60260772705078 to execute.\n",
      "24 -th iteration took 24.932884693145752 to execute.\n",
      "25 -th iteration took 25.130613088607788 to execute.\n",
      "26 -th iteration took 25.325184106826782 to execute.\n",
      "27 -th iteration took 25.452670335769653 to execute.\n",
      "28 -th iteration took 24.259822607040405 to execute.\n",
      "29 -th iteration took 23.60060691833496 to execute.\n",
      "30 -th iteration took 23.818846225738525 to execute.\n",
      "31 -th iteration took 24.34464406967163 to execute.\n",
      "32 -th iteration took 24.301928758621216 to execute.\n",
      "33 -th iteration took 23.14770770072937 to execute.\n",
      "34 -th iteration took 23.14901304244995 to execute.\n",
      "35 -th iteration took 25.214627504348755 to execute.\n",
      "36 -th iteration took 25.165579080581665 to execute.\n",
      "37 -th iteration took 24.082488298416138 to execute.\n",
      "38 -th iteration took 25.33447003364563 to execute.\n",
      "39 -th iteration took 24.486979007720947 to execute.\n",
      "40 -th iteration took 24.66359233856201 to execute.\n",
      "41 -th iteration took 24.701664686203003 to execute.\n",
      "42 -th iteration took 24.84695529937744 to execute.\n",
      "43 -th iteration took 23.986178159713745 to execute.\n",
      "44 -th iteration took 25.846885204315186 to execute.\n",
      "45 -th iteration took 24.149953842163086 to execute.\n",
      "46 -th iteration took 25.639819622039795 to execute.\n",
      "47 -th iteration took 26.208609342575073 to execute.\n",
      "48 -th iteration took 26.178250551223755 to execute.\n",
      "49 -th iteration took 24.919225215911865 to execute.\n",
      "50 -th iteration took 25.217904090881348 to execute.\n",
      "51 -th iteration took 23.754377841949463 to execute.\n",
      "52 -th iteration took 25.18455743789673 to execute.\n",
      "53 -th iteration took 24.903316736221313 to execute.\n",
      "54 -th iteration took 24.601991891860962 to execute.\n",
      "55 -th iteration took 24.32833981513977 to execute.\n",
      "56 -th iteration took 25.18531584739685 to execute.\n",
      "57 -th iteration took 24.026367902755737 to execute.\n",
      "58 -th iteration took 24.521310091018677 to execute.\n",
      "59 -th iteration took 24.93058705329895 to execute.\n",
      "60 -th iteration took 24.20749020576477 to execute.\n",
      "61 -th iteration took 23.49236512184143 to execute.\n",
      "62 -th iteration took 25.147790670394897 to execute.\n",
      "63 -th iteration took 24.04516315460205 to execute.\n",
      "64 -th iteration took 25.09157657623291 to execute.\n",
      "65 -th iteration took 24.619941234588623 to execute.\n",
      "66 -th iteration took 25.490046739578247 to execute.\n",
      "67 -th iteration took 24.26006817817688 to execute.\n",
      "68 -th iteration took 25.216858625411987 to execute.\n",
      "69 -th iteration took 24.52005171775818 to execute.\n",
      "70 -th iteration took 25.4677996635437 to execute.\n",
      "71 -th iteration took 24.77733302116394 to execute.\n",
      "72 -th iteration took 24.4067223072052 to execute.\n",
      "73 -th iteration took 24.591670274734497 to execute.\n",
      "74 -th iteration took 25.242889165878296 to execute.\n",
      "75 -th iteration took 25.413061380386353 to execute.\n",
      "76 -th iteration took 23.750635385513306 to execute.\n",
      "77 -th iteration took 23.94399619102478 to execute.\n",
      "78 -th iteration took 24.760543823242188 to execute.\n",
      "79 -th iteration took 23.710671424865723 to execute.\n",
      "80 -th iteration took 24.778351068496704 to execute.\n",
      "81 -th iteration took 24.303751468658447 to execute.\n",
      "82 -th iteration took 24.020217657089233 to execute.\n",
      "83 -th iteration took 24.263299703598022 to execute.\n",
      "84 -th iteration took 22.775520086288452 to execute.\n",
      "85 -th iteration took 24.086087465286255 to execute.\n",
      "86 -th iteration took 24.184792041778564 to execute.\n",
      "87 -th iteration took 24.29128336906433 to execute.\n",
      "88 -th iteration took 24.383270740509033 to execute.\n",
      "89 -th iteration took 24.82006549835205 to execute.\n",
      "90 -th iteration took 24.32274055480957 to execute.\n",
      "91 -th iteration took 25.0598201751709 to execute.\n",
      "92 -th iteration took 23.75376319885254 to execute.\n",
      "93 -th iteration took 25.275885820388794 to execute.\n",
      "94 -th iteration took 23.66220736503601 to execute.\n",
      "95 -th iteration took 24.62480854988098 to execute.\n",
      "96 -th iteration took 23.95264434814453 to execute.\n",
      "97 -th iteration took 24.287663459777832 to execute.\n",
      "98 -th iteration took 25.012372255325317 to execute.\n",
      "99 -th iteration took 23.1621732711792 to execute.\n",
      "100 -th iteration took 25.13355302810669 to execute.\n"
     ]
    }
   ],
   "source": [
    "estimation = descente_gradient(df = df,\n",
    "                               eps = 0.1,\n",
    "                               lr_patients=0.01,\n",
    "                               lr_doctors=0.01,\n",
    "                               max_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.86510073e-01, -2.93828795e+00, -4.01088596e+00,\n",
       "         3.32367652e+00, -2.42663156e+00],\n",
       "       [-6.86205939e+00,  5.33422064e+00,  2.26828245e+00,\n",
       "        -4.09973338e+00, -6.24010126e+00],\n",
       "       [ 2.10215838e+00, -8.88230774e+00, -2.36393694e+00,\n",
       "        -4.92893894e-01,  3.59808490e+00],\n",
       "       [-1.41164891e+01,  1.00682957e+01,  5.39026517e+00,\n",
       "         9.08717800e-03, -1.21585999e+01],\n",
       "       [-5.45473010e+00, -8.26662145e+00,  2.52056527e+00,\n",
       "         5.63085716e+00, -3.68921928e+00],\n",
       "       [ 1.13120530e+01, -8.05152227e+00,  4.87896954e+00,\n",
       "         9.97776168e+00, -1.49861909e+01],\n",
       "       [ 7.67342151e+00,  1.23532720e+00, -4.37892911e+00,\n",
       "        -8.56023614e+00,  1.04389170e+01],\n",
       "       [ 3.28167147e+00, -7.74976258e+00, -1.16264563e+01,\n",
       "        -1.93535674e+00, -7.49534011e+00],\n",
       "       [-6.92277611e+00, -5.44984994e-01, -8.88486620e-01,\n",
       "        -1.39881267e+00, -8.36237379e+00],\n",
       "       [ 1.16263014e+01, -1.58366721e+00,  9.85928106e+00,\n",
       "        -6.39932648e+00,  8.35666639e+00],\n",
       "       [-6.29952448e+00, -1.64503882e+00,  7.30453927e+00,\n",
       "         3.55019215e+00, -8.63654520e+00],\n",
       "       [-2.82905956e+00,  6.29599132e+00,  1.26253390e-01,\n",
       "        -4.28026559e+00, -1.03401184e+01],\n",
       "       [-5.35122256e+00,  2.39148500e-01, -4.07083542e-01,\n",
       "        -8.16447285e+00, -3.69803789e+00],\n",
       "       [ 4.28813690e+00,  2.22374740e-01, -2.14563730e+00,\n",
       "        -6.93784637e+00, -1.54658660e+00],\n",
       "       [ 6.16407207e-01,  5.37315719e+00,  5.07108816e+00,\n",
       "        -4.44469764e+00, -1.71739181e+00],\n",
       "       [ 7.86118185e+00,  8.56017965e+00, -5.12411103e+00,\n",
       "         2.38142243e+00,  7.30213466e+00],\n",
       "       [ 5.55698490e+00,  1.42025851e+00,  5.96152787e+00,\n",
       "         5.73616448e+00, -6.01481820e+00],\n",
       "       [ 2.45676172e+00, -3.33622087e+00, -3.28494869e+00,\n",
       "        -1.55052681e+00,  5.96491746e+00],\n",
       "       [-5.22682405e+00,  4.55245303e+00,  2.37641607e+00,\n",
       "         4.83742875e+00,  4.05379270e+00],\n",
       "       [ 9.08043151e-01, -4.16635995e+00,  6.63357806e+00,\n",
       "         2.18771838e+00,  7.09258471e+00],\n",
       "       [ 1.37294726e+00, -2.54186177e-01, -2.75868185e+00,\n",
       "         4.46920666e+00, -5.51835031e+00],\n",
       "       [ 6.03808975e+00,  1.55371089e+00,  2.52618379e+00,\n",
       "        -1.00306356e-01, -1.72078801e+00],\n",
       "       [-7.54681640e+00, -8.02246586e+00,  1.07130838e+01,\n",
       "         1.60997032e+01, -6.36796165e+00],\n",
       "       [ 4.06244528e+00, -8.64966191e+00,  1.03371953e+01,\n",
       "        -5.75706891e+00, -9.69108377e+00],\n",
       "       [-2.46480667e+00, -5.15281777e-02, -1.65051509e+00,\n",
       "        -4.99641391e+00, -4.57023240e+00],\n",
       "       [ 6.00766521e-01,  2.86174491e+00,  3.37397636e+00,\n",
       "        -5.37331861e+00, -2.54590344e+00],\n",
       "       [ 3.41579002e+00,  1.17352543e+01, -1.15000679e+01,\n",
       "        -5.79217240e+00,  2.57558348e+00],\n",
       "       [-9.90689105e+00,  2.27681428e+00,  1.37074612e+00,\n",
       "         7.55247047e-01, -2.33828031e+00],\n",
       "       [-4.40964223e+00,  1.82845407e+00,  2.98715706e+00,\n",
       "         1.71635688e+00, -5.31710402e+00],\n",
       "       [-4.82073971e+00,  3.75161034e-01, -1.70219666e+00,\n",
       "        -7.27915673e+00,  2.31941929e+00],\n",
       "       [ 5.01599743e-01,  1.06682036e+01,  6.57983689e+00,\n",
       "        -4.16329000e+00, -1.39040126e+01],\n",
       "       [-8.20580085e+00, -8.64720090e+00, -1.04958511e+01,\n",
       "         3.74882458e+00,  2.20821947e+00],\n",
       "       [ 4.56489541e+00,  5.25988179e+00,  2.92501757e+00,\n",
       "        -1.09673313e+00, -4.58051201e+00],\n",
       "       [ 3.05399856e+00,  3.03611107e+00, -1.08459751e+00,\n",
       "         2.98250499e+00,  4.56094893e-01],\n",
       "       [ 9.95575381e+00,  3.96764808e+00,  2.83457628e+00,\n",
       "        -1.33023761e+00, -4.62276138e-01],\n",
       "       [-5.61419501e+00,  4.20876832e+00,  1.14046913e+00,\n",
       "        -9.08055939e-01, -1.04931728e+01],\n",
       "       [ 2.24327803e+00,  3.05045592e+00,  1.86814175e+00,\n",
       "         5.59896197e-01, -4.11790426e+00],\n",
       "       [ 1.19665745e+01, -2.03333063e+00, -4.88412269e-01,\n",
       "         9.60803773e+00,  6.52865151e+00],\n",
       "       [ 2.18523497e+00, -6.81317195e+00,  7.56652054e-01,\n",
       "         4.63519544e+00,  3.31030961e+00],\n",
       "       [ 1.34820015e+01, -2.52916391e+00, -1.49044957e+00,\n",
       "        -1.01814323e+01,  1.05959006e+01],\n",
       "       [ 1.06009982e+01,  1.88363130e+00,  7.29266670e+00,\n",
       "        -5.44263245e+00, -8.10320921e+00],\n",
       "       [ 3.72163709e+00, -3.86658727e-01,  3.97376204e+00,\n",
       "        -4.54149658e+00, -5.19397835e+00],\n",
       "       [-3.23784372e+00, -1.73084426e+00, -5.69382881e+00,\n",
       "        -6.23011293e+00, -8.21583104e+00],\n",
       "       [ 1.19666472e+01,  6.19867509e+00, -3.18122784e+00,\n",
       "        -1.37228263e+01,  6.68779174e+00],\n",
       "       [-1.59709371e+00,  2.26585742e+00, -8.21126070e+00,\n",
       "        -2.30648397e+00, -7.41688325e-01],\n",
       "       [-4.69306892e+00,  3.89814096e+00,  2.35075689e+00,\n",
       "         5.59765602e+00, -2.80945615e+00],\n",
       "       [-3.23170955e+00, -2.81651367e+00, -5.79307749e+00,\n",
       "        -4.67724624e+00,  3.98796377e+00],\n",
       "       [ 2.35068279e-01,  3.85783809e+00,  7.46611144e-01,\n",
       "         2.07066910e+00,  3.40265470e+00],\n",
       "       [-3.74447206e-01, -4.88067755e+00,  1.69586500e+00,\n",
       "         1.27923109e+00,  2.13262207e+00],\n",
       "       [ 1.47918026e+01,  4.57708384e+00, -5.29951276e+00,\n",
       "        -6.22444384e+00, -2.24853330e+00],\n",
       "       [-6.00721131e-02, -3.07995312e+00, -7.50028560e-01,\n",
       "         4.04276781e+00,  8.08108693e+00],\n",
       "       [ 1.23619928e+01, -9.32859310e+00, -7.73326292e+00,\n",
       "         9.07401852e+00, -7.85615765e+00],\n",
       "       [ 5.89780678e+00, -4.97233125e+00, -4.21117114e+00,\n",
       "         4.50066427e+00, -2.94151074e-01],\n",
       "       [-1.17873025e+01, -2.06802302e+00,  3.30677011e+00,\n",
       "         1.22261740e+01, -4.57801304e+00],\n",
       "       [-5.13701724e+00,  2.48300386e+00, -3.76431381e+00,\n",
       "        -6.14849807e+00, -5.06188441e+00],\n",
       "       [-4.59101788e-01,  4.10207034e+00, -4.58134331e+00,\n",
       "        -4.11186176e+00, -3.47435329e+00],\n",
       "       [-7.79068563e-01, -2.96137861e-01, -5.78748622e-01,\n",
       "         4.39234672e+00,  5.32454281e-01],\n",
       "       [-5.03837096e+00,  1.42010576e+00,  5.24519340e+00,\n",
       "        -4.65887333e-01, -2.02373839e-01],\n",
       "       [ 6.17899053e-01,  1.79668085e+00, -3.46259122e+00,\n",
       "        -2.77234759e+00, -1.02397782e+00],\n",
       "       [ 2.80846112e+00, -1.26182795e+01, -3.60483127e+00,\n",
       "         1.09487473e+01, -4.80349576e-01],\n",
       "       [-5.12081573e+00, -1.00989931e+01, -4.94196359e+00,\n",
       "         1.39771687e-01, -1.73926975e+00],\n",
       "       [-3.44339027e+00,  4.97912355e-01,  6.71045366e-01,\n",
       "         1.13721207e+00,  3.45069527e+00],\n",
       "       [-4.83927213e+00, -6.86857488e-01,  1.17018835e+00,\n",
       "        -5.27203809e-01, -8.67182418e+00],\n",
       "       [ 1.32795303e+00,  1.06525402e+00, -2.15519952e+00,\n",
       "         8.09519303e-01,  2.92319559e+00],\n",
       "       [-9.85151802e+00,  8.18218453e+00, -5.01452165e+00,\n",
       "         6.34783956e-01, -2.83554733e+00],\n",
       "       [ 7.02903800e+00, -2.70866912e+00,  2.00624273e+00,\n",
       "        -3.83962082e+00, -5.19014146e+00],\n",
       "       [-9.42600049e+00, -1.06099040e+01,  1.61261104e+01,\n",
       "        -1.25121462e+01,  9.40152561e+00],\n",
       "       [ 1.12416313e+01,  2.71416818e+00,  8.25203220e-01,\n",
       "        -3.72963285e+00,  4.99558746e+00],\n",
       "       [ 4.33447658e+00,  4.84548634e+00, -1.44924261e+00,\n",
       "         1.46232507e+00, -6.13695062e+00],\n",
       "       [-3.55081758e-01, -8.60771117e+00,  5.46689085e+00,\n",
       "         5.79349075e+00, -8.68454563e+00],\n",
       "       [ 1.07897001e+01, -1.33907634e+01, -2.73519447e-01,\n",
       "        -5.39460039e+00,  6.06576504e+00],\n",
       "       [ 5.70899239e+00, -4.88439503e+00, -1.11607092e+00,\n",
       "         5.14633466e+00, -5.58782186e+00],\n",
       "       [ 1.08718611e+00,  7.26430193e-01, -4.02329668e-01,\n",
       "         1.22425801e+00,  2.18536416e-01],\n",
       "       [-1.67394389e-01, -8.48100436e+00, -3.70817528e+00,\n",
       "         3.35953792e+00,  3.96064925e+00],\n",
       "       [ 2.62562399e+00, -9.95556634e+00,  2.81431694e+00,\n",
       "         5.35700173e+00, -4.50237403e+00],\n",
       "       [ 6.16130692e+00, -3.10595997e+00,  6.04925236e+00,\n",
       "         5.74445999e+00, -4.21483888e+00],\n",
       "       [-6.44837917e+00, -1.43306769e+01, -2.80355975e+00,\n",
       "         1.15784281e+01,  1.71463579e+00],\n",
       "       [ 1.79704062e+00, -3.28077021e+00, -2.85749524e+00,\n",
       "         1.06799139e+00,  6.85982249e-01],\n",
       "       [-7.47835149e+00, -1.06318894e+00, -4.70401933e-01,\n",
       "         1.56308491e+00, -5.79896232e+00],\n",
       "       [ 1.57503647e+00,  3.45497618e+00, -3.53593275e+00,\n",
       "         3.36268184e+00,  4.40095066e+00],\n",
       "       [-2.62186100e+00, -1.30067515e+01,  6.19691431e+00,\n",
       "         8.21959524e+00, -7.71590358e-01],\n",
       "       [ 2.28737065e+00,  4.04633885e-01,  5.11453173e+00,\n",
       "        -1.51704661e+01, -1.59282861e+00],\n",
       "       [ 2.91563045e-01, -9.18180507e+00,  2.11331110e+00,\n",
       "         4.03712175e+00, -4.46904267e+00],\n",
       "       [ 1.95831669e+00,  3.55714775e+00, -1.73723939e+00,\n",
       "        -5.80411430e+00, -4.83670453e-01],\n",
       "       [-1.20998157e+01, -5.23897786e+00,  6.10526107e+00,\n",
       "         5.41196372e-01, -5.68871642e+00],\n",
       "       [ 1.29015003e+00,  2.67372655e+00, -6.04987644e+00,\n",
       "        -3.06012803e+00,  3.01239101e+00],\n",
       "       [ 3.98397765e+00,  4.63101851e+00,  1.94118615e+00,\n",
       "         2.67468424e+00, -4.18846329e-01],\n",
       "       [-4.76414036e+00, -1.41947373e-01,  5.12741018e+00,\n",
       "         2.11437370e+00,  1.60064782e+00],\n",
       "       [ 1.04876309e+00, -3.06029828e+00, -8.25941175e-01,\n",
       "         4.00308242e+00,  2.71413493e+00],\n",
       "       [-3.95411715e+00, -2.17174876e+00, -5.85562270e-01,\n",
       "        -2.71478821e+00,  1.87274903e+00],\n",
       "       [-5.06476745e-01, -4.94469668e+00, -3.39646478e+00,\n",
       "         3.27313425e+00, -3.38006191e+00],\n",
       "       [ 2.43126640e+00, -4.61246840e+00, -2.49860508e+00,\n",
       "         9.63148285e+00, -2.30965472e+00],\n",
       "       [-1.88994674e+01, -1.21778964e+01,  1.52595998e+01,\n",
       "        -6.29691029e+00,  1.46311157e+01],\n",
       "       [ 1.17368624e+01, -1.37415002e+01,  9.68711375e-01,\n",
       "         1.08259900e+01, -1.57139780e+01],\n",
       "       [ 4.90749050e+00, -5.23508961e+00,  6.26053687e+00,\n",
       "        -4.27531171e+00,  4.39406059e-01],\n",
       "       [-7.92274098e+00,  1.34375725e+01,  2.52662513e+00,\n",
       "        -6.62353401e+00,  1.01395302e+00],\n",
       "       [-4.09859001e+00, -8.44691703e+00, -1.19894963e+01,\n",
       "         8.89973479e+00,  1.50504599e+01],\n",
       "       [-6.51152771e+00, -1.96191468e+00, -2.07297133e-01,\n",
       "        -2.76968662e+00,  2.47184592e+00],\n",
       "       [-7.37874929e+00, -3.64997797e+00, -2.88154374e-01,\n",
       "        -5.35758544e+00,  1.37094917e+01],\n",
       "       [ 3.08307261e+00, -1.08536127e+01,  3.04030091e-01,\n",
       "         4.86268267e+00, -1.15837476e+01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimation[1][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
