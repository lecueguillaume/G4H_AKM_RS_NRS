{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of the Keras framework to build recommender systems. \n",
    "\n",
    "key ideas :\n",
    "1) We take advantage of the power of a modern computation framework like Keras (i.e. automatic differentiation) to implement a vanilla matrix factorization recommender with minimal code.\n",
    "2) We make it a bit more complex by adding users biais and items biais as well as a non linear output function\n",
    "3) We finally design a neural network\n",
    "\n",
    "The data used for this task is the smallest MovieLens data set already saved to a local directory so we can get started with some imports and reading in the ratings.csv file, which is where the data for this task comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie lens dataset\n",
    "\n",
    "This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from [MovieLens](http://movielens.org), a movie recommendation service. It contains **100836 ratings** and 3683 tag applications across **9724 movies**. These data were created by **610 users** between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The data are contained in the files `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`. More details about the contents and use of all these files follows.\n",
    "\n",
    "This is a *development* dataset. As such, it may change over time and is not an appropriate dataset for shared research results. See available *benchmark* datasets if that is your intent.\n",
    "\n",
    "This and other GroupLens data sets are publicly available for download at <http://grouplens.org/datasets/> **Dowlaod dataset from [here](https://grouplens.org/datasets/movielens/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"./ml-latest-small/\"\n",
    "ratings = pd.read_csv(PATH + \"ratings.csv\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is tabular and consists of a user ID, a movie ID, and a rating (there’s also a timestamp but we won’t use it for this task). Our task is to predict the rating for a user/movie pair, with the idea that if we had a model that’s good at this task then we could predict how a user would rate movies they haven’t seen yet and recommend movies with the highest predicted rating.\n",
    "\n",
    "The zip file also includes a listing of movies and their associated genres. We don’t actually need this for the model but it’s useful to know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv(PATH + \"movies.csv\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better sense of what the data looks like, we can turn it into a table by selecting the top 15 users/movies from the data and joining them together. The result shows how each of the top users rated each of the top movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>1</th>\n",
       "      <th>50</th>\n",
       "      <th>110</th>\n",
       "      <th>260</th>\n",
       "      <th>296</th>\n",
       "      <th>318</th>\n",
       "      <th>356</th>\n",
       "      <th>480</th>\n",
       "      <th>527</th>\n",
       "      <th>589</th>\n",
       "      <th>593</th>\n",
       "      <th>1196</th>\n",
       "      <th>2571</th>\n",
       "      <th>2858</th>\n",
       "      <th>2959</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  1     50    110   260   296   318   356   480   527   589   593   \\\n",
       "userId                                                                      \n",
       "68        2.5   3.0   2.5   5.0   2.0   3.0   3.5   3.5   4.0   3.5   3.5   \n",
       "182       4.0   4.5   3.5   3.5   5.0   4.5   5.0   3.5   4.0   2.0   4.5   \n",
       "249       4.0   4.0   5.0   5.0   4.0   4.5   4.5   4.0   4.5   4.0   4.0   \n",
       "274       4.0   4.0   4.5   3.0   5.0   4.5   4.5   3.5   4.0   4.5   4.0   \n",
       "288       4.5   NaN   5.0   5.0   5.0   5.0   5.0   2.0   5.0   4.0   5.0   \n",
       "307       4.0   4.5   3.5   3.5   4.5   4.5   4.0   3.5   4.5   2.5   4.5   \n",
       "380       5.0   4.0   4.0   5.0   5.0   3.0   5.0   5.0   NaN   5.0   5.0   \n",
       "387       NaN   4.5   3.5   4.5   5.0   3.5   4.0   3.0   NaN   3.5   4.0   \n",
       "414       4.0   5.0   5.0   5.0   5.0   5.0   5.0   4.0   4.0   5.0   4.0   \n",
       "448       5.0   4.0   NaN   5.0   5.0   NaN   3.0   3.0   NaN   3.0   5.0   \n",
       "474       4.0   4.0   3.0   4.0   4.0   5.0   3.0   4.5   5.0   4.0   4.5   \n",
       "599       3.0   3.5   3.5   5.0   5.0   4.0   3.5   4.0   NaN   4.5   3.0   \n",
       "603       4.0   NaN   1.0   4.0   5.0   NaN   3.0   NaN   3.0   NaN   5.0   \n",
       "606       2.5   4.5   3.5   4.5   5.0   3.5   4.0   2.5   5.0   3.5   4.5   \n",
       "610       5.0   4.0   4.5   5.0   5.0   3.0   3.0   5.0   3.5   5.0   4.5   \n",
       "\n",
       "movieId  1196  2571  2858  2959  \n",
       "userId                           \n",
       "68        5.0   4.5   5.0   2.5  \n",
       "182       3.0   5.0   5.0   5.0  \n",
       "249       5.0   5.0   4.5   5.0  \n",
       "274       4.5   4.0   5.0   5.0  \n",
       "288       4.5   3.0   NaN   3.5  \n",
       "307       3.0   3.5   4.0   4.0  \n",
       "380       5.0   4.5   NaN   4.0  \n",
       "387       4.5   4.0   4.5   4.5  \n",
       "414       5.0   5.0   5.0   5.0  \n",
       "448       5.0   2.0   4.0   4.0  \n",
       "474       5.0   4.5   3.5   4.0  \n",
       "599       5.0   5.0   5.0   5.0  \n",
       "603       3.0   5.0   5.0   4.0  \n",
       "606       4.5   5.0   4.5   5.0  \n",
       "610       5.0   5.0   3.5   5.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = ratings.groupby(\"userId\")[\"rating\"].count()\n",
    "top_users = g.sort_values(ascending=False)[:15]\n",
    "g = ratings.groupby(\"movieId\")[\"rating\"].count()\n",
    "top_movies = g.sort_values(ascending=False)[:15]\n",
    "top_r = ratings.join(top_users, rsuffix=\"_r\", how=\"inner\", on=\"userId\")\n",
    "top_r = top_r.join(top_movies, rsuffix=\"_r\", how=\"inner\", on=\"movieId\")\n",
    "pd.crosstab(top_r.userId, top_r.movieId, top_r.rating, aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our first collaborative filtering model, we need to take care of a few things first. The user/movie fields are currently non-sequential integers representing some unique ID for that entity. We need them to be sequential starting at zero to use for modeling (you’ll see why later). We can use scikit-learn’s LabelEncoder class to transform the fields. We’ll also create variables with the total number of unique users and movies in the data, as well as the min and max ratings present in the data, for reasons that will become apparent shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724, 0.5, 5.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_enc = LabelEncoder()\n",
    "ratings[\"user\"] = user_enc.fit_transform(ratings[\"userId\"].values)\n",
    "n_users = ratings[\"user\"].nunique()\n",
    "item_enc = LabelEncoder()\n",
    "ratings[\"movie\"] = item_enc.fit_transform(ratings[\"movieId\"].values)\n",
    "n_movies = ratings[\"movie\"].nunique()\n",
    "ratings[\"rating\"] = ratings[\"rating\"].values.astype(np.float32)\n",
    "min_rating = min(ratings[\"rating\"])\n",
    "max_rating = max(ratings[\"rating\"])\n",
    "n_users, n_movies, min_rating, max_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a traditional (X, y) pairing of data and label, then split the data into training and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90752, 2), (10084, 2), (90752,), (10084,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ratings[[\"user\", \"movie\"]].values\n",
    "y = ratings[\"rating\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another constant we’ll need for the model is the number of latent factors per user/movie. This number can be whatever we want, however 50 seemed to work best so we’ll go with that.\n",
    "Finally, we need to turn users and movies into separate arrays in the training and test data. This is because in Keras they’ll each be defined as distinct inputs, and the way Keras works is each input needs to be fed in as its own array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 50\n",
    "X_train_array = [X_train[:, 0], X_train[:, 1]]\n",
    "X_test_array = [X_test[:, 0], X_test[:, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a matrix factorization recommendation system with keras\n",
    "\n",
    "Now we get to a first model which is the vanilla matrix factorization model. The main idea here is we’re going to use embeddings to represent each user and each movie in the dataset. These embeddings will be vectors (of size n_factors=50) that start out as random numbers but are fit by the model to capture the essential qualities of each user/movie. We can accomplish this by computing the dot product between a user vector and a movie vector to get a predicted rating. The code is fairly simple, there isn’t even a traditional neural network layer or activation involved. I stuck some regularization on the embedding layers and used a different initializer but even that probably isn’t necessary. Notice that this is where we need the number of unique users and movies, since those are required to define the size of each embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, Dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def RecommenderV1(n_users, n_movies, n_factors):\n",
    "    user = Input(shape=(1,))  # One hot encoding into a vector of size 610 (nb of users)\n",
    "    u = Embedding(\n",
    "        n_users,\n",
    "        n_factors,\n",
    "        embeddings_initializer=\"he_normal\",\n",
    "        embeddings_regularizer=l2(1e-6),\n",
    "    )(\n",
    "        user\n",
    "    )  # Embedding 610->50 with a l2 regularization\n",
    "    u = Reshape((n_factors,))(u)\n",
    "\n",
    "    movie = Input(\n",
    "        shape=(1,)\n",
    "    )  # One hot encoding into a vector of size 9724 (nb of movies)\n",
    "    m = Embedding(\n",
    "        n_movies,\n",
    "        n_factors,\n",
    "        embeddings_initializer=\"he_normal\",\n",
    "        embeddings_regularizer=l2(1e-6),\n",
    "    )(\n",
    "        movie\n",
    "    )  # Embedding 9724->50 with a l2 regularization\n",
    "    m = Reshape((n_factors,))(m)\n",
    "\n",
    "    x = Dot(axes=1)([u, m])\n",
    "    model = Model(inputs=[user, movie], outputs=x)\n",
    "    opt = Adam(lr=0.001)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is kind of a neat example of how flexible and powerful modern computation frameworks like Keras and PyTorch are. Even though these are billed as deep learning libraries, they have the building blocks to **quickly create any computation graph you want and get automatic differentiation essentially for free**. Below you can see that all of the parameters are in the embedding layers, we don’t have any traditional neural net components at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1, 50)        30500       input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1, 50)        486200      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 50)           0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 50)           0           embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1)            0           reshape_13[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 516,700\n",
      "Trainable params: 516,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RecommenderV1(n_users, n_movies, n_factors)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recover the number of parameters to learn for the two embeddings layers via **610(nb of users)*50(dimension of the latent space)=30500** and **9724(nb of movies)*50=486200**\n",
    "\n",
    "Let’s go ahead and train this for a few epochs and see what we get.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90752 samples, validate on 10084 samples\n",
      "Epoch 1/5\n",
      "90752/90752 [==============================] - 9s 98us/step - loss: 9.3618 - val_loss: 3.1964\n",
      "Epoch 2/5\n",
      "90752/90752 [==============================] - 8s 93us/step - loss: 1.9611 - val_loss: 1.6230\n",
      "Epoch 3/5\n",
      "90752/90752 [==============================] - 9s 94us/step - loss: 1.1066 - val_loss: 1.3503\n",
      "Epoch 4/5\n",
      "90752/90752 [==============================] - 8s 93us/step - loss: 0.8477 - val_loss: 1.2677\n",
      "Epoch 5/5\n",
      "90752/90752 [==============================] - 8s 93us/step - loss: 0.7186 - val_loss: 1.2370\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=X_train_array,\n",
    "    y=y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test_array, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a bias and a sigmoid output\n",
    "Not bad for a first try. We can make some improvements though. \n",
    "\n",
    "The first thing we can do is add a “bias” to each embedding. The concept is similar to the bias in a fully-connected layer or the intercept in a linear model. It just provides an extra degree of freedom. We can implement this idea using new embedding layers with a vector length of one. The bias embeddings get added to the result of the dot product.\n",
    "\n",
    "The second improvement we can make is running the output of the dot product through a sigmoid layer and then scaling the result using the min and max ratings in the data. This is a neat technique that introduces a non-linearity into the output and results in a modest performance bump.\n",
    "\n",
    "I also refactored the code a bit by pulling out the embedding layer and reshape operation into a separate class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Add, Activation, Lambda\n",
    "\n",
    "\n",
    "class EmbeddingLayer:\n",
    "    def __init__(self, n_items, n_factors):\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = Embedding(\n",
    "            self.n_items,\n",
    "            self.n_factors,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=l2(1e-6),\n",
    "        )(x)\n",
    "        x = Reshape((self.n_factors,))(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def RecommenderV2(n_users, n_movies, n_factors, min_rating, max_rating):\n",
    "    user = Input(shape=(1,))\n",
    "    u = EmbeddingLayer(n_users, n_factors)(user)\n",
    "    ub = EmbeddingLayer(n_users, 1)(user)  # It is the bias term for users\n",
    "\n",
    "    movie = Input(shape=(1,))\n",
    "    m = EmbeddingLayer(n_movies, n_factors)(movie)\n",
    "    mb = EmbeddingLayer(n_movies, 1)(movie)  # It is the bias term for movies\n",
    "    x = Dot(axes=1)([u, m])\n",
    "    x = Add()([x, ub, mb])\n",
    "    x = Activation(\"sigmoid\")(x)  # The model is sigma(<u,m> + ub + um)\n",
    "    x = Lambda(lambda x: x * (max_rating - min_rating) + min_rating)(\n",
    "        x\n",
    "    )  # rescale function from [0,1] to the range of the grades\n",
    "    model = Model(inputs=[user, movie], outputs=x)\n",
    "    opt = Adam(lr=0.001)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary shows the new graph. Notice the additional embedding layers with parameter numbers equal to the unique user and movie counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 50)        30500       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 50)        486200      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 50)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 50)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 1)         610         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 1)         9724        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           reshape_3[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1)            0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dot_2[0][0]                      \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           activation_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 527,034\n",
      "Trainable params: 527,034\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RecommenderV2(n_users, n_movies, n_factors, min_rating, max_rating)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary shows the new graph. Notice the additional embedding layers with parameter numbers equal to the unique user and movie counts (We see here that 610 parameters and 9724 parameters have been added to the model : one bias for each user and each movie.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90752 samples, validate on 10084 samples\n",
      "Epoch 1/5\n",
      "90752/90752 [==============================] - 9s 100us/step - loss: 1.1964 - val_loss: 0.8559\n",
      "Epoch 2/5\n",
      "90752/90752 [==============================] - 9s 95us/step - loss: 0.6987 - val_loss: 0.7564\n",
      "Epoch 3/5\n",
      "90752/90752 [==============================] - 9s 97us/step - loss: 0.5301 - val_loss: 0.7333\n",
      "Epoch 4/5\n",
      "90752/90752 [==============================] - 9s 97us/step - loss: 0.4017 - val_loss: 0.7358\n",
      "Epoch 5/5\n",
      "90752/90752 [==============================] - 9s 97us/step - loss: 0.3092 - val_loss: 0.7500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=X_train_array,\n",
    "    y=y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test_array, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those two additions to the model resulted in a pretty sizable improvement. Validation error is now down to ~0.76 which is about as good as the SOTA for this data set.\n",
    "That pretty much covers the conventional approach to solving this problem, but there’s another way we can tackle this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Recommendation system neural network\n",
    "\n",
    "Instead of taking the dot product of the embedding vectors, what if we just concatenated the embeddings together and stuck a fully-connected layer on top of them? It’s still not technically “deep” but it would at least be a neural network! To modify the code, we can remove the bias embeddings from V2 and do a concat on the embedding layers instead of the dot product. Then we can add some dropout, insert a dense layer, and stick some dropout on the dense layer as well. Finally, we’ll run it through a single-unit dense layer to keep the sigmoid trick at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate, Dense, Dropout\n",
    "\n",
    "\n",
    "def RecommenderNet(n_users, n_movies, n_factors, min_rating, max_rating):\n",
    "    user = Input(shape=(1,))\n",
    "    u = EmbeddingLayer(n_users, n_factors)(\n",
    "        user\n",
    "    )  # embedding 610->50 without regularization\n",
    "\n",
    "    movie = Input(shape=(1,))\n",
    "    m = EmbeddingLayer(n_movies, n_factors)(\n",
    "        movie\n",
    "    )  # embedding 9724->50 without regularization\n",
    "\n",
    "    x = Concatenate()([u, m])\n",
    "    x = Dropout(0.05)(\n",
    "        x\n",
    "    )  # 5/100 of the units are desactivated at each pass during learning\n",
    "\n",
    "    x = Dense(10, kernel_initializer=\"he_normal\")(x)  # A dense layer 100+1(bias)->10\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(1, kernel_initializer=\"he_normal\")(x)  # A dense layer 10+1(bias)->1\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "    x = Lambda(lambda x: x * (max_rating - min_rating) + min_rating)(x)\n",
    "    model = Model(inputs=[user, movie], outputs=x)\n",
    "    opt = Adam(lr=0.001)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1, 50)        30500       input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 1, 50)        486200      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 50)           0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 50)           0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 100)          0           reshape_15[0][0]                 \n",
      "                                                                 reshape_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1010        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 10)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 10)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1)            0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1)            0           activation_6[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 517,721\n",
      "Trainable params: 517,721\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RecommenderNet(n_users, n_movies, n_factors, min_rating, max_rating)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the parameters are still in the embedding layers (30.500 + 486.200), but we have some added learning capability from the dense layers (101*10 + 11*1 = 1010+11 = 1021 parameters to learn in the two dense layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90752 samples, validate on 10084 samples\n",
      "Epoch 1/5\n",
      "90752/90752 [==============================] - 10s 111us/step - loss: 0.9082 - val_loss: 0.7941\n",
      "Epoch 2/5\n",
      "90752/90752 [==============================] - 10s 105us/step - loss: 0.7734 - val_loss: 0.7783\n",
      "Epoch 3/5\n",
      "90752/90752 [==============================] - 9s 104us/step - loss: 0.7425 - val_loss: 0.7810\n",
      "Epoch 4/5\n",
      "90752/90752 [==============================] - 9s 104us/step - loss: 0.7266 - val_loss: 0.7811\n",
      "Epoch 5/5\n",
      "90752/90752 [==============================] - 9s 104us/step - loss: 0.7164 - val_loss: 0.7779\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=X_train_array,\n",
    "    y=y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test_array, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions :\n",
    "\n",
    "1) Keras can be used to differentiate almost any function !\n",
    "\n",
    "2) Without doing any tuning at all we still managed to get a result that’s pretty close to the best performance we saw with the traditional approach. This technique has the added benefit that we can easily incorporate additional features into the model. For instance, we could create some date features from the timestamp or throw in the movie genres as a new embedding layer. We could tune the size of the movie and user embeddings independently since they no longer need to match. Lots of possibilities here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
